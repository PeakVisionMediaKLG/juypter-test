{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55a826b",
   "metadata": {},
   "source": [
    "## Lineare Regression durch Überwachtes Lernen von Grund auf erklärt\n",
    "Meistens werden, um maschinelles Lernen zu erklären, Module aus Libraries wie NumPy, Matplotlib, pandas oder sklearn benutzt, die einerseits den Umgang mit den Daten selbst erleichtern und andererseits vorgefertigte Methoden enthalten, die wir dann nur noch anwenden müssen. Hier wollen wir zunächst einen anderen Weg gehen und ein Problem (zugegebenermassen ein einfaches Problem) mit maschinellem Lernen lösen und hierzu nur Python-Grundlagen verwenden. Dies wird unser Verständnis vertiefen und außerdem genau aufzeigen, was hinter den Kulissen des maschinellen Lernens vorgeht. Lediglich das Modul ```matplotlib``` werden wir in sehr einfacher Form verwenden, um uns die Daten graphisch darstellen zu lassen. Wir werden danach unser Vorgehen zunehmend professionell gestalten.\n",
    "\n",
    "\n",
    "Stellen wir uns vor, ein Bekannter von uns betreibt ein Restaurant, dass jeden Abend geöffnet hat. Er hat das Problem, daß er wissen möchte, wieviele  Gerichte er durchschnittlich an einem Abend verkaufen kann, damit er das Personal und die Rohstoffe entsprechend im Voraus einplanen kann. Er hat bemerkt, daß diese Zahlen sehr von der Anzahl der Reservierungen an diesem Tag abhängen. Diese Reservierungen sind ihm jeweils bis einige Stunden vor der Öffnung bekannt und er hat beobachtet, daß er natürlich mehr Gerichte anbieten kann, wenn mehr Reservierungen vorliegen. Aber neben den Reservierungen kommt auch eine variable Anzahl von Gästen, die nicht reserviert haben. Er fragt uns, ob wir ihm vielleicht helfen können, aus den Reservierungszahlen der letzten Monate und der dann erfolgten Abgabe von Gerichten auf zukünftige Zahlen schließen zu können. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925a9dd",
   "metadata": {},
   "source": [
    "<img width=300 height=300 class=\"imgright\" src=\"Images/restaurant.png\" alt=\"restaurant\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9aeee",
   "metadata": {},
   "source": [
    "Dies ist ein Problem, welches wir hier versuchen wollen durch überwachtes Lernen zu lösen. Wir haben Reservierungszahlen und die jeweils zugehörige Anzahl von abgegebenen Gerichten vorliegen, haben also einige Datensätze mit den zugehörigen Labeln und könnten einen Algorithmus mit diesen Daten trainieren, um dann Voraussagen zu treffen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d21996",
   "metadata": {},
   "source": [
    "Wir bitten unseren Bekannten, uns seine Daten zukommen zu lassen und erhalten von ihm das Textfile \"Reservierung.txt\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2811385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservierungen: 10 Mahlzeiten 32\n",
      "Reservierungen: 2 Mahlzeiten 30\n",
      "Reservierungen: 49 Mahlzeiten 66\n",
      "Reservierungen: 5 Mahlzeiten 32\n",
      "Reservierungen: 97 Mahlzeiten 98\n",
      "Reservierungen: 90 Mahlzeiten 90\n",
      "Reservierungen: 100 Mahlzeiten 88\n",
      "Reservierungen: 14 Mahlzeiten 45\n",
      "Reservierungen: 36 Mahlzeiten 48\n",
      "Reservierungen: 13 Mahlzeiten 26\n",
      "Reservierungen: 50 Mahlzeiten 69\n",
      "Reservierungen: 88 Mahlzeiten 73\n",
      "Reservierungen: 39 Mahlzeiten 48\n",
      "Reservierungen: 4 Mahlzeiten 35\n",
      "Reservierungen: 16 Mahlzeiten 35\n",
      "Reservierungen: 52 Mahlzeiten 62\n",
      "Reservierungen: 61 Mahlzeiten 69\n",
      "Reservierungen: 62 Mahlzeiten 67\n",
      "Reservierungen: 23 Mahlzeiten 52\n",
      "Reservierungen: 56 Mahlzeiten 56\n",
      "Reservierungen: 3 Mahlzeiten 34\n",
      "Reservierungen: 37 Mahlzeiten 60\n",
      "Reservierungen: 96 Mahlzeiten 88\n",
      "Reservierungen: 83 Mahlzeiten 78\n",
      "Reservierungen: 70 Mahlzeiten 76\n",
      "Reservierungen: 95 Mahlzeiten 85\n",
      "Reservierungen: 40 Mahlzeiten 60\n",
      "Reservierungen: 98 Mahlzeiten 91\n",
      "Reservierungen: 19 Mahlzeiten 44\n",
      "Reservierungen: 68 Mahlzeiten 77\n",
      "Reservierungen: 17 Mahlzeiten 46\n",
      "Reservierungen: 25 Mahlzeiten 56\n",
      "Reservierungen: 91 Mahlzeiten 81\n",
      "Reservierungen: 20 Mahlzeiten 52\n",
      "Reservierungen: 29 Mahlzeiten 40\n",
      "Reservierungen: 75 Mahlzeiten 76\n",
      "Reservierungen: 33 Mahlzeiten 46\n",
      "Reservierungen: 84 Mahlzeiten 92\n",
      "Reservierungen: 78 Mahlzeiten 76\n",
      "Reservierungen: 74 Mahlzeiten 66\n",
      "Reservierungen: 86 Mahlzeiten 78\n",
      "Reservierungen: 11 Mahlzeiten 43\n",
      "Reservierungen: 44 Mahlzeiten 63\n",
      "Reservierungen: 28 Mahlzeiten 38\n",
      "Reservierungen: 76 Mahlzeiten 69\n",
      "Reservierungen: 24 Mahlzeiten 53\n",
      "Reservierungen: 41 Mahlzeiten 61\n",
      "Reservierungen: 77 Mahlzeiten 76\n",
      "Reservierungen: 89 Mahlzeiten 91\n",
      "Reservierungen: 64 Mahlzeiten 61\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data/Reservierungen.txt\",\"r\") as fh:\n",
    "    for line in fh:\n",
    "        x,y=line.rstrip().split(\",\") #um doppelten Zeilenumbruch zu vermeiden\n",
    "        print(f\"Reservierungen: {x} Mahlzeiten {y}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b9804",
   "metadata": {},
   "source": [
    "Diesen Zusammenhang lassen wir uns mit dem Modul Matplotlib graphisch ausgeben. Dies muss jetzt nicht gelernt oder verstanden werden, dazu folgt ein ganzes Kapitel, es dient nur der Veranschaulichung, denn als erstes wollen wir uns einen Überblick über die Daten verschaffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f37366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1088a99430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "import matplotlib.pyplot as plt \n",
    "xliste,yliste=[],[]\n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x,y=line.rstrip().split(\",\")\n",
    "        xliste.append(int(x))\n",
    "        yliste.append(int(y))  #wir schreiben unsere Reservierungen und die Anzahl der Mahlzeiten in\n",
    "                                   #eine 2-dimensionale Liste\n",
    "\n",
    "plt.xlabel(\"Reservierungen\", fontsize=12)#wir legen die Label für X-Achse und Y-Achse fest.                 \n",
    "plt.ylabel(\"Mahlzeiten\", fontsize=12)\n",
    "plt.scatter(xliste,yliste,color=\"red\",marker=\"*\")#wir machen mit matplotlib ein Streudiagramm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8dd288",
   "metadata": {},
   "source": [
    "Wir sehen, daß die Daten grob gesehen einen linearen Zusammenhang aufweisen. Deshalb legen wir durch die Punkte eine Gerade, die sich den Punkten bestmöglich annähert. Dieses wäre das klassische Vorgehen der linearen Regression und natürlich gibt es hierzu einfache Methoden, die man benutzen könnte. Für die Steigung m und den Schnittpunkt b mit der y-Achse bekommen wir mit S für Summe, n Anzahl der Punkte :<br><br>\n",
    "\\begin{aligned}{ {m }}&={\\frac {nS_{xy}-S_{x}S_{y}}{nS_{xx}-S_{x}^{2}}}\\\\[8pt]{ {b }}&={\\frac {1}{n}}S_{y}-{ {m }}{\\frac {1}{n}}S_{x}\\end{aligned}<br>\n",
    "Wir wollen das Problem jedoch mit maschinellem Lernen lösen. Die hier dargestellte berechnete optimale Linie dient sozusagen als \"Referenz\" unseres mit ML anzustrebenden Zieles. Die Steigung m dieser Optimalgeraden beträgt .587, der Schnittpunkt b mit der y-Achse 31.72 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebfecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1088b7fdf0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Optimalgerade\")\n",
    "plt.xlabel(\"Reservierungen\", fontsize=12)#wir legen die Label für X-Achse und Y-Achse fest.                 \n",
    "plt.ylabel(\"Mahlzeiten\", fontsize=12)\n",
    "plt.scatter(xliste,yliste,marker=\"*\")#wir machen mit matplotlib ein Streudiagramm\n",
    "plt.plot((0,100),(31.72,90.59))#wir zeichnen die Gerade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949b4a3",
   "metadata": {},
   "source": [
    "Wir wollen unsere Reservierungsdaten jetzt nutzen, um in der Trainingsphase unseres Programmes dieses selbstständig einen Zusammenhang zwischen den Eingangsdaten (Reservierungen) und den Labeln (Mahlzeiten) finden zu lassen, wobei wir von einem linearen Zusammenhang ausgehen. Dies ist natürlich nicht immer gegeben, aber hier geht es ja um die grundsätzliche Erklärung des Verfahrens. Dann wollen wir vom Programm Voraussagen erstellen lassen und sehen, wie gut sie in unser Beispiel passen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0ab79",
   "metadata": {},
   "source": [
    "<b>Um es klarer zu formulieren: das Modell soll aus allen denkbaren Geraden, die die Punkteverteilung unseres Modells approximieren, diejenige herausfinden, welche dies am besten kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a34801",
   "metadata": {},
   "source": [
    "Dazu müssen wir uns überlegen, wie wir eine solche Gerade mathematisch repräsentieren können und werden hierzu die übliche Geradengleichung verwenden:<br><br> <b>$$ y(Mahlzeiten) = m * x(Reservierungen) + b$$ </b><br> Hier vereinfachen wir zunächst weiter und lassen unsere Geraden durch den Nullpunkt gehen (b=0), um nur einen Parameter optimieren zu müssen. Hier ein Bündel mit solchen Geraden mit unterschiedlichen Werten für m und unsere Referenzgerade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b84df",
   "metadata": {},
   "source": [
    "\t\t\t\t\t\t\t\t\tMögliche Gerade durch den Nullpunkt und optimale Gerade \n",
    "<img width=500 src=\"Images/plotwithmultiplelines.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b037db",
   "metadata": {},
   "source": [
    "Unsere Steigung m ist im Moment alles, was die jeweilige Gerade definiert. Wenn wir nun für eine solche Gerade eine Voraussage treffen wollen, ist das sehr einfach und folgt der Formel: <b>$$y_{vor}=m*x $$</b><br> dabei soll <b>$y_{vor}$</b> für den jeweiligen vorausgesagten Wert stehen (<b>y</b> gibt dagegen den wahren y-Wert am Punkt <b>x</b> an.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafdee2",
   "metadata": {},
   "source": [
    "Wenn wir nun durch unsere Liste von Reservierungen gehen und jeweils mit unserer momentanen Steigung m einen y_vor- Wert für jeden x-Wert erzeugen, werden wir feststellen, daß sich diese Werte von den echten y-Werten, die für unseren x-Wert vorliegen, abweichen. Es ergibt sich also ein Fehler für jeden x-Wert. Diese Fehler können wir summieren. Es ergibt sich dabei aber das Problem, daß der Fehler positiv sein kann, wenn y_vor > y ist oder negativ im umgekehrten Fall. So könnten sich die Fehler in der Fehlersumme ausgleichen und nicht die wahre Abweichung abbilden. Will man den wirklichen Fehler berücksichtigen, müßte man die absolute Abweichung von <b>$|y - y_{vor}|$</b> summieren. Man kann sich stattdessen aber auch damit behelfen, daß man die Abweichung jeweils quadriert und erhält damit immer positive Werte, die man einfach summieren kann, wobei die Quadrierung grössere Abweichung noch betont. Man bildet also als Wert zur Beurteilung des Verfahrens die Fehlerquadratsumme:<br><br>$$Fehlerquadratsumme = \\sum_{i=1}^{n} (y - y_{vor})^2$$<br> Nun zur Implementierung. Wir berechnen für eine Steigung m (am Anfang auf willkürlich 0 gesetzt) die Summe der Fehlerquadrate und ändern dann die Steigung plus und minus um einen kleinen Wert, die Lernrate. Ist die Fehlersumme bei einem der beiden veränderten Werte der Steigung kleiner als die Fehlersumme mit der Originalsteigung, dann übernehmen wir diesen Steigungswert als neue Steigung für den nächsten Schritt. Dies wiederholen wir, bis zur Maximalzahl von Versuchen erreicht ist oder bis die Fehlersumme durch kleine Veränderungen der Steigung nicht mehr verbessert werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36225fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainieren(x_werte,y_werte,anzahl_durchgänge,lernrate):\n",
    "    m=0 #Anfangssteigung beliebig    \n",
    "    for lauf in range(anzahl_durchgänge):        \n",
    "        sum_0,sum_plus_lr,sum_minus_lr=0,0,0\n",
    "        fehler_werte=0\n",
    "        for i in range(len(x_werte)): #für alle Wertepaare Fehlerquadratsumme berechnen\n",
    "            sum_0+=(m*x_werte[i]- y_werte[i])**2 #m Differenz berechneter Wert mit m und echter Wert zum Quadrat\n",
    "            sum_plus_lr+=((m+lernrate)*x_werte[i]- y_werte[i])**2 #m+lernrate\n",
    "            sum_minus_lr+=((m-lernrate)*x_werte[i]- y_werte[i])**2 #m-lernrate\n",
    "        fehler_werte=sum_0,sum_plus_lr,sum_minus_lr\n",
    "        min_index=fehler_werte.index(min(fehler_werte))\n",
    "        if min_index==0:\n",
    "            print(f\"\\nEnde, keine Verbesserung mehr,  Fehlerquadratsumme {min(fehler_werte):6.2f}, m = {m:4.2f} in Lauf {lauf} \\n\")                        \n",
    "            return m\n",
    "        elif min_index==1:                    \n",
    "            m+=lernrate\n",
    "        else:\n",
    "            m-=lernrate        \n",
    "    return m #beste Steigung für m, m+lernrate, m-lernrate\n",
    "\n",
    "def voraussagen(m,x,y):\n",
    "    for x_item,y_item in sorted(zip(x,y)):\n",
    "        print(f\"X-Wert:{x_item:4.2f} wahrer Y-Wert:{y_item:4.2f} vorausgesagter Y-Wert: {x_item*m:4.2f}\")\n",
    "    \n",
    "    return \n",
    "    \n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))\n",
    "\n",
    "voraussagen(trainieren(x,y,50000,1e-4),x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6e988",
   "metadata": {},
   "source": [
    "Wir sehen nun unsere errechnete Gerade und erkennen, dass hier noch viel Raum zur Verbesserung ist. Wir haben noch eine große Fehlerquadratsumme. Der Wert der optimalen Steigung unseres Verfahrens mit 1.04 weicht vom Wert der Referenzgeraden mit 0.587 noch gewaltig ab.\n",
    "<img class=\"imgright\" src=\"Images/plotwithlearnerline.png\" alt=\"mit Linie\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc3e5d",
   "metadata": {},
   "source": [
    "Ein deutlicher Schritt zur Verbesserung des Systems wäre, die errechnete Gerade nicht durch den Nullpunkt gehen zu lassen, wie es auch unsere Referenzgerade nicht tut. Hier benötigen wir nun unsere komplette Geradengleichung mit <b> $y_{vor}=x*m + b $</b> wobei b den Schnittpunkt mit der y-Achse darstellt. Wir würden unseren Algorithmus dann so implementieren, daß wir auch den Wert für b in kleinen Schritten verändern und optimieren (wie für m).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0859a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainieren(x_werte,y_werte,anzahl_durchgänge,lernrate_m,lernrate_b):\n",
    "    m,b=0,0 #1,1 #Anfangssteigung,Anfangsschnittpunkt\n",
    "    \n",
    "    for lauf in range(anzahl_durchgänge):\n",
    "        sum_0,sum_m_plus_lr,sum_m_minus_lr,sum_b_plus_lr,sum_b_minus_lr=0,0,0,0,0\n",
    "        for i in range(len(x_werte)):\n",
    "            sum_0+=((m*x_werte[i]+b)- y_werte[i])**2\n",
    "            sum_m_plus_lr+=((m+lernrate_m)*x_werte[i]+b- y_werte[i])**2 #m+lr\n",
    "            sum_m_minus_lr+=((m-lernrate_m)*x_werte[i]+b- y_werte[i])**2 #m-lr\n",
    "            sum_b_plus_lr+=((m*x_werte[i]+b+lernrate_b)- y_werte[i])**2 #b+lr\n",
    "            sum_b_minus_lr+=((m*x_werte[i]+b-lernrate_b)- y_werte[i])**2 #b-lr\n",
    "            \n",
    "        werte=[sum_0,sum_m_plus_lr,sum_m_minus_lr,sum_b_plus_lr,sum_b_minus_lr]\n",
    "        \n",
    "        if min(werte)==sum_0:\n",
    "            print(f\"\\n keine Verbesserung mehr,Fehlerquadratsumme {sum_0:6.1f}, m = {m:6.4f}, b= {b:6.4f} in Lauf {lauf}\\n\") \n",
    "            return m,b\n",
    "           \n",
    "        if min(werte)==sum_m_plus_lr:\n",
    "            m+=lernrate_m\n",
    "        elif min(werte)==sum_m_minus_lr:\n",
    "            m-=lernrate_m\n",
    "        elif min(werte)==sum_b_plus_lr:\n",
    "            b+=lernrate_b\n",
    "        elif min(werte)==sum_b_minus_lr:\n",
    "            b-=lernrate_b      \n",
    "    print(f\"\\n Fehlerquadratsumme {sum_0:6.1f} , m = {m:6.4f}, b= {b:6.4f} in Lauf {anzahl_durchgänge}\\n\")\n",
    "    return m,b\n",
    "\n",
    "def voraussagen(m,b,x,y):    \n",
    "    for x_item,y_item in sorted(zip(x,y)):\n",
    "        print(f\"X-Wert:{x_item:4.3f} wahrer Y-Wert:{y_item:4.3f} vorausgesagter Y-Wert: {x_item*m+b:4.2f}\")\n",
    "    \n",
    "    return \n",
    "    \n",
    "    \n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))\n",
    "\n",
    "voraussagen(*trainieren(x,y,10000,0.001,0.1),x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(xliste,yliste,marker=\"*\")#wir machen mit matplotlib ein Streudiagramm\n",
    "plt.plot((0,100),(31.72,90.59),c=\"r\",label=\"Referenz\")#wir zeichnen die Refrenzgerade\n",
    "plt.plot((0,100),(31.9,90.3),c=\"b\",label=\"System\")\n",
    "plt.xlabel(\"x\",fontsize=20)\n",
    "plt.ylabel(\"y\",fontsize=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756610a4",
   "metadata": {},
   "source": [
    "Wir sehen, dass wir unser Ziel erreicht haben. Die Gerade liegt fast genau auf unserer \"Referenzgeraden\". Die Werte für m und b sind sehr nahe an den Werten der Referenzgeraden mit <br>\n",
    "<table style=\"text-align: left; width: 50%; background-color: rgb(255, 255, 102);font-size:14pt;\" border=\"1\" cellpadding=\"2\" cellspacing=\"2\">\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">Referenz\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">Lernsystem\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">m= .587\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">m= .584\n",
    "</td>\n",
    "</tr>    \n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">b= 31.72\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">b= 31.90\n",
    "</td>\n",
    "</tr>      \n",
    "</table>    \n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a46b08",
   "metadata": {},
   "source": [
    "Fassen wir zusammen, haben wir ein Programm geschrieben, welches Eingangsdaten auf Ausgangsdaten abbildet und hierfür in der Lage ist, den Zusammenhang selbstständig zu optimieren. Im obigen Programm findet man einige Parameter, die auf willkürliche Werte gesetzt zu sein scheinen. Es sind die Ausgangswerte m und b für den Optimierungsprozess.\n",
    "Variieren wir diese, hat das Auswirkungen auf den Zeitbedarf der Optimierung, das Ergebnis bleibt jedoch gleich. So sinkt der Zeitbedarf, wenn wir diese Werte von 0 , 0 auf 1 , 1 setzen, von 453 Läufen in diesem Beispiel auf 341 Läufe. Wichtiger sind die Werte für die lernrate_m und die lernrate_b, die jeweils die Schrittgrösse der Änderung für die Steigung m und den Schnittpunkt mit der y-Achse darstellen. Diese Werte haben ebenfalls einen Einfluss auf die Geschwindigkeit der Optimierung aber auch auf deren Genauigkeit.\n",
    "\n",
    "<table style=\"text-align: left; width: 100%; background-color: rgb(255, 255, 102);font-size:14pt;\" border=\"1\" cellpadding=\"2\" cellspacing=\"2\">\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">lernrate_m\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">lernrate_b\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">Gesamtfehlerquadratsumme\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">Anzahl Läufe bis Ende<br>(max 10000)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "2473\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "273\n",
    "</td>    \n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -2\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "1868\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "453\n",
    "</td>    \n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -3\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "1860\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "1267\n",
    "</td>    \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -4\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "1860\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "6760\n",
    "</td>    \n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -5\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -1\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "13559\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    ">10000\n",
    "</td>    \n",
    "</tr>    \n",
    "    \n",
    "    \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -3\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -2\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "1860\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "4670\n",
    "</td>    \n",
    "</tr>    \n",
    " \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -3\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -3\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "8950\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    ">10000\n",
    "</td>    \n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -3\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "10 ** -4\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    "14727\n",
    "</td>\n",
    "<td style=\"vertical-align: top;\">\n",
    ">10000\n",
    "</td>    \n",
    "</tr> \n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e3d05",
   "metadata": {},
   "source": [
    "Wir erkennen in obiger Tabelle, welchen Einfluss diese Parameter, die beim maschinellen Lernen als Hyperparameter bezeichnet werden, auf das Laufzeitverhalten bis zum Optimalwert haben, wobei wir die Anzahl der Durchgänge auf maximal 10000 beschränkt haben. Um die beste Leistung aus dem Lernverfahren herauszuholen, ist es eine notwendige Aufgabe, diese Hyperparameter auszutesten und zu optimieren. Wir erkennen außerdem, dass trotz eigentlich anzunehmender höherer Genauigkeit des Verfahrens bei sehr kleinen Werten für die Lernraten irgendwann der Gesamtfehler sogar wieder zuzunehmen scheint.<br><br>Dies deshalb, weil der Algorithmus in 10000 Schritten sein Optimum noch gar nicht erreicht hat. Um vergleichbare Fehlerwerte für die letzte Zeile zu erreichen wie für die beste Zeile müsste man <b>318517 Läufe</b> zulassen!\n",
    "\n",
    "Nun werden wir unser Programm mit NumPy professioneller gestalten. Wir erkennen nun auch, wie schrittweise die Gerade verbessert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from matplotlib.pyplot import figure\n",
    "#daten einlesen und übergeben\n",
    "\n",
    "def fehler(X,Y,w,b):\n",
    "    return np.sum((vorausgesagt(X,w,b)-Y)**2)\n",
    "\n",
    "def vorausgesagt(X,w,b):\n",
    "    return X*w+b\n",
    "    \n",
    "\n",
    "def train(X,Y,lauf_anzahl,lern_rate_w,lern_rate_b,step):\n",
    "    w,ws,b,bs=0,[],0,[]\n",
    "    flagw,flagb=0,0\n",
    "    for _ in range(lauf_anzahl):\n",
    "        walt,balt=w,b\n",
    "        fehler_jetzt=fehler(X,Y,w,b)\n",
    "        ###############Auskommentieren für Diagramm allein\n",
    "        print(f\"Lauf: {_:3d} Gesamtfehlerquadrat: {fehler_jetzt:5.2f}\")\n",
    "        w_up,w_down,b_up,b_down=w+lern_rate_w,w-lern_rate_w,b+lern_rate_b,b-lern_rate_b\n",
    "        fehler_w_up,fehler_w_down=fehler(X,Y,w_up,b),fehler(X,Y,w_down,b)\n",
    "        if fehler_w_up<fehler_jetzt:\n",
    "            w=w_up\n",
    "            ws.append(w)\n",
    "        elif fehler_w_down<fehler_jetzt:\n",
    "            w=w_down\n",
    "            ws.append(w)\n",
    "        else:\n",
    "            ws.append(w)\n",
    "            flagw=1\n",
    "        fehler_b_up,fehler_b_down=fehler(X,Y,walt,b_up),fehler(X,Y,walt,b_down)\n",
    "        if fehler_b_up<fehler_jetzt:\n",
    "            b=b_up\n",
    "            bs.append(b)\n",
    "        elif fehler_b_down<fehler_jetzt:\n",
    "            b=b_down\n",
    "            bs.append(b)\n",
    "        \n",
    "        else:\n",
    "            bs.append(b)\n",
    "            flagb=1\n",
    "        \n",
    "        if flagw==1 and flagb==1:\n",
    "            diagramm(X,Y,ws,bs,step) #um Diagramm zu produzieren\n",
    "            return w,b\n",
    "    print(f\"Noch keine Konvergenz erreicht bei w = {w:5.2f} b = {b:5.2f}\")\n",
    "    diagramm(X,Y,ws,bs,step) #um Diagramm zu produzieren\n",
    "    return w,b\n",
    "    sys.exit()\n",
    "              \n",
    "\n",
    "def diagramm(X,Y,ws,bs,step):\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.axis([0,100,0,100])\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xlabel(\"Reservierungen\")\n",
    "    plt.ylabel(\"Pizzas\")\n",
    "    plt.plot(X,Y,\"ro\")\n",
    "    wsmax=max(ws)\n",
    "    wsmin=min(ws)\n",
    "    \n",
    "    for i in range(0,len(ws),step):\n",
    "        green=(i/len(ws))\n",
    "        \n",
    "        plt.plot((0,100),(bs[i],100*ws[i]+bs[i]),color=(0,1-green,0),linewidth=1,label=str(round(ws[i],2))+\"/\"+str(round(bs[i],2)))\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "X,Y=np.loadtxt(\"Data/Reservierungen_np.txt\",skiprows=1,unpack=True)\n",
    "print(f\"die ersten 5 X_Werte {X[:5]}\") # Numpy ohne Kommata beim Ausdruck\n",
    "print(f\"die ersten 5 Y_Werte {Y[:5]}\")\n",
    "\n",
    "w_ende,b_ende=train (X,Y,lauf_anzahl=500,lern_rate_w=.01,lern_rate_b=.1,step=10)\n",
    "print(f\" Bestes w ist: {w_ende:5.2f} Bestes b ist: {b_ende:5.2f}\")\n",
    "#diagramm(X,Y,w_ende,b_ende,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc81c4b",
   "metadata": {},
   "source": [
    "Natürlich beschränkt sich unser Verfahren nicht nur auf die lineare Regression. Wir zeigen im Folgenden ein Beispiel mit einem Polynom 3. Grades, welches wir mit ML approximieren möchten.<br>\n",
    "Das Polynom wäre $$y = x^3*w0 + x^2*w1+ x * w2 + w3$$<br> Dabei ist zu beachten, daß wir nicht mehr die Unterscheidung in die Gewichte w und einen Wert für b machen. Unsere Gewichte (w0,w1,w2,w3) werden jetzt gleichberechtigt verwendet, w3  ist dann äquivalent zu b.<br>\n",
    "Unser Testpolynom wäre:<br>$$y = x^3 * -.2 + x^2 * 5 + x * -4 + 3$$<br>\n",
    "Dabei haben wir die Werte ein wenig gestreut.\n",
    "\n",
    " Hier unsere Werte zum Annähern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set()\n",
    "#make the values\n",
    "rng = np.random.RandomState(9)\n",
    "X = 30 * rng.rand(200)-10\n",
    "Y = X**3*-.2+X**2*5+X*-4+3 + 30 * rng.randn(200)\n",
    "#print(X[:5])\n",
    "#print(Y[:5])\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Beispielpunktewolke zum Annähern\",fontsize =16)\n",
    "plt.scatter(X,Y,c=\"red\",s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f71cc",
   "metadata": {},
   "source": [
    "Wir nähern jetzt unsere Punktewolke mit Polynomen an, den Grad des Polynoms können wir eingeben. Unser Polynom, was die Punktwolke erzeugt hat, war dritten Grades, wie oben beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fehler(X,Y,w):    \n",
    "    return np.average((vorausgesagt(X,w)-Y)**2)\n",
    "\n",
    "def vorausgesagt(X,w):    \n",
    "    predict_sum=0\n",
    "    for i in range(len(w)): #0 bis grad inclusiv        \n",
    "        predict_sum+=X**i*w[i] # hier wird das Polynom erzeugt\n",
    "    return predict_sum\n",
    "\n",
    "def train(X,Y,lernrate,lauf_anzahl,grad):\n",
    "    w=rng.uniform(-.2,.2,grad+1) #setze auf kleine pos-neg Werte\n",
    "    ws,fehler_list=[],[]\n",
    "    for lauf in range(lauf_anzahl):        \n",
    "        fehler_jetzt=fehler(X,Y,w)\n",
    "        ###############Auskommentieren für Diagramm\n",
    "        #print(f\"Lauf: {lauf:3d} Gesamtfehlerquadrat: {fehler_jetzt:6.4f}\")\n",
    "        w_new=w.copy()\n",
    "        for index,w_to_change in enumerate(w):\n",
    "            w_new[index]=w_to_change+lernrate\n",
    "            if fehler(X,Y,w_new)<fehler_jetzt:\n",
    "                continue\n",
    "            w_new[index]=w_to_change-lernrate\n",
    "            if fehler(X,Y,w_new)<fehler_jetzt:\n",
    "                continue\n",
    "        w=w_new.copy()\n",
    "        ws.append(w)\n",
    "        fehler_list.append(fehler(X,Y,ws[-1]))\n",
    "        #f lauf>2 and fehler_list[lauf-2]==fehler_list[lauf]:\n",
    "            #eturn w,ws,lauf_anzahl\n",
    "#        \n",
    "    return w,ws,lauf_anzahl\n",
    "\n",
    "pol_grad=int(input(\"Welcher Polynomgrad?: \"))\n",
    "w_ende,ws,lauf_anzahl=train(X,Y,lauf_anzahl=1000,lernrate=.004,grad=pol_grad)#grad 5\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(10,6))\n",
    " \n",
    "for lauf,lauf_ws in enumerate(ws):\n",
    "    X0=np.linspace(-10,20,200)\n",
    "    Y0=vorausgesagt(X0,lauf_ws)\n",
    "    green=(lauf/lauf_anzahl)\n",
    "    plt.title(f\"Polynom Grad {pol_grad}\")\n",
    "    plt.plot(X0,Y0,color=(0,1-green,0),linewidth=1,alpha=.02) #alpha hochsetzen für Darstellung?\n",
    "    plt.ylim((-100,800))\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Fehlerquadratsumme\")\n",
    "    \n",
    "plt.scatter(X,Y,c=\"red\",s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c850196",
   "metadata": {},
   "source": [
    "Wir können nun die Qualität der Annäherung beurteilen in Abhängigkeit vom Polynomgrad, den wir verwenden. Dabei zeigt sich, dass bei Erhöhung des Grades des Polynoms über eine gewisse Grenze hinaus die Annäherung nicht verbessert wird, im Gegenteil streben die Werte an der Grenze des Bereichs schnell gegen Unendlich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ff85a",
   "metadata": {},
   "source": [
    "Hier zuletzt ein Beispiel unter Einsatz von sklearn als ML-Bibliothek. Zuerst die lineare Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c11f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True) #die Linie muss nicht durch den 0 Punkt gehen\n",
    "x,y=np.loadtxt(\"Data/Reservierungen_np.txt\",skiprows=1,unpack=True)\n",
    "newx=x[:,np.newaxis]\n",
    "#print(x,np.shape(x)) #lineares Array\n",
    "#print(newx[:10],np.shape(newx)) # Methode möchte x im shape (50,1) bei weiteren Features (n) wäre es (50,n)\n",
    "#print(y) #y darf lineares Array bleiben\n",
    "model.fit(newx, y)\n",
    "\n",
    "xfit = np.linspace(0, 100, 1000) # 1000 Werte mit gleichem Absatnd zwischen 0 und 100\n",
    "yfit = model.predict(xfit[:, np.newaxis]) #die entsprechenden y Werte des Modells\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit); # die Gerade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd715a",
   "metadata": {},
   "source": [
    "Hier nochmal ein nichtlineares Beispiel mit sklearn, auch wieder interessant die Auswirkung des Polynomgrades. Wir sehen zunächst die anzunähernde Punktwolke, die mit einem Sinus und dann einer Zufallsstreuung produziert wurde.<br>\n",
    "Bei einem Polynom 19. Grades z.B. sehen wir wieder das \"Verschwinden\" der Annäherungskurve im Unendlichen und ein klares \"Overfitting\" vor allem im hohen Bereich. Hier versucht die Kurve einzelne Punkte abzubilden. Bei Polynomen geringen Grades sieht man eher \"Underfitting\" mit unzureichender Annäherung der Kurve.<br>Wir haben das Bestimmtheitsmass $R^2$ als Hinweis auf die Qualität der Annäherung ausgedruckt.<br>Diese Abwägung wie genau die Anpassung sein soll, um ein höchstmögliches Mass an Übereinstimmung mit den wahren Werten zu erreichen, wird uns im ML immer begleiten. <br>\n",
    "<b>Overfitting: übergenaue Optimierung mit Überbetonung von einzelnen Samples und schlechter genereller Anpassung<br>\n",
    "Underfitting: unzureichende Optimierung, Fehlersumme noch zu groß "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#mache Punktwolke\n",
    "rng = np.random.RandomState(10)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "#zeichne Punktwolke\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, y,color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "#mache Regression mit den Graden bei color_nums und den Farben bei colors\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylim((-3,1.5))\n",
    "colormap=np.empty(22,dtype=object)    \n",
    "color_nums=[2,3,6,10,15,19,21]\n",
    "colors=[\"orange\",\"blue\",\"green\",\"red\",\"turquoise\",\"black\",\"violet\"]\n",
    "count=0\n",
    "for i in color_nums:\n",
    "    colormap[i]=colors[count]\n",
    "    count+=1\n",
    "    \n",
    "#hier die eigentliche Regression    \n",
    "for i in [2,3,6,10,15,19,21]:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(i), \n",
    "                           LinearRegression()) #benutze die Pipeline um die Polynome zu machen und dann die Regression\n",
    "\n",
    "\n",
    "    poly_model.fit(x[:, np.newaxis], y) #berechne die beste Annäherung des Polynoms mit Grad i\n",
    "    \n",
    "    #drucke Werte für Coeffizienten aus\n",
    "    print(f\" R²:{poly_model.score(x[:, np.newaxis], y):4.3f} Grad {i}\")\n",
    "    \n",
    "    #predicte für jweils 1000 Werte zwischen 0 und 10 die Werte des Polynoms \n",
    "    xfit = np.linspace(0, 10, 1000)\n",
    "    yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "    plt.plot(xfit, yfit, color=colormap[i],alpha=.7,label=str(i));\n",
    "    \n",
    "plt.scatter(x, y,color=\"red\")\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe23e0",
   "metadata": {},
   "source": [
    "Wir haben bisher nur gezeigt, dass wir mit ML in der Lage sind, Annäherungen an Punktewolken durchzuführen. Erinnern wir uns an die Definitionen im Einführungskapitel geht es aber darum, nun neue Punkte (neue X-Werte) mit der optimalen Annäherung, die wir bei den hier gezeigten Lernvorgängen erreicht haben, vorauszusagen (bezüglich des zugehörigen Y-Wertes, den wir nicht haben!). Dazu später mehr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2943b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
