{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74d0ddb",
   "metadata": {},
   "source": [
    "## Gradientenabstiegsverfahren von Grund auf erklärt\n",
    "<img width=300 height=300 class=\"imgright\" src=\"Images/climbing1.png\" alt=\"restaurant\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36878d1",
   "metadata": {},
   "source": [
    "In unserem Beispiel der linearen Regression haben wir getestet, welche der Kombinationen von Änderungen von m (Steigung) und b (Schnittpunkt mit der y-Achse) jeweils zu einer Verminderung des Gesamtfehlers führen und haben schrittweise diese Parameter-Kombination optimiert. Wir hatten die Optimierung so durchgeführt, dass wir die Parameter jeweils um kleine Beträge verändert haben in plus- und minus-Richtung und dann jeweils den besten Wert für den entsprechenden Parameter im nächsten Optimierungsschritt verwendet haben. <br><br>Jetzt wollen wir ein schnelleres und gebräuchlicheres Verfahren besprechen, das Gradientenabstiegsverfahren, bei der die Gesamtfehlerfunktion nach den einzelnen Parametern differenziert wird und so das Fehlerminimum und die zugehörigen Parameterwerte bestimmt werden. Das Ziel ist wieder, die Parameter dann unabhängig voneinander so zu verändern, dass das Fehlerminimum gefunden wird. Dies wird durch kleine Schritte erreicht, deren Grösse von der Entfernung zum Minimum abhängen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98216cc2",
   "metadata": {},
   "source": [
    "Um unsere Überlegungen zu erleichtern, lassen wir jetzt wieder die Optimierung von b zunächst wieder einmal weg und beschränken uns auf das Modell mit alleiniger Optimierung von m, <b>b wird konstant auf 0 gesetzt</b>.\n",
    "Wir zeichnen nun zunächst eine Kurve, die uns den Gesamtfehler für unser System in Abhängigkeit von m angibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccd9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minimum Fehlerquadratsumme 15486.000000000042 bei m von  1.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "b=0\n",
    "\n",
    "def gesamtfehler(x_werte,y_werte,m):\n",
    "    sum_0=0\n",
    "    for i in range(len(x_werte)):\n",
    "            sum_0+=(m*x_werte[i]+b - y_werte[i])**2 \n",
    "    return sum_0\n",
    "\n",
    "\n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))\n",
    "\n",
    "    \n",
    "res_list,m_list=[],[]\n",
    "m=-5\n",
    "while m<6:\n",
    "    m_list.append(m)\n",
    "    res_list.append(gesamtfehler(x,y,m))\n",
    "    m+=.01\n",
    "plt.xlabel(\"m-Werte\", fontsize=10)                 \n",
    "plt.ylabel(\"Fehlerquadratsumme\", fontsize=10)\n",
    "plt.plot(m_list,res_list)\n",
    "plt.plot([-3.9,-1.8],[4e6,1.2e6]) # Beispiel Tangente\n",
    "\n",
    "#print(m_list.index(1.0399999999999383))\n",
    "print(f\" Minimum Fehlerquadratsumme {res_list[604]} bei m von {m_list[res_list.index(min(res_list))]:5.2f}\") #wo ist das Minimum?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75fdad9",
   "metadata": {},
   "source": [
    "Wir sehen hier eine glatte Kurve mit einem Minimum für m bei 1.04. Wollen wir diesen Punkt (x,y) für das Minimum von m in unserer Grafik finden, so wäre eine Möglichkeit, an jedem beliebigen Punkt der Kurve deren Steigung festzustellen, wie es die orange Linie (Tangente) an einer Stelle exemplarisch tut. Am Minimum der Kurve wäre dann die Steigung 0. Um diesen Punkt <b> mit schrittweiser Veränderung von m</b> zu finden, würde man sich bei negativer Steigung (wie dargestellt) einen Schritt zu grösseren m-Werten hin bewegen, bei positver Steigung umgekehrt. Man könnte zusätzlich noch die Grösse des Schritts von der Grösse der Steigung abhängig machen, je steiler, desto grösser der Schritt. Wir würden also das Minimum mittels Differentialrechnung finden. (Wir könnten hier im Falle nur eines Parameters natürlich direkt den Minimalwert berechnen, an dem die Steigung der Fehlerfunktion 0 ist. Bei Problemen mit mehreren (bis vielen) Parametern ist die Funktion für den Gesamtfehler aber meist sehr komplex, Einzelfehlerkurven für jeden der Parameter sind dann bei konstanten Werten für die anderen Parameter darstellbar (und hoffentlich differenzierbar), die Gesamtfehlerfunktion aber nur noch iterativ zu optimieren.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb30119",
   "metadata": {},
   "source": [
    "Um dies zu erreichen, müsste man die Fehlerkurve differenzieren. \n",
    "Die Gleichung für die Fehlerquadratsumme lautet:<br>\n",
    "<br> \n",
    "$$mittlereFehlerquadratsumme = 1/n\\sum_{i=1}^{n} (x_i*m+b-y_i)^2$$\n",
    "<br><br>\n",
    "für b = 0, <br><br>\n",
    "$$mittlereFehlerquadratsumme = 1/n\\sum_{i=1}^{n} (x_i*m-y_i)^2$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a20c4",
   "metadata": {},
   "source": [
    "<b>Leiten wir dieses nach m ab</b> (die x-Werte und y-Werte und damit auch der Einzelwert x[i] und y[i] sowie n stellen Konstanten dar) so erhält man (äussere Ableitung mal innere Ableitung):\n",
    "<br><br> \n",
    "$$dmittlereFehlerquadratsumme/dm = 2/n\\sum_{i=1}^{n} x_i * (x_i*m -y_i)$$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995baf2",
   "metadata": {},
   "source": [
    "Passen wir nun unser ursprüngliches Modell ohne Optimierung von b unserer neuen Methode an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237b1009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ende, keine Verbesserung mehr möglich  m =  1.04, b = 0 in Lauf 6 \n",
      "\n",
      "X-Wert:2.00 wahrer Y-Wert:30.00 vorausgesagter Y-Wert: 2.08\n",
      "X-Wert:3.00 wahrer Y-Wert:34.00 vorausgesagter Y-Wert: 3.12\n",
      "X-Wert:4.00 wahrer Y-Wert:35.00 vorausgesagter Y-Wert: 4.16\n",
      "X-Wert:5.00 wahrer Y-Wert:32.00 vorausgesagter Y-Wert: 5.21\n",
      "X-Wert:10.00 wahrer Y-Wert:32.00 vorausgesagter Y-Wert: 10.41\n",
      "X-Wert:11.00 wahrer Y-Wert:43.00 vorausgesagter Y-Wert: 11.45\n",
      "X-Wert:13.00 wahrer Y-Wert:26.00 vorausgesagter Y-Wert: 13.54\n",
      "X-Wert:14.00 wahrer Y-Wert:45.00 vorausgesagter Y-Wert: 14.58\n",
      "X-Wert:16.00 wahrer Y-Wert:35.00 vorausgesagter Y-Wert: 16.66\n",
      "X-Wert:17.00 wahrer Y-Wert:46.00 vorausgesagter Y-Wert: 17.70\n",
      "X-Wert:19.00 wahrer Y-Wert:44.00 vorausgesagter Y-Wert: 19.78\n",
      "X-Wert:20.00 wahrer Y-Wert:52.00 vorausgesagter Y-Wert: 20.82\n",
      "X-Wert:23.00 wahrer Y-Wert:52.00 vorausgesagter Y-Wert: 23.95\n",
      "X-Wert:24.00 wahrer Y-Wert:53.00 vorausgesagter Y-Wert: 24.99\n",
      "X-Wert:25.00 wahrer Y-Wert:56.00 vorausgesagter Y-Wert: 26.03\n",
      "X-Wert:28.00 wahrer Y-Wert:38.00 vorausgesagter Y-Wert: 29.15\n",
      "X-Wert:29.00 wahrer Y-Wert:40.00 vorausgesagter Y-Wert: 30.20\n",
      "X-Wert:33.00 wahrer Y-Wert:46.00 vorausgesagter Y-Wert: 34.36\n",
      "X-Wert:36.00 wahrer Y-Wert:48.00 vorausgesagter Y-Wert: 37.48\n",
      "X-Wert:37.00 wahrer Y-Wert:60.00 vorausgesagter Y-Wert: 38.53\n",
      "X-Wert:39.00 wahrer Y-Wert:48.00 vorausgesagter Y-Wert: 40.61\n",
      "X-Wert:40.00 wahrer Y-Wert:60.00 vorausgesagter Y-Wert: 41.65\n",
      "X-Wert:41.00 wahrer Y-Wert:61.00 vorausgesagter Y-Wert: 42.69\n",
      "X-Wert:44.00 wahrer Y-Wert:63.00 vorausgesagter Y-Wert: 45.81\n",
      "X-Wert:49.00 wahrer Y-Wert:66.00 vorausgesagter Y-Wert: 51.02\n",
      "X-Wert:50.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 52.06\n",
      "X-Wert:52.00 wahrer Y-Wert:62.00 vorausgesagter Y-Wert: 54.14\n",
      "X-Wert:56.00 wahrer Y-Wert:56.00 vorausgesagter Y-Wert: 58.31\n",
      "X-Wert:61.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 63.51\n",
      "X-Wert:62.00 wahrer Y-Wert:67.00 vorausgesagter Y-Wert: 64.56\n",
      "X-Wert:64.00 wahrer Y-Wert:61.00 vorausgesagter Y-Wert: 66.64\n",
      "X-Wert:68.00 wahrer Y-Wert:77.00 vorausgesagter Y-Wert: 70.80\n",
      "X-Wert:70.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 72.89\n",
      "X-Wert:74.00 wahrer Y-Wert:66.00 vorausgesagter Y-Wert: 77.05\n",
      "X-Wert:75.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 78.09\n",
      "X-Wert:76.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 79.13\n",
      "X-Wert:77.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 80.17\n",
      "X-Wert:78.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 81.22\n",
      "X-Wert:83.00 wahrer Y-Wert:78.00 vorausgesagter Y-Wert: 86.42\n",
      "X-Wert:84.00 wahrer Y-Wert:92.00 vorausgesagter Y-Wert: 87.46\n",
      "X-Wert:86.00 wahrer Y-Wert:78.00 vorausgesagter Y-Wert: 89.55\n",
      "X-Wert:88.00 wahrer Y-Wert:73.00 vorausgesagter Y-Wert: 91.63\n",
      "X-Wert:89.00 wahrer Y-Wert:91.00 vorausgesagter Y-Wert: 92.67\n",
      "X-Wert:90.00 wahrer Y-Wert:90.00 vorausgesagter Y-Wert: 93.71\n",
      "X-Wert:91.00 wahrer Y-Wert:81.00 vorausgesagter Y-Wert: 94.75\n",
      "X-Wert:95.00 wahrer Y-Wert:85.00 vorausgesagter Y-Wert: 98.92\n",
      "X-Wert:96.00 wahrer Y-Wert:88.00 vorausgesagter Y-Wert: 99.96\n",
      "X-Wert:97.00 wahrer Y-Wert:98.00 vorausgesagter Y-Wert: 101.00\n",
      "X-Wert:98.00 wahrer Y-Wert:91.00 vorausgesagter Y-Wert: 102.04\n",
      "X-Wert:100.00 wahrer Y-Wert:88.00 vorausgesagter Y-Wert: 104.12\n",
      "Fehlerquadratsumme: 15485.456558626669\n"
     ]
    }
   ],
   "source": [
    "def trainieren(x_werte,y_werte,anzahl_durchgänge,lernrate):\n",
    "    m=0\n",
    "    b=0\n",
    "    \n",
    "    for lauf in range(anzahl_durchgänge):\n",
    "        gradient_sum=0\n",
    "        for i in range(len(x_werte)):            \n",
    "            gradient_sum+=x_werte[i]*(x_werte[i]*m - y_werte[i]) #Ableitung Fehlerquadratsummenkurve\n",
    "        change= 2*-gradient_sum/len(x_werte) *lernrate         \n",
    "        if abs(change)<1e-3: #Kurvensteigung (Tangente) kleiner 1/1000 Änderung\n",
    "            print(f\"\\nEnde, keine Verbesserung mehr möglich  m = {m:5.2f}, b = 0 in Lauf {lauf} \\n\")\n",
    "            return m\n",
    "        else:\n",
    "            m += change \n",
    "        \n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "def voraussagen(m,x,y):\n",
    "    my_sum=0\n",
    "    for x_item,y_item in sorted(zip(x,y)):\n",
    "        print(f\"X-Wert:{x_item:4.2f} wahrer Y-Wert:{y_item:4.2f} vorausgesagter Y-Wert: {x_item*m:4.2f}\")\n",
    "        my_sum+=(y_item-x_item*m)**2\n",
    "    print (f\"Fehlerquadratsumme: {my_sum}\")\n",
    "    return\n",
    "\n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))#wir schreiben unsere Reservierungen und die Anzahl der Mahlzeiten in L\n",
    "\n",
    "   \n",
    "\n",
    "voraussagen(trainieren(x,y,1000,1e-4),x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f58bd",
   "metadata": {},
   "source": [
    "Wir erhalten nahezu identische Werte zu unserem Programm im letzten Kapitel, als wir auch nur m optimierten, aber dies bereits nach 6 Läufen und nicht nach 10419 Läufen bei gleicher Lernrate von 1e-4.\n",
    "Die Methode scheint also sehr effizient zu sein.\n",
    "Man beachte auch, dass die Änderung von m nicht mehr in festen Schritten erfolgt wie vorher. Die bei der Optimierung immer kleiner werdende Steigung der Fehlerquadratsumme wird mit der Lernrate multipliziert und dieses immer kleiner werdende Produkt als Änderung zu m addiert. Wir haben damit bei Annäherung an unseren Optimalwert automatisch immer kleinere Änderungsschritte.<br><br>\n",
    "Nun wollen wir die mittlere Fehlerquadratsumme der Geradengleichung für den Parameter b ermitteln, <b>m bleibt konstant</b>.\n",
    "<br><br>\n",
    "Um dies zu erreichen, müsste man die Fehlerkurve differenzieren. \n",
    "Die Gleichung für die Fehlerquadratsumme lautet:<br>\n",
    "<br> \n",
    "$$mittlereFehlerquadratsumme = 1/n\\sum_{i=1}^{n} (x_i*m+b-y_i)^2$$\n",
    "\n",
    "<br><br>\n",
    "für m = 0, konstant und willkürlich auf 0 gesetzt<br><br>\n",
    "$$mittlereFehlerquadratsumme = 1/n\\sum_{i=1}^{n} (b-y_i)^2$$<br>\n",
    "\n",
    "Dasselbe können wir auch für b durchführen und dabei m konstant halten.\n",
    "Die Ableitung der Fehlerquadratsumme nach b sieht folgendermassen aus:\n",
    "<br>\n",
    "$$dmittlereFehlerquadratsumme/db = 2/n\\sum_{i=1}^{n}  (b -y_i)$$<br>\n",
    "\n",
    "Hier die Kurven der Fehlerquadratsumme für m (b=0) und für b (m=0).<br><br>\n",
    "<img width=600 height=600 class=\"imgright\" src=\"Images/Fehlerquadratsumme_mb.png\"  />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5018060",
   "metadata": {},
   "source": [
    "\n",
    "Und unser Programm würde sich wie folgt verändern:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfeb6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ende, keine Verbesserung mehr möglich  m = 0.00 in Lauf 33 \n",
      "\n",
      " 61.557 errechneter b-Wert\n",
      "\n",
      " Für 30 Reservierungen auf der vorausgesagten Geraden:  61.56, Wert der Referenzgeraden: 49.3 \n",
      "\n",
      " Für 50 Reservierungen auf der vorausgesagten Geraden:  61.56, Wert der Referenzgeraden: 61.1 \n",
      "\n",
      " Für 80 Reservierungen auf der vorausgesagten Geraden:  61.56, Wert der Referenzgeraden: 78.7 \n",
      "\n",
      " Gesamtfehler 444.3251533808939\n"
     ]
    }
   ],
   "source": [
    "def trainieren(x_werte,y_werte,anzahl_durchgänge,lernrate):\n",
    "    m=0\n",
    "    b=0\n",
    "    for lauf in range(anzahl_durchgänge):\n",
    "        gradient_sum=0\n",
    "        for i in range(len(x_werte)):\n",
    "            gradient_sum+=(x_werte[i]*m+b - y_werte[i]) #m=0\n",
    "        change= 2*-gradient_sum/len(x_werte) *lernrate        \n",
    "        if abs(change)<1e-3:\n",
    "            print(f\"\\nEnde, keine Verbesserung mehr möglich  m ={m:5.2f} in Lauf {lauf} \\n\")\n",
    "            return b\n",
    "        else:\n",
    "            b += change\n",
    "        \n",
    "    return b\n",
    "\n",
    "def voraussagen(b):\n",
    "    fehler,Ergebnisse=0,{30:49.3,50:61.1,80:78.7}\n",
    "    print(f\" {b:2.3f} errechneter b-Wert\\n\")\n",
    "    for x,y in Ergebnisse.items():\n",
    "        print(f\" Für {x} Reservierungen auf der vorausgesagten Geraden: {b:6.2f}, Wert der Referenzgeraden: {y} \\n\")\n",
    "        fehler+=(b-y)**2\n",
    "    print(f\" Gesamtfehler {fehler}\")    \n",
    "\n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))#wir schreiben unsere Reservierungen und die Anzahl der Mahlzeiten in L\n",
    "\n",
    "   \n",
    "\n",
    "voraussagen(trainieren(x,y,1000,.13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67702fe2",
   "metadata": {},
   "source": [
    "Wir sehen hier bei noch grossem Gesamtfehler eine relativ geringe Anzahl von Läufen bis zum Optimum. Nachdem wir nun gezeigt haben, dass man die Optimierung für beide Einzelparameter durchführen kann, werden wir nun unsere beiden Verfahren kombinieren und erhalten dann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43a0569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ende, keine Verbesserung mehr möglich m =  0.59 b= 31.49 in Lauf 97 \n",
      "\n",
      "X-Wert:2.00 wahrer Y-Wert:30.00 vorausgesagter Y-Wert: 32.67\n",
      "X-Wert:3.00 wahrer Y-Wert:34.00 vorausgesagter Y-Wert: 33.26\n",
      "X-Wert:4.00 wahrer Y-Wert:35.00 vorausgesagter Y-Wert: 33.85\n",
      "X-Wert:5.00 wahrer Y-Wert:32.00 vorausgesagter Y-Wert: 34.44\n",
      "X-Wert:10.00 wahrer Y-Wert:32.00 vorausgesagter Y-Wert: 37.40\n",
      "X-Wert:11.00 wahrer Y-Wert:43.00 vorausgesagter Y-Wert: 37.99\n",
      "X-Wert:13.00 wahrer Y-Wert:26.00 vorausgesagter Y-Wert: 39.17\n",
      "X-Wert:14.00 wahrer Y-Wert:45.00 vorausgesagter Y-Wert: 39.76\n",
      "X-Wert:16.00 wahrer Y-Wert:35.00 vorausgesagter Y-Wert: 40.94\n",
      "X-Wert:17.00 wahrer Y-Wert:46.00 vorausgesagter Y-Wert: 41.53\n",
      "X-Wert:19.00 wahrer Y-Wert:44.00 vorausgesagter Y-Wert: 42.71\n",
      "X-Wert:20.00 wahrer Y-Wert:52.00 vorausgesagter Y-Wert: 43.30\n",
      "X-Wert:23.00 wahrer Y-Wert:52.00 vorausgesagter Y-Wert: 45.07\n",
      "X-Wert:24.00 wahrer Y-Wert:53.00 vorausgesagter Y-Wert: 45.66\n",
      "X-Wert:25.00 wahrer Y-Wert:56.00 vorausgesagter Y-Wert: 46.25\n",
      "X-Wert:28.00 wahrer Y-Wert:38.00 vorausgesagter Y-Wert: 48.02\n",
      "X-Wert:29.00 wahrer Y-Wert:40.00 vorausgesagter Y-Wert: 48.62\n",
      "X-Wert:33.00 wahrer Y-Wert:46.00 vorausgesagter Y-Wert: 50.98\n",
      "X-Wert:36.00 wahrer Y-Wert:48.00 vorausgesagter Y-Wert: 52.75\n",
      "X-Wert:37.00 wahrer Y-Wert:60.00 vorausgesagter Y-Wert: 53.34\n",
      "X-Wert:39.00 wahrer Y-Wert:48.00 vorausgesagter Y-Wert: 54.52\n",
      "X-Wert:40.00 wahrer Y-Wert:60.00 vorausgesagter Y-Wert: 55.11\n",
      "X-Wert:41.00 wahrer Y-Wert:61.00 vorausgesagter Y-Wert: 55.70\n",
      "X-Wert:44.00 wahrer Y-Wert:63.00 vorausgesagter Y-Wert: 57.47\n",
      "X-Wert:49.00 wahrer Y-Wert:66.00 vorausgesagter Y-Wert: 60.42\n",
      "X-Wert:50.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 61.01\n",
      "X-Wert:52.00 wahrer Y-Wert:62.00 vorausgesagter Y-Wert: 62.20\n",
      "X-Wert:56.00 wahrer Y-Wert:56.00 vorausgesagter Y-Wert: 64.56\n",
      "X-Wert:61.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 67.51\n",
      "X-Wert:62.00 wahrer Y-Wert:67.00 vorausgesagter Y-Wert: 68.10\n",
      "X-Wert:64.00 wahrer Y-Wert:61.00 vorausgesagter Y-Wert: 69.28\n",
      "X-Wert:68.00 wahrer Y-Wert:77.00 vorausgesagter Y-Wert: 71.64\n",
      "X-Wert:70.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 72.82\n",
      "X-Wert:74.00 wahrer Y-Wert:66.00 vorausgesagter Y-Wert: 75.18\n",
      "X-Wert:75.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 75.77\n",
      "X-Wert:76.00 wahrer Y-Wert:69.00 vorausgesagter Y-Wert: 76.37\n",
      "X-Wert:77.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 76.96\n",
      "X-Wert:78.00 wahrer Y-Wert:76.00 vorausgesagter Y-Wert: 77.55\n",
      "X-Wert:83.00 wahrer Y-Wert:78.00 vorausgesagter Y-Wert: 80.50\n",
      "X-Wert:84.00 wahrer Y-Wert:92.00 vorausgesagter Y-Wert: 81.09\n",
      "X-Wert:86.00 wahrer Y-Wert:78.00 vorausgesagter Y-Wert: 82.27\n",
      "X-Wert:88.00 wahrer Y-Wert:73.00 vorausgesagter Y-Wert: 83.45\n",
      "X-Wert:89.00 wahrer Y-Wert:91.00 vorausgesagter Y-Wert: 84.04\n",
      "X-Wert:90.00 wahrer Y-Wert:90.00 vorausgesagter Y-Wert: 84.63\n",
      "X-Wert:91.00 wahrer Y-Wert:81.00 vorausgesagter Y-Wert: 85.22\n",
      "X-Wert:95.00 wahrer Y-Wert:85.00 vorausgesagter Y-Wert: 87.58\n",
      "X-Wert:96.00 wahrer Y-Wert:88.00 vorausgesagter Y-Wert: 88.17\n",
      "X-Wert:97.00 wahrer Y-Wert:98.00 vorausgesagter Y-Wert: 88.76\n",
      "X-Wert:98.00 wahrer Y-Wert:91.00 vorausgesagter Y-Wert: 89.35\n",
      "X-Wert:100.00 wahrer Y-Wert:88.00 vorausgesagter Y-Wert: 90.54\n",
      "Fehlerquadratsumme: 51607.45139058406\n"
     ]
    }
   ],
   "source": [
    "def trainieren(x_werte,y_werte,anzahl_durchgänge,lernrate_m,lernrate_b):\n",
    "    m=0\n",
    "    b=0\n",
    "    for lauf in range(anzahl_durchgänge):\n",
    "        gradient_sum_b,gradient_sum_m=0,0\n",
    "        for i in range(len(x_werte)):\n",
    "            gradient_sum_b+=(x_werte[i]*m+b - y_werte[i])            \n",
    "            gradient_sum_m+= x_werte[i]*(x_werte[i]*m+b - y_werte[i])\n",
    "        change_b=2*-gradient_sum_b/len(x_werte) *lernrate_b\n",
    "        change_m=2*-gradient_sum_m/len(x_werte) *lernrate_m  \n",
    "        \n",
    "        if abs(change_b)<.01 and abs(change_m)<.01:\n",
    "            print(f\"\\nEnde, keine Verbesserung mehr möglich m = {m:5.2f} b= {b:5.2f} in Lauf {lauf} \\n\")\n",
    "            return m,b\n",
    "        else:\n",
    "            b += change_b\n",
    "            m += change_m            \n",
    "    return m,b\n",
    "\n",
    "def voraussagen(m,b,x,y):\n",
    "    my_sum=0\n",
    "    for x_item,y_item in sorted(zip(x,y)):\n",
    "        print(f\"X-Wert:{x_item:4.2f} wahrer Y-Wert:{y_item:4.2f} vorausgesagter Y-Wert: {x_item*m+b:4.2f}\")\n",
    "        my_sum+=(y_item-x_item*m)**2\n",
    "    print (f\"Fehlerquadratsumme: {my_sum}\")\n",
    "    return\n",
    "    \n",
    "x,y=[],[]    \n",
    "with open(\"Data/Reservierungen.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        x_roh,y_roh=line.rstrip().split(\",\")        \n",
    "        x.append(int(x_roh))#x und y kommt als String\n",
    "        y.append(int(y_roh))#wir schreiben unsere Reservierungen und die Anzahl der Mahlzeiten in L\n",
    "\n",
    "   \n",
    "\n",
    "voraussagen(*trainieren(x,y,1000,1e-4,1e-1),x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b61cb5",
   "metadata": {},
   "source": [
    "Dieses Verfahren läuft mit sehr schneller Annäherung ans Optimum.<b> Wichtig zu erwähnen ist, daß man davon ausgeht, dass die Gewichte voneinander unabhängig verändert werden können, um zum Fehlerminimum zu kommen. Diese Annahme ist nur zu halten, wenn die Lernrate genügend klein ist. Alle Gewichte werden dann um einen kleinen Betrag in die richtige Richtung verändert. Es könnte bei hohen Lernraten ein Gewicht zufällig die Oberhand gewinnen und das Ergebnis quasi allein bestimmen.</b>\n",
    "Ob es möglich ist, das Verfahren zu verwenden, hängt davon ab, ob die jeweilige Funktion für den Fehler (bei uns die Fehlerquadratsumme) glatt verläuft und keine Sprünge macht, sodass man sie differenzieren kann, und ob sie nur ein Minimum hat oder mehrere, von denen dann eines zufällig angesteuert wird, aber nicht zwingend das <b>absolute Minimum</b>. Für hohe Werte der Lernrate funktioniert unser Verfahren nicht. Der jeweilige Änderungsschritt der Parameter, der sich aus der Steigung der Fehlerfunktion ergibt, kann so groß ein, daß der neue Wert zu einem höheren Wert der Fehlerfunktion führt. Die Veränderung wird dann noch größer beim nächsten Schritt und dies kann zu sich ständig erhöhenden Fehlerwerten führen, wie unsere Grafik unten zeigt. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61495d",
   "metadata": {},
   "source": [
    "<img class=\"imgright\" src=\"Images/overshoot.png\" alt=\"overshoot\" />\n",
    "Hier führt jeder Schritt der Anpassung, weil er zu groß ist, zu immer schlechteren Werten auf der Fehlerfunktion.\n",
    "Man nennt dies \"overshoot\" oder überschwingen. Deshalb muss die Lernrate genügend klein sein und als Hyperparameter optimiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b121a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
