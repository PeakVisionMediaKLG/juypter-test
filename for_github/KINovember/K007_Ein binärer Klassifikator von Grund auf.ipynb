{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4bb003",
   "metadata": {},
   "source": [
    "# Ein binärer Klassifikator von Grund auf\n",
    "Nachdem wir bisher uns mit Regressionsproblemen beschäftigt haben, möchten wir jetzt einen binären Klassifikator aufbauen. Hierbei handelt es sich um ein maschinelles Lernsystem, welches als Label nur zwei Ergebnisse zulässt. Ein Beispiel wäre z.B. ein Autohändler, der abhängig vom Ankaufpreis, dem Baujahr und dem Neupreis eines Autos wissen möchte, ob er das Auto ankaufen soll oder nicht. Dies erschließt er aus den Erfahrungen von bisherigen Verkäufen. Wir hätten dann 3 Eingangsvariablen (oder auch ```Features```) genannt und als Label hätten wir \"Auto wurde gut verkauft\", ja oder nein. Wieder würden wir mit einigen Datensätzen unser System trainieren und dann mit einer Reihe unabhängiger Datensätze die Qualität unserer Prognosen testen.<br><br>\n",
    "<img width=300 height=300 class=\"imgright\" src=\"Images/carsales.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793cf59",
   "metadata": {},
   "source": [
    "Hierzu bekommen wir vom Händler diesen Datensatz von 50 Autos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b449d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankaufspreis    Baujahr       Neupreis     wurde verkauft\n",
      "    17376         2012         27982          True\n",
      "    18746         1995         28790          False\n",
      "    11788         2011         19423          True\n",
      "    16021         2013         24464          True\n",
      "    17609         2021         27054          True\n",
      "    25418         2015         35951          False\n",
      "    27035         2002         35659          False\n",
      "    28440         2006         39808          False\n",
      "    15481         2015         23066          True\n",
      "    13617         2006         19866          True\n",
      "    25126         2004         36969          False\n",
      "    11459         2001         19421          False\n",
      "    23170         2017         28383          False\n",
      "    27691         2020         35542          False\n",
      "    21934         2006         30645          False\n",
      "    25162         2009         36553          False\n",
      "    11046         2011         16644          True\n",
      "    15959         2008         27150          True\n",
      "    10416         2013         14904          True\n",
      "    15952         2018         27023          True\n",
      "    20603         1998         27102          False\n",
      "    15862         2021         27747          True\n",
      "    14841         2018         25693          True\n",
      "    28633         2000         33421          False\n",
      "    18690         2010         28984          True\n",
      "    14725         2021         17932          True\n",
      "    25422         1997         32738          False\n",
      "    13840         2010         18959          True\n",
      "    18189         2007         21237          True\n",
      "    27156         1995         30450          False\n",
      "    26636         2014         36106          False\n",
      "    22670         2009         34501          False\n",
      "    21243         1995         32119          False\n",
      "    24984         2013         31334          False\n",
      "    28732         2014         37139          False\n",
      "    20372         2015         30529          False\n",
      "    23991         2019         29293          False\n",
      "    10924         1997         14546          False\n",
      "    21671         2005         29929          False\n",
      "    20011         2001         26418          False\n",
      "    16037         2001         21668          False\n",
      "    24159         2010         33932          False\n",
      "    15035         2011         22557          True\n",
      "    16955         2017         27020          True\n",
      "    14819         1995         23492          False\n",
      "    27274         2014         32281          False\n",
      "    25686         2012         35427          False\n",
      "    20121         2008         27535          False\n",
      "    26464         2008         29716          False\n",
      "    19539         2013         30260          True\n",
      "    12990         2008         19115          True\n",
      "    17766         2011         22728          True\n",
      "    28357         1996         39825          False\n",
      "    17030         2004         22100          False\n",
      "    24570         2020         32915          False\n",
      "    27867         2020         37895          False\n",
      "    10151         2014         18553          True\n",
      "    15023         2012         26098          True\n",
      "    13706         2015         24419          True\n",
      "    12277         2014         19066          True\n",
      "    27561         2017         34789          False\n",
      "    10447         2021         17866          True\n",
      "    28332         2006         31437          False\n",
      "    13075         1997         22843          False\n",
      "    15212         2006         25440          True\n",
      "    11047         2007         18420          True\n",
      "    14725         2008         24871          True\n",
      "    24570         2003         30914          False\n",
      "    12486         2002         19015          False\n",
      "    12705         2000         19014          False\n",
      "    25009         2007         28963          False\n",
      "    17071         2000         22167          False\n",
      "    27393         2004         31204          False\n",
      "    19660         1998         23836          False\n",
      "    12805         2000         20572          False\n",
      "    14641         2017         22490          True\n",
      "    24257         2018         31653          False\n",
      "    14874         2016         24915          True\n",
      "    16768         1998         20249          False\n",
      "    29688         2008         33355          False\n",
      "    21756         2006         33442          False\n",
      "    13035         2002         18112          False\n",
      "    11814         2020         22463          True\n",
      "    11778         1997         19060          False\n",
      "    20475         1998         29050          False\n",
      "    24050         2013         31749          False\n",
      "    19111         2013         29411          True\n",
      "    17229         1999         23013          False\n",
      "    28265         1995         33933          False\n",
      "    28116         2021         32680          False\n",
      "    25639         2019         31573          False\n",
      "    19498         2019         27946          True\n",
      "    22521         2000         29276          False\n",
      "    18852         2008         27519          True\n",
      "    24111         2006         29453          False\n",
      "    12489         1996         21549          False\n",
      "    26094         2002         32112          False\n",
      "    14497         2016         26139          True\n",
      "    15691         2005         19732          False\n",
      "    21747         1995         31430          False\n",
      "    24499         1996         36006          False\n",
      "    26265         2011         31003          False\n",
      "    14351         2001         19050          False\n",
      "    23789         2012         33274          False\n",
      "    19632         2012         26695          True\n",
      "    29640         2002         35542          False\n",
      "    28439         2017         36956          False\n",
      "    24102         2008         30742          False\n",
      "    18242         2006         25690          True\n",
      "    20238         2021         25720          False\n",
      "    28880         1998         33816          False\n",
      "    10228         2007         15977          True\n",
      "    18870         2012         24918          True\n",
      "    10975         1996         14047          False\n",
      "    18829         2000         30792          False\n",
      "    15564         1996         26674          False\n",
      "    13263         1999         21193          False\n",
      "    10404         2003         21899          False\n",
      "    27066         1997         30709          False\n",
      "    23304         1998         34362          False\n",
      "    21203         2008         31330          False\n",
      "    19031         2003         23808          False\n",
      "    26197         2002         34418          False\n",
      "    25009         1996         31744          False\n",
      "    18387         2009         25262          True\n",
      "    20313         2002         27495          False\n",
      "    23407         2013         35278          False\n",
      "    12476         2020         17209          True\n",
      "    14972         2005         23150          False\n",
      "    12467         2009         20561          True\n",
      "    23340         1997         27235          False\n",
      "    25325         2020         28691          False\n",
      "    26741         2002         30257          False\n",
      "    21714         2004         26393          False\n",
      "    25979         2015         37123          False\n",
      "    17577         2011         24746          True\n",
      "    22767         2018         26967          False\n",
      "    16341         2020         20144          True\n",
      "    14155         2009         19798          True\n",
      "    13223         2018         16362          True\n",
      "    29863         2004         32950          False\n",
      "    13629         2002         20713          False\n",
      "    28836         2014         33489          False\n",
      "    24212         2002         29193          False\n",
      "    11185         2014         15224          True\n",
      "    18488         2003         27636          False\n",
      "    23962         2016         34541          False\n",
      "    19641         2010         26896          True\n",
      "    26421         2012         36884          False\n",
      "    22517         1998         31007          False\n",
      "    23428         2021         27142          False\n",
      "    10169         2014         15363          True\n",
      "    25251         2010         28383          False\n",
      "    29440         1998         35658          False\n",
      "    17808         2004         22676          False\n",
      "    19268         2008         30109          True\n",
      "    21522         2013         30795          False\n",
      "    24183         1998         31641          False\n",
      "    22384         2004         26905          False\n",
      "    10111         2005         19327          False\n",
      "    27936         2019         36232          False\n",
      "    26022         2018         31518          False\n",
      "    19627         2009         27060          True\n",
      "    16445         2001         19471          False\n",
      "    26215         2021         30345          False\n",
      "    21669         2005         28253          False\n",
      "    15911         2012         20515          True\n",
      "    14651         2008         25268          True\n",
      "    18214         2019         26407          True\n",
      "    10774         2000         15015          False\n",
      "    29012         1998         34227          False\n",
      "    27365         2008         36269          False\n",
      "    20316         2010         29173          False\n",
      "    13767         1997         21335          False\n",
      "    18494         2009         21995          True\n",
      "    27228         2003         33715          False\n",
      "    13507         2004         23404          False\n",
      "    27216         2020         34600          False\n",
      "    11614         2011         14902          True\n",
      "    24272         2013         34901          False\n",
      "    23156         2001         33490          False\n",
      "    24270         2017         27840          False\n",
      "    15951         2021         22321          True\n",
      "    12692         2011         23906          True\n",
      "    28053         2005         35839          False\n",
      "    10936         2006         18471          True\n",
      "    18254         2009         27780          True\n",
      "    18386         2005         21487          False\n",
      "    22825         1995         27175          False\n",
      "    24465         2007         28534          False\n",
      "    26861         1996         31772          False\n",
      "    24690         2007         33938          False\n",
      "    15502         2018         26284          True\n",
      "    12654         1998         20167          False\n",
      "    19443         2000         24963          False\n",
      "    20093         2000         30989          False\n",
      "    20553         2003         29415          False\n",
      "    20960         1999         29717          False\n",
      "    23786         2004         27709          False\n",
      "    20465         2017         23613          False\n",
      "    16997         2005         27147          False\n",
      "    22494         2005         27552          False\n",
      "    12986         1996         22836          False\n",
      "    28799         2001         38943          False\n",
      "    21982         2014         31690          False\n",
      "    28171         2020         31935          False\n",
      "    28041         1995         31315          False\n",
      "    18415         1997         28561          False\n",
      "    21084         2015         31539          False\n",
      "    20252         2017         31400          False\n",
      "    22072         2008         26327          False\n",
      "    20683         2004         28432          False\n",
      "    13234         2021         23723          True\n",
      "    13473         1998         18958          False\n",
      "    17249         2017         25551          True\n",
      "    10286         2003         16043          False\n",
      "    21545         2020         28251          False\n",
      "    25188         2016         31278          False\n",
      "    11659         1997         20241          False\n",
      "    21239         2017         27995          False\n",
      "    25917         2006         29562          False\n",
      "    23657         2013         28017          False\n",
      "    17345         2011         27577          True\n",
      "    14158         2011         24098          True\n",
      "    14806         2005         17818          False\n",
      "    16874         2021         25772          True\n",
      "    25159         1996         30908          False\n",
      "    15365         2004         20795          False\n",
      "    26581         1999         35252          False\n",
      "    25274         2006         35507          False\n",
      "    19565         2007         24463          True\n",
      "    28899         2008         32377          False\n",
      "    27235         2017         31828          False\n",
      "    14736         2014         24484          True\n",
      "    18941         1996         28660          False\n",
      "    24375         1998         34562          False\n",
      "    10586         2012         14885          True\n",
      "    16329         1996         19847          False\n",
      "    13447         2020         21243          True\n",
      "    22821         1997         34634          False\n",
      "    27552         2019         37579          False\n",
      "    24622         2006         29423          False\n",
      "    20735         2013         32502          False\n",
      "    28463         1998         35490          False\n",
      "    28791         2007         40222          False\n",
      "    28536         2019         33180          False\n",
      "    18348         2006         27545          True\n",
      "    20209         2001         28675          False\n",
      "    16480         2007         27505          True\n",
      "    12741         2016         24150          True\n",
      "    13936         2014         23226          True\n",
      "    15523         2006         19716          True\n",
      "    19571         2016         22718          True\n",
      "    24924         2006         31206          False\n",
      "    19147         1997         26472          False\n",
      "    12260         2008         19707          True\n",
      "    19246         2009         25156          True\n",
      "    25809         1996         32080          False\n",
      "    15139         2003         22343          False\n",
      "    12640         2012         17121          True\n",
      "    10074         2004         20971          False\n",
      "    14135         2003         18102          False\n",
      "    20412         1999         24884          False\n",
      "    21890         2008         31064          False\n",
      "    27416         2014         37123          False\n",
      "    26653         2016         33939          False\n",
      "    13950         2021         22368          True\n",
      "    20658         2007         32001          False\n",
      "    11940         1995         22954          False\n",
      "    14724         2018         18486          True\n",
      "    11009         2011         22878          True\n",
      "    20188         2002         30682          False\n",
      "    21423         2017         32402          False\n",
      "    23065         2012         26481          False\n",
      "    22789         2015         27740          False\n",
      "    28730         2004         32601          False\n",
      "    19528         2004         24432          False\n",
      "    20900         2003         32589          False\n",
      "    26762         2008         29995          False\n",
      "    17052         2000         28002          False\n",
      "    11395         2021         23027          True\n",
      "    18904         1999         22752          False\n",
      "    21062         2008         31930          False\n",
      "    28472         1995         33111          False\n",
      "    10671         2017         19475          True\n",
      "    16144         2015         27199          True\n",
      "    18702         2009         27126          True\n",
      "    13035         2017         17176          True\n",
      "    23099         2005         34620          False\n",
      "    14293         2008         18179          True\n",
      "    18913         2000         26906          False\n",
      "    11982         1995         19819          False\n",
      "    13710         2009         22348          True\n",
      "    16458         2016         21222          True\n",
      "    16774         2016         27151          True\n",
      "    10321         2016         19500          True\n",
      "    14061         2019         19597          True\n",
      "    25140         2013         30775          False\n",
      "    17353         2014         25990          True\n",
      "    13423         2010         24156          True\n",
      "300 Autos\n",
      "mittlerer Ankaufspreis:  118542.68\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data/Autos3.txt\",\"r\") as fh:\n",
    "    print(\"Ankaufspreis    Baujahr       Neupreis     wurde verkauft\")\n",
    "    counter,sum_apreis=0,0\n",
    "    for line in fh:\n",
    "        rawlist=line.rstrip().split(\",\")\n",
    "        counter+=1\n",
    "        print(f\" {int(rawlist[0]):8d}       {int(rawlist[1]):6d}      {int(rawlist[2]):8d}          {bool(int((rawlist[3])))}\")\n",
    "        sum_apreis+=int(rawlist[0])\n",
    "print(counter, \"Autos\")\n",
    "print(f\"mittlerer Ankaufspreis:  {sum_apreis/50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47477687",
   "metadata": {},
   "source": [
    "Diesen teilen wir auf in einen Teildatensatz von 200 Autos, den der Klassifikator zum trainieren braucht und 100 Autos, bei denen wir die Qualität des Systems testen wollen.<br>\n",
    "Wir wollen unser System so aufsetzen, dass wir als Eingabe für den Klassifikator unsere 3 Eingangsvariablen (x1,x2,x3) verwenden und sie mit den \"Gewichten\" (w1,w2,w3)  multiplizieren als Linearkombination. Also:<br>\n",
    "<br>$$y_{vorausgesagt} = w1 * x1 + w2 * x2 + w3 * x3...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d64a",
   "metadata": {},
   "source": [
    "Im Gegensatz zu unseren Regressionsprogrammen haben wir hier nur True und False als Labels.\n",
    "Wollen wir die Daten zum Beispiel als Zusammenhang zwischen Ankaufspreis (x1) und Label (als 0 oder 1) darstellen, erhalten wir folgendes Bild.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674ccb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean 19757.113333333335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ecbafdef40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "with open(\"Data/Autos3.txt\",\"r\") as fh:\n",
    "    #print(\"Ankaufspreis    Baujahr       Neupreis     wurde verkauft\")\n",
    "    \n",
    "    L=[]\n",
    "    x0,x1,x2,x3,y=[],[],[],[],[]\n",
    "    for line in fh:\n",
    "        rawlist=line.rstrip().split(\",\")\n",
    "        L.append(rawlist)\n",
    "        \n",
    "    for line in L:\n",
    "        \n",
    "        x0.append(float(line[0]))#Ankaufspreis        \n",
    "        y.append(int(line[3]))\n",
    "mean=sum(x0)/len(x0)\n",
    "print(f\" Mean {mean}\")\n",
    "plt.xlabel(\"Ankaufspreis\")\n",
    "plt.ylabel(\"1->verkauft, 0-> nicht verkauft\")\n",
    "plt.scatter(x0,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55901a71",
   "metadata": {},
   "source": [
    "Wir hatten ja bei unserem Regressions-Lernprogramm betont, dass wir von einem annähernd linearen Zusammenhang zwischen den Features und den Labeln ausgehen. Dies ist hier offensichtlich überhaupt nicht der Fall. Eine solche Gerade hier einzuzeichnen ist völlig sinnlos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534f4c8",
   "metadata": {},
   "source": [
    "In diesem Fall arbeitet man beim maschinellen Lernen gerne mit der sogenannten Sigmoid-Funktion <br>\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n",
    "Diese hat einen Wertebereich zwischen 0 und 1. Dazwischen gibt es einen glatten Übergang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0375a",
   "metadata": {},
   "source": [
    "<br><br><img width=400 height=400 class=\"imgright\" src=\"Images/sigmoid.png\"><br><br>\n",
    "\n",
    "Für uns sähe eine Sigmoidfunktion (1-sigmoid) für unsere Werte z.B. den Ankaufspreis wie unten aus, sie spiegelt die Verteilung wesentlich besser.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e29c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean 19757.113333333335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ecbb32ddc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "with open(\"Data/Autos3.txt\",\"r\") as fh:\n",
    "    #print(\"Ankaufspreis    Baujahr       Neupreis     wurde verkauft\")\n",
    "    \n",
    "    L=[]\n",
    "    x0,x1,x2,x3,y=[],[],[],[],[]\n",
    "    for line in fh:\n",
    "        rawlist=line.rstrip().split(\",\")\n",
    "        L.append(rawlist)\n",
    "        \n",
    "    for line in L:\n",
    "        \n",
    "        x0.append(float(line[0]))#Ankaufspreis        \n",
    "        y.append(int(line[3]))\n",
    "mean=sum(x0)/len(x0)\n",
    "print(f\" Mean {mean}\")\n",
    "plt.xlabel(\"Ankaufspreis\")\n",
    "plt.ylabel(\"1->verkauft, 0-> nicht verkauft\")\n",
    "plt.scatter(x0,y)  \n",
    "plt.scatter([x for x in range(10000,30000,10)],[1/(1+math.e**(x/100-160)) for x in range(10000,30000,10)],s=1,marker=\"x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271600e",
   "metadata": {},
   "source": [
    "Wenn wir uns vorstellen, dass wir die Werte unserer Linearkombination der Eingangswerte<br><br>\n",
    "$$y_{vorausgesagt} = w1 * x1 + w2 * x2 + w3 * x3...$$<br>\n",
    "in diese Sigmoid-Funktion eingeben würden, hätten wir nur Werte zwischen 0 und 1.<br>\n",
    "Wir würden also als Fehlerfunktion<br><br>\n",
    "$$Fehler = sigmoid(w1 * x1 + w2 * x2 + w3 * x3... - ytrue)$$ <br> verwenden, wobei ytrue der wahre Label (0 oder 1) wäre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522a089",
   "metadata": {},
   "source": [
    "Um unser Gradientenabstiegs-Verfahren anwenden zu können, müssen wir unsere Fehlerfunktion nach den Gewichten w1,w2... differenzieren, und wir müssen prüfen, ob unsere vorherige Formel hierzu geeignet ist, da eine völlig andere Funktion vorliegt. Würden wir versuchen würden, weiterhin mit unserer quadratischen Fehlersumme zu arbeiten mit<br><br>\n",
    "$$Fehlersumme = \\sum_{i=1}^{n} sigmoid(y1_i * w1 + y2_i * w2 ... - ytrue_i))^2 $$<br> ergäbe sich zum Beispiel eine Fläche wie diese, wenn wir den Fehler für w1 und w2 darstellen. <br><img width=600 height=600 class=\"imgright\" src=\"Images/w2fehler.png\">\n",
    "    \n",
    "Wenn wir hier zum Beispiel nach w1 auftragen mit  w2 = 2, ergäbe sich:<br><br><img width=600 height=600 class=\"imgright\" src=\"Images/w1square.png\"><br><br>Von einer glatten differenzierbaren Kurve kann damit nicht die Rede sein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c2f2f",
   "metadata": {},
   "source": [
    "Wir müssen also überlegen, wie wir unsere Fehlerkurve differenzierbar machen und die Sigmoid-Funktion nutzen können, die Fehlerquadratmethode zusammen mit der Sigmoid-Funktion ist sicher nicht geeignet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddd7c9",
   "metadata": {},
   "source": [
    "Unsere neue Fehlerfunktion soll nur <b>ein</b> eindeutiges Minimum haben und eine möglichst glatte Oberfläche, damit das Gradientenabstiegsverfahren zu diesem Minimum führt. Dafür gibt es viele Möglichkeiten, beim maschinellen Lernen wird hierbei aber meist die logistische Verlustfunktion verwendet. Dabei soll ```yv``` der vorhergesagte y-Wert sein und ```y``` der wahre Label.<br><br>\n",
    "$$Fehlersumme = \\sum_{i=1}^{n} (y_i * log (yv_i) + (1 - y_i) * log (1 - yv_i)) $$<br>\n",
    "\n",
    "Diese Funktion sieht vielleicht zunächst wild aus, ist aber leicht zu berechnen und gibt uns eine Fehlerfunktion, die den obigen Kriterien entspricht. Wir sehen ein klares Minimum und die Oberfläche ist glatt und ohne lokale Minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc857a",
   "metadata": {},
   "source": [
    "<img width=600 height=600 class=\"imgright\" src=\"Images/Logloss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423c46f",
   "metadata": {},
   "source": [
    "Es ergibt sich nun die Frage, wie die Ableitung der Fehlerfunktion für ein bestimmtes Gewicht w aussieht. \n",
    "Hier ergibt sich:<br><br>\n",
    "$$dmittlereFehlersumme/dw =  1/n\\sum_{i=1}^{n} x_i * (yv_i- y_i)  $$<br><br>\n",
    "Unsere Fehlerfunktion lässt sich also auch einfach ableiten. Wir adaptieren unser Programm für multidimensionale Regression auf unsere neue Fragestellung mit der neuen Fehlerfunktion. Im Programm sehen wir, dass wir bei \"trainieren\" unser Gradientabstiegsverfahren für alle Gewichte w1,w2,w3 anwenden. Wir haben auch noch ein zusätzliches Gewicht w4 eingeführt, das wir mit einem konstanten Wert 1 für alle Datensätze multiplizieren. Dies entspricht dem Bias aus Kapitel 5, den wir auch hier anwenden wollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3db2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 1 Eingabedaten: 0falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "-26.534581177898517 108.8958564815457 3.997655649963864 0.03590388250217234\n",
      " Accuracy  0.64\n",
      "64 von 100 richtig\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math as m \n",
    "import random as rd\n",
    "rd.seed(0)\n",
    "def sigmoid(x):\n",
    "    if x<-40: \n",
    "        return 0.0 #sonst overflow error durch zu kleine Werte z.B. 1E-530\n",
    "        \n",
    "    return 1/(1+m.exp(-x))\n",
    "\n",
    "def vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4):\n",
    "    yvor=[]\n",
    "    for i in range(len(x1)):\n",
    "        yvor.append(sigmoid(x1[i]*w1+x2[i]*w2+x3[i]*w3+x4[i]*w4))\n",
    "    return yvor\n",
    "\n",
    "def verlust(yvor,y):    \n",
    "    sum_loss=0\n",
    "    for i in range(len(y)):\n",
    "        sum_loss-=y[i]* m.log(yvor[i])+(1-y[i])*m.log(1-yvor[i])  #hier die logloss Funktion zur Fehlerberechnung \n",
    "    return sum_loss/len(y)\n",
    "\n",
    "def gradient(yvor,y,x1,x2,x3,x4):   #Ableitung der logloss Funktion zur Gradientbestimmung \n",
    "    sum1,sum2,sum3,sum4=0,0,0,0\n",
    "    for  i in range(len(x1)):\n",
    "        sum1-=(yvor[i]-y[i])*x1[i]\n",
    "        sum2-=(yvor[i]-y[i])*x2[i]\n",
    "        sum3-=(yvor[i]-y[i])*x3[i]\n",
    "        sum4-=(yvor[i]-y[i])*x4[i]\n",
    "    return sum1,sum2,sum3,sum4\n",
    "\n",
    "def trainieren(x1,x2,x3,x4,y,reps,lernrate):\n",
    "    w1,w2,w3,w4=0,0,0,0\n",
    "    for i in range(reps):\n",
    "        yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)\n",
    "        wlist=gradient(vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4),y,x1,x2,x3,x4)        \n",
    "        w1+=wlist[0]/len(x1)*lernrate\n",
    "        w2+=wlist[1]/len(x1)*lernrate\n",
    "        w3+=wlist[2]/len(x1)*lernrate\n",
    "        w4+=wlist[3]/len(x1)*lernrate        \n",
    "    return w1,w2,w3,w4\n",
    "    \n",
    "def test(x1,x2,x3,x4,y,w1,w2,w3,w4):    \n",
    "    yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)    \n",
    "    richtig=0\n",
    "    for i in range(len(x1)):\n",
    "        print(f\"Klassifikator: {round(yvor[i])} Eingabedaten: {y[i]}\",end=\"\")\n",
    "        if round(yvor[i])==y[i]:\n",
    "            print(\"  richtig klassifiziert\")\n",
    "            richtig+=1\n",
    "        else:\n",
    "            print(\"falsch\")\n",
    "    return richtig\n",
    "    \n",
    "    \n",
    "Examples=[]\n",
    "with open(\"Data/Autos3.txt\", \"r\") as fh:    \n",
    "    for line in fh:        \n",
    "        x1,x2,x3,y=line.rstrip().split(\",\") \n",
    "        Examples.append([x1,x2,x3,1,y]) #hier der konstante Wert 1 für den sog. Bias\n",
    "        \n",
    "x1,x2,x3,x4,y=[],[],[],[],[]\n",
    "for elem in Examples: \n",
    "    x1.append(int(elem[0])) #x1\n",
    "    x2.append(int(elem[1])) #x2\n",
    "    x3.append(int(elem[2])) #x3\n",
    "    x4.append(int(elem[3])) #Bias\n",
    "    y.append(int(elem[4])) #Label\n",
    "x1tr,x2tr,x3tr,x4tr,ytr=[],[],[],[],[]\n",
    "\n",
    "for i in range(200):\n",
    "    num=rd.randint(0,299-i)\n",
    "    x1tr.append(x1[num])\n",
    "    x2tr.append(x2[num])\n",
    "    x3tr.append(x3[num])\n",
    "    x4tr.append(x4[num])\n",
    "    ytr.append(y[num])\n",
    "    x1.remove(x1[num])\n",
    "    x2.remove(x2[num])\n",
    "    x3.remove(x3[num])\n",
    "    x4.remove(x4[num])\n",
    "    y.remove(y[num])\n",
    "    \n",
    "    \n",
    "w1,w2,w3,w4=trainieren(x1tr,x2tr,x3tr,x4tr,ytr,30000,0.001)\n",
    "richtige=test(x1,x2,x3,x4,y,w1,w2,w3,w4)\n",
    "print(w1,w2,w3,w4)\n",
    "print(f\" Accuracy {richtige/100:5.2f}\")\n",
    "print(f\"{richtige} von 100 richtig\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6bbe2",
   "metadata": {},
   "source": [
    " Dieser Klassifizierer erkennt nur 64 von 100 Testdaten richtig, ein zu erwartendes Ergebnis für so wenige Trainingsdatensätze. Wir sehen auch die Endwerte für unsere 4 Gewichte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bb03a",
   "metadata": {},
   "source": [
    "In unserem Minibeispiel können wir es uns noch einfacher machen und immer dann, wenn der Wert der Summe<br><br> $$x1_i* w1 + x2_i*w2 + x3_i*w3 + w4  $$<br><br> kleiner als .5 ist, den Vorhersagewert auf 0 setzen und im anderen Fall auf 1. Wir würden dann eine solche Stufenfunktion für den Vorhersagewert verwenden.<br><img width=600 height=600 class=\"imgright\" src=\"Images/Stufe.png\"><br>\n",
    "Unsere Fehlersumme würden wir für jeden unserer Eingabewerte x1,x2 und x3 so verwenden.<br><br>\n",
    "$$Fehlersumme für x1 =  \\sum_{i=1}^{n} x1_i * (yv_i- y_i)  $$<br><br> \n",
    "Unsere Gewichte würden wir in jedem Schritt so verändern:(z.B. w1)\n",
    "<br><br>\n",
    "$$w1_{neu} = w1_{alt} + \\sum_{i=1}^{n} x1_i * (yv_i- y_i) / n * lernrate  $$<br><br> \n",
    "also:<br><br>\n",
    "$$w1_{neu} = w1_{alt} + Fehlersumme für x1 / n * lernrate $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a17f25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 1 Eingabedaten: 0  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 1 Eingabedaten: 0  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 1 Eingabedaten: 0  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "Klassifikator: 0 Eingabedaten: 1  falsch\n",
      "-27.10670499997429 232.57753500009423 5.435174999995704 0.09731999999997604\n",
      " Accuracy  0.65\n",
      "65 von 100 richtig\n"
     ]
    }
   ],
   "source": [
    "import math as m \n",
    "import random as rd\n",
    "rd.seed(0)\n",
    "def sigmoid(x):\n",
    "    if x<-40: \n",
    "        return 0.0 #sonst overflow error durch zu kleine Werte z.B. 1E-530\n",
    "        \n",
    "    return 1/(1+m.exp(-x))\n",
    "\n",
    "def vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4):\n",
    "    yvor=[]\n",
    "    for i in range(len(x1)):\n",
    "        yvor.append(x1[i]*w1+x2[i]*w2+x3[i]*w3+x4[i]*w4)  \n",
    "    yvor_min=min(yvor)\n",
    "    yvor_max=max(yvor)\n",
    "    border=(yvor_max-yvor_min)/2 #Stufenfunktion\n",
    "    for i in range(len(x1)):\n",
    "        if yvor[i]>=border:\n",
    "            yvor[i]=1.0\n",
    "        else:\n",
    "            yvor[i]=0.0\n",
    "    return yvor\n",
    "\n",
    "def gradient(yvor,y,x1,x2,x3,x4):\n",
    "    sum1,sum2,sum3,sum4=0,0,0,0\n",
    "    for  i in range(len(x1)):\n",
    "        sum1-=(yvor[i]-y[i])*x1[i]\n",
    "        sum2-=(yvor[i]-y[i])*x2[i]\n",
    "        sum3-=(yvor[i]-y[i])*x3[i]\n",
    "        sum4-=(yvor[i]-y[i])*x4[i]\n",
    "    return sum1,sum2,sum3,sum4\n",
    "\n",
    "def trainieren(x1,x2,x3,x4,y,reps,lernrate):\n",
    "    w1,w2,w3,w4=0,0,0,0    \n",
    "    for i in range(reps):\n",
    "        yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)        \n",
    "        wlist=gradient(vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4),y,x1,x2,x3,x4)        \n",
    "        w1+=wlist[0]/len(x1)*lernrate\n",
    "        w2+=wlist[1]/len(x1)*lernrate\n",
    "        w3+=wlist[2]/len(x1)*lernrate\n",
    "        w4+=wlist[3]/len(x1)*lernrate        \n",
    "    return w1,w2,w3,w4\n",
    "    \n",
    "def test(x1,x2,x3,x4,y,w1,w2,w3,w4):    \n",
    "    yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)    \n",
    "    richtig=0    \n",
    "    for i in range(len(x1)):\n",
    "        print(f\"Klassifikator: {round(yvor[i])} Eingabedaten: {y[i]}\",end=\"\")\n",
    "        if round(yvor[i])==y[i]:\n",
    "            print(\"  richtig klassifiziert\")\n",
    "            richtig+=1\n",
    "        else:\n",
    "            print(\"  falsch\")\n",
    "    return richtig\n",
    "    \n",
    "    \n",
    "Examples=[]\n",
    "with open(\"Data/Autos3.txt\", \"r\") as fh:    \n",
    "    for line in fh:        \n",
    "        x1,x2,x3,y=line.rstrip().split(\",\") \n",
    "        Examples.append([x1,x2,x3,1,y])\n",
    "        \n",
    "x1,x2,x3,x4,y=[],[],[],[],[]\n",
    "for elem in Examples: \n",
    "    x1.append(int(elem[0])) #x1\n",
    "    x2.append(int(elem[1])) #x2\n",
    "    x3.append(int(elem[2])) #x3\n",
    "    x4.append(int(elem[3])) #Bias\n",
    "    y.append(int(elem[4])) #Label\n",
    "x1tr,x2tr,x3tr,x4tr,ytr=[],[],[],[],[]\n",
    "\n",
    "for i in range(200):\n",
    "    num=rd.randint(0,299-i)\n",
    "    x1tr.append(x1[num])\n",
    "    x2tr.append(x2[num])\n",
    "    x3tr.append(x3[num])\n",
    "    x4tr.append(x4[num])\n",
    "    ytr.append(y[num])\n",
    "    x1.remove(x1[num])\n",
    "    x2.remove(x2[num])\n",
    "    x3.remove(x3[num])\n",
    "    x4.remove(x4[num])\n",
    "    y.remove(y[num])\n",
    "    \n",
    "    \n",
    "w1,w2,w3,w4=trainieren(x1tr,x2tr,x3tr,x4tr,ytr,30000,0.001)\n",
    "richtige=test(x1,x2,x3,x4,y,w1,w2,w3,w4)\n",
    "print(w1,w2,w3,w4)\n",
    "print(f\" Accuracy {richtige/100:5.2f}\")\n",
    "print(f\"{richtige} von 100 richtig\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a3bec6d",
   "metadata": {},
   "source": [
    "Wir haben jetzt ohne die Sigmoidfunktion und ohne die logarithmische Verlustfunktion sogar 65 von 100 Testdatensätzen richtig klassifiziert. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f8d8e",
   "metadata": {},
   "source": [
    "Interessant ist der Blick auf die Gewichte nach dem Training.<br>\n",
    "\n",
    "w1 = -27.1 <br>\n",
    "w2 =  232.6 <br>\n",
    "w3 =  5.4<br> \n",
    "w4 = 0.097<br><br>\n",
    "Wir sehen hier, dass der Verkaufserfolg negativ mit dem Ankaufspreis zusammenhängt (Gewicht -27.1), während das Baujahr den Verkaufserfolg stark positiv beeinflusst (Gewicht 232) und der Neupreis nur schwach positiv mit dem Verkaufserfolg zusammenhängt, aber dies schwächer (5.4).\n",
    "Aber haben die Gewichte nicht auch etwas mit der Grösse des Eingabewertes zu tun? Was wäre, wenn wir die Werte alle in den gleichen Bereich normieren würden, also sowohl die Preise als auch das Baujahr auf Werte zwischen 0 und 1 im Verhältnis zum Maximum verändern würden? Denn die Baujahre schwanken nur zwischen 1995 und 2021, während z.B. der Ankaufspreis sich zwischen 10000 und 30000 Euro bewegt. Das Ergebnis zeigt das untenstehende Programm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94de838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 0  richtig klassifiziert\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "Klassifikator: 0 Eingabedaten: 1falsch\n",
      "-0.4403269015760592 0.014523525183625678 -0.17269379578415098 -0.5157291386645767\n",
      " Accuracy  0.70\n",
      "70 von 100 richtig\n"
     ]
    }
   ],
   "source": [
    "import math as m \n",
    "import random as rd\n",
    "rd.seed(0)\n",
    "\n",
    "def vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4):\n",
    "    yvor=[]\n",
    "    for i in range(len(x1)):\n",
    "        yvor.append(sigmoid(x1[i]*w1+x2[i]*w2+x3[i]*w3+x4[i]*w4))\n",
    "    return yvor\n",
    "\n",
    "def gradient(yvor,y,x1,x2,x3,x4):\n",
    "    sum1,sum2,sum3,sum4=0,0,0,0\n",
    "    for  i in range(len(x1)):\n",
    "        sum1-=(yvor[i]-y[i])*x1[i]\n",
    "        sum2-=(yvor[i]-y[i])*x2[i]\n",
    "        sum3-=(yvor[i]-y[i])*x3[i]\n",
    "        sum4-=(yvor[i]-y[i])*x4[i]\n",
    "    return sum1,sum2,sum3,sum4\n",
    "\n",
    "def trainieren(x1,x2,x3,x4,y,reps,lernrate):\n",
    "    w1,w2,w3,w4=0,0,0,0\n",
    "    for i in range(reps):\n",
    "        yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)        \n",
    "        wlist=gradient(vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4),y,x1,x2,x3,x4)        \n",
    "        w1+=wlist[0]/len(x1)*lernrate\n",
    "        w2+=wlist[1]/len(x1)*lernrate\n",
    "        w3+=wlist[2]/len(x1)*lernrate\n",
    "        w4+=wlist[3]/len(x1)*lernrate        \n",
    "    return w1,w2,w3,w4\n",
    "    \n",
    "def test(x1,x2,x3,x4,y,w1,w2,w3,w4):    \n",
    "    yvor=vorhersagen(x1,x2,x3,x4,w1,w2,w3,w4)    \n",
    "    richtig=0\n",
    "    for i in range(len(x1)):\n",
    "        print(f\"Klassifikator: {round(yvor[i])} Eingabedaten: {y[i]}\",end=\"\")\n",
    "        if round(yvor[i])==y[i]:\n",
    "            print(\"  richtig klassifiziert\")\n",
    "            richtig+=1\n",
    "        else:\n",
    "            print(\"falsch\")\n",
    "    return richtig\n",
    "    \n",
    "    \n",
    "Examples=[]\n",
    "with open(\"Data/Autos3.txt\", \"r\") as fh:    \n",
    "    for line in fh:        \n",
    "        x1,x2,x3,y=line.rstrip().split(\",\") \n",
    "        Examples.append([x1,x2,x3,1,y])\n",
    "        \n",
    "x1,x2,x3,x4,y=[],[],[],[],[]\n",
    "    \n",
    "for elem in Examples: \n",
    "    x1.append(int(elem[0])) #x1\n",
    "    x2.append(int(elem[1])) #x2\n",
    "    x3.append(int(elem[2])) #x3\n",
    "    x4.append(int(elem[3])) #Bias\n",
    "    y.append(int(elem[4])) #y_werte\n",
    "\n",
    "##############normieren###########################\n",
    "x1max,x2max,x3max=max(x1),max(x2),max(x3)\n",
    "x1min,x2min,x3min=min(x1),min(x2),min(x3) \n",
    "for i in range(len(x1)):\n",
    "    x1[i]=(x1[i]-x1min)/x1max \n",
    "    x2[i]=(x2[i]-x2min)/x2max\n",
    "    x3[i]=(x3[i]-x3min)/x3max\n",
    "###################################################\n",
    "\n",
    "x1tr,x2tr,x3tr,x4tr,ytr=[],[],[],[],[]\n",
    "\n",
    "for i in range(200):\n",
    "    num=rd.randint(0,299-i)\n",
    "    x1tr.append(x1[num])\n",
    "    x2tr.append(x2[num])\n",
    "    x3tr.append(x3[num])\n",
    "    x4tr.append(x4[num])\n",
    "    ytr.append(y[num])\n",
    "    x1.remove(x1[num])\n",
    "    x2.remove(x2[num])\n",
    "    x3.remove(x3[num])\n",
    "    x4.remove(x4[num])\n",
    "    y.remove(y[num])\n",
    "    \n",
    "    \n",
    "w1,w2,w3,w4=trainieren(x1tr,x2tr,x3tr,x4tr,ytr,30000,0.001)\n",
    "richtige=test(x1,x2,x3,x4,y,w1,w2,w3,w4)\n",
    "print(w1,w2,w3,w4)\n",
    "print(f\" Accuracy {richtige/100:5.2f}\")\n",
    "print(f\"{richtige} von 100 richtig\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d07e2",
   "metadata": {},
   "source": [
    "Diese Gewichte verhalten sich auch nach der Normierung im Vergleich zueinander zwar ähnlich wie die Gewichte ohne Normierung (w1 stark negativ, w2 leicht positiv, w3 leichter negativ). Die Werte selbst sind aber sehr unterschiedlich. Insbesondere ist das sehr hohe positive Gewicht für das Baujahr im Verhältnis stark reduziert worden. Für eine richtige Interpretation ist die Skalierung sehr wichtig. Wir sehen außerdem, daß die Accuracy etwas gewonnen hat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3baed",
   "metadata": {},
   "source": [
    "Es ist leicht, den Klassifikator auszubauen auf mehr als eine Entscheidung, also nicht nur binär mit \"ja\" oder \"nein\", sondern auch auf mehrere Klassen. Unser Beispiel könnte man so auf die Ergebnisklassen \"einfach zu verkaufen\" , \"mittelgradig zu verkaufen\", \"schwer zu verkaufen\" und \"gar nicht zu verkaufen\" ausbauen. Die einfachste Möglichkeit wäre es, den Klassifikator für jede Ergebnisklasse binär auszuführen mit \"gehört in die Klasse\" oder \"gehört nicht in die Klasse\" als Label, und dann das Ergebnis mit der höchsten Wahrscheinlichkeit auszugegeben.<br>\n",
    "Wir würden unsere Datensätze im Training in jeweils vier Durchläufen so trainieren, dass zum Beispiel für die Entscheidung  \"gehört in Klasse ´einfach zu verkaufen´ oder nicht\", die Gewichte optimiert würden.\n",
    "Bei der Vorhersage würden wir dann mit für unsere vier Klassen vier Werte für die Zugehörigkeit zu der jeweiligen Klasse erhalten und uns dann z.B. für die Klasse mit dem höchsten Zugehörigkeitswert entscheiden. Hier ein Beispiel mit einem neuronalen Netzwerk, was wir später besprechen. <br><br><img width=1500 height=1500 class=\"imgright\" src=\"Images/Neuronales_Netzwerk1.png\"><br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a53e86",
   "metadata": {},
   "source": [
    "Hier haben wir uns für die Klasse mit dem Label 3 entschieden, weil der Klassifikator für diese den höchsten Vorhersagewert erzielte. Was aber verloren geht, ist die Information, wie sicher diese Vorhersage war. Denn im unten gezeigten Beispiel wäre die Klassifizierung viel eindeutiger gewesen.<br><br><img width=1800 height=1500 class=\"imgright\" src=\"Images/Neuronales_Netz2.png\"><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f90f1",
   "metadata": {},
   "source": [
    "Wir wollen nicht die Verteilung der Ergebnisse verlieren, wenn wir wie oben agieren und nur das beste Ergebnis nehmen. Sehen wir uns die Zahlen an den Ausgängen an, erkennen wir, daß wir nicht einfach die Zahl als Wahrscheinlichkeit interpretieren können. In diesem Fall müßten alle Ausgänge zusammen die Summe 1 ergeben. Dann könnten wir sagen, die Wahrscheinlichkeit, daß die Sample zur Klasse X gehört ist p(X). Wir müssen irgendwie unsere Ergebnisse normieren.<br><br> Was sind die Anforderungen daran?<br>die Ergebnisse sollen auf Werte im Intervall [0,1] tranformiert werden<br>die Werte der Ergebnisse sollen in Summe 1 ergeben<br>die Werte der Ergebnisse sollen die Größe der Eingangswerte abbilden <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb68ed",
   "metadata": {},
   "source": [
    "Hierfür gibt es die Softmax Funktion, die häufig am Ausgang eines Klassifikators angewendet wird, um die Ergebnisse als Wahrscheinlichkeiten interpretieren zu können. Sie ist definiert als:<br><b><img width=800 src=\"Images/Softmax.png\" /><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c6bb6",
   "metadata": {},
   "source": [
    "Dies erlaubt uns die Interpretation in Wahrscheinlichkeiten. Für unser Beispiel ergibt sich:<br>\n",
    "<br><img width=800 src=\"Images/Softmax2.png\" />    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a4bb6",
   "metadata": {},
   "source": [
    "Wir haben jetzt eine Verteilung mit den gewünschten Eigenschaften. Dabei erkennen wir, daß durch Softmax die numerischen Unterschiede der Ausgangswerte näher aneinander liegen. In der sklearn Bibliothek, die wir später besprechen, haben viele ML-Algorithmen die Möglichkeit, die Ergebnisse nach Durchlaufen einer Softmax Funktion anzuzeigen. z.B. (multi_class= ‘multinomial’) für logistische Regression, (mlp.out_activation_='softmax') für MPLClassifier... s.u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f20b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "mlp.out_activation_='softmax'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
