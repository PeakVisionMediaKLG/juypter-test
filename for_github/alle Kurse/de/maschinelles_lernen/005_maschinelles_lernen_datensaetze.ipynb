{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufteilungen der Daten\n",
    "\n",
    "### Datensets in Lern- und Testsets trennen\n",
    "\n",
    "<img class=\"imgright\" width=\"40%\" src=\"../images/data_splitting.webp\" srcset=\"../images/data_splitting_800w.webp 800w,../images/data_splitting_700w.webp 700w,../images/data_splitting_600w.webp 600w,../images/data_splitting_500w.webp 500w,../images/data_splitting_400w.webp 400w,../images/data_splitting_350w.webp 350w,../images/data_splitting_300w.webp 300w\" alt=\"Splitting into train and test sets\" />  \n",
    "\n",
    "Sie haben Ihre Daten bereit und möchten den Klassifikator trainieren? Aber seien Sie vorsichtig: Wenn Ihr Klassifikator fertig ist, benötigen Sie einige Testdaten, um Ihren Klassifikator zu bewerten. Wenn Sie Ihren Klassifikator mit den zum Lernen verwendeten Daten bewerten, sehen Sie möglicherweise überraschend gute Ergebnisse. Was wir tatsächlich testen möchten, ist die Leistung der Klassifizierung auf unbekannten Daten.\n",
    "\n",
    "Zu diesem Zweck müssen wir unsere Daten in zwei Teile aufteilen: \n",
    "\n",
    "1. Einen Trainingssatz, mit dem der Lernalgorithmus das Modell anpasst oder lernt\n",
    "2. Ein Testset zur Bewertung der Generalisierungsleistung des Modells\n",
    "\n",
    "\n",
    "Wenn man bedenkt, wie das maschinelle Lernen normalerweise funktioniert, ist die Idee einer Aufteilung zwischen Lern- und Testdaten sinnvoll. Real existierende Systeme trainieren auf existierenden Daten und wenn dann andere also neue Daten (von Kunden, Sensoren oder anderen Quellen) kommen, muss der trainierte Klassifikator diese neue Daten vorhersagen bzw. klassifizieren. Wir können dies während des Trainings mit einem Trainings- und Testdatenset simulieren - die Testdaten sind eine Simulation von \"zukünftigen Daten\", die während der Produktion in das System eingehen werden.\n",
    "\n",
    "In diesem Kapitel unseres Tutorials über Maschinelles Lernen mit Python erfahren Sie, wie Sie die Aufspaltung mit einfachem Python tun können. Danach werden wir zeigen, dass es nicht notwendig ist, dies manuell zu tun, da die Funktion ```train_test_split``` aus dem Modul ``` model_selection``` dies für uns tun kann.\n",
    "\n",
    "Wenn der Datensatz nach Labels sortiert ist, müssen wir ihn vor dem Teilen mischen.\n",
    "\n",
    "<img src = \"../images/splitting_data_learn_test.webp\" width = \"80%\">\n",
    "\n",
    "Wir haben den Datensatz in einen Lerndatensatz (a.k.a. Training) und einen Testdatensatz unterteilt. Die beste Vorgehensweise besteht darin, es in einen Lern-, Test- und Bewertungsdatensatz aufzuteilen.\n",
    "\n",
    "Wir werden unser Modell (Klassifikator) Schritt für Schritt trainieren und jedes Mal, wenn das Ergebnis getestet werden muss. Wenn wir nur einen Testdatensatz haben. Die Testergebnisse könnten in das Modell einfließen. Daher werden wir einen Bewertungsdatensatz für die gesamte Lernphase verwenden.\n",
    "\n",
    "<img src = \"../images/splitting_learn_evaluation_testdata.webp\" width = \"50%\">\n",
    "\n",
    "In unserem Tutorial werden wir jedoch nur Aufteilungen in Lern- und Testdatensätze verwenden. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung des Iris-Datensatzes\n",
    "\n",
    "Die  150 Datensätze des Iris-Datensatzes sind sortiert, d.h. die ersten 50 Daten entsprechen der ersten Blumeklasse (0 = Setosa), die nächsten 50 der zweiten Blumenklasse (1 = Versicolor) und die restlichen Daten entsprechen der letzten Klasse (2 = Virginica).\n",
    "\n",
    "Würden wir nun unsere Daten im Verhältnis 2/3 (Lernset) und 1/3 (Testset) aufteilen, enthielte das Lernset alle Blumen der beiden ersten Klassen und das Testset alle Blumen der dritten Klasse. Der KLassifikator könnte also nur zwei Klassen lernen und die dritte Klasse wäre komplett unbekannt. Deshalb müssen wir die Daten dringend mischen.\n",
    "\n",
    "Unter der Annahme, dass alle Stichproben unabhängig voneinander sind, möchten wir den Datensatz  **zufällig mischen, bevor wir den Datensatz** wie oben dargestellt aufteilen.\n",
    "\n",
    "Im Folgenden teilen wir die Daten manuell auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir uns die Labels anschauen, sehen wir, dass die Daten, wie eben beschrieben, sortiert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes müssen wir die Daten neu anordnen, damit sie nicht mehr sortiert werden. \n",
    "Dazu benutzen wir die permutation-Funktion des random-Submoduls von Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124,  15,  63, 133,  55, 113,  43, 107,   9, 140, 104, 118,  60,\n",
       "        17,  76, 119,  95, 148,  89, 102,  87, 122,  69, 123,  75,  22,\n",
       "        44,  85,  68,  38,  16,  30,  92,   2, 135,  35,  32,  84, 138,\n",
       "        54,  34,  71,  77, 139,  74, 116, 108, 147,  66,  57,  97,  28,\n",
       "        80,  91, 127,  20,  18,   3, 144,  86, 110,   8,  26,  65,  51,\n",
       "         4, 120,  88,  36, 106, 141,  56,  52,  64, 100, 132,  29, 103,\n",
       "       112, 149,  25,  19,  10,  40, 114,  79,  83, 130,  41,  31,  67,\n",
       "       131,  82,  42, 105, 142,   0, 121,  11,   5,  14, 145,  27,  39,\n",
       "        53,  72,  81,  93, 134, 129, 101,   7, 136, 115,  58,  94,  78,\n",
       "        50,   6,  12,  48,  61,  37,  99, 137,  45,  70,  33, 128, 111,\n",
       "        59, 117, 143,   1,  96, 125, 109,  13,  47,  49,  46, 146,  24,\n",
       "        21,  98,  23, 126,  62,  90,  73])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.permutation(len(iris.data))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.7 3.3 5.7 2.1]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.3 2.8 5.1 1.5]] [2 0 1 2]\n",
      "[[4.6 3.2 1.4 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.3 2.5 5.  1.9]] [0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "n_training_samples = 12\n",
    "learnset_data = iris.data[indices[:-n_training_samples]]\n",
    "learnset_labels = iris.target[indices[:-n_training_samples]]\n",
    "testset_data = iris.data[indices[-n_training_samples:]]\n",
    "testset_labels = iris.target[indices[-n_training_samples:]]\n",
    "print(learnset_data[:4], learnset_labels[:4])\n",
    "print(testset_data[:4], testset_labels[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilungen mit Sklearn\n",
    "\n",
    "Obwohl es nicht schwierig war müssen wir die Aufteilung nicht, wie oben gezeigt, manuell durchführen. Da dies beim maschinellen Lernen häufig erforderlich ist, verfügt scikit-learn über eine vordefinierte Funktion zum Aufteilen von Daten in Trainings- und Testsätze.\n",
    "\n",
    "Wir werden dies im Folgenden demonstrieren. Wir werden 80% der Daten als Trainings- und 20% als Testdaten verwenden. Ebensogut hätten wir 70% und 30% nehmen können, denn  es gibt keine festen Regeln. Das Wichtigste ist, dass Sie Ihr System fair anhand von Daten bewerten, die es während des Trainings *nicht* gesehen hat! Außerdem müssen in beiden Datensätze genügend Daten enthalten sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ersten 7-Datensätze:\n",
      "[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.6 2.9 3.6 1.3]]\n",
      "Die zugehörigen ersten 7-Labels:\n",
      "[1 0 2 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42 # garantees same output for every run\n",
    "                      )\n",
    "train_data, test_data, train_labels, test_labels = res    \n",
    "\n",
    "n = 7\n",
    "print(f\"Die ersten {n}-Datensätze:\")\n",
    "print(test_data[:7])\n",
    "print(f\"Die zugehörigen ersten {n}-Labels:\")\n",
    "print(test_labels[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geschichtete Zufallsprobe\n",
    "\n",
    "Insbesondere bei relativ kleinen Datenmengen ist es besser, die Aufteilung zu schichten. Schichtung bedeutet, dass wir den ursprünglichen Klassenanteil des Datensatzes in den Test- und Trainingssätzen beibehalten. Wir berechnen die Klassenanteile der vorherigen Aufteilung in Prozent unter Verwendung des folgenden Codes. Um die Anzahl der Vorkommen jeder Klasse zu berechnen, verwenden wir die Numpy-Funktion 'bincount'. Es zählt die Anzahl der Vorkommen jedes Werts in dem als Argument übergebenen Array nicht negativer Integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [33.33333333 33.33333333 33.33333333]\n",
      "Training: [33.33333333 34.16666667 32.5       ]\n",
      "Test: [33.33333333 30.         36.66666667]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('All:', np.bincount(labels) / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels) / float(len(test_labels)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Aufteilung zu schichten, können wir das Label-Array als zusätzliches Argument an die Funktion `train_test_split` übergeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [33.33333333 33.33333333 33.33333333]\n",
      "Training: [33.33333333 33.33333333 33.33333333]\n",
      "Test: [33.33333333 33.33333333 33.33333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42,\n",
    "                       stratify=labels)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "\n",
    "print('All:', np.bincount(labels) / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels) / float(len(test_labels)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies war ein dummes Beispiel, um die Arbeitsweise der geschichteten Zufallsstichprobe zu testen, da der Iris-Datensatz die gleichen Anteile aufweist, d.h. jede Klasse 50 Items.\n",
    "\n",
    "Wir werden jetzt mit der Datei ```strange_flowers.txt``` des Verzeichnisses ```data``` arbeiten. Dieser Datensatz wird im Kapitel [Datasets in Python generieren](maschinelles_lernen_daten_erzeugen.php) erstellt.\n",
    "Die Klassen in diesem Datensatz haben eine unterschiedliche Anzahl von Elementen. Zuerst laden wir die Daten: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = np.loadtxt(\"data/strange_flowers.txt\", delimiter=\" \")\n",
    "data = content[:, :-1]    # Letzte Spalte wird abgeschnitten\n",
    "labels = content[:, -1]\n",
    "labels.dtype\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n",
      "Training: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n",
      "Test: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n"
     ]
    }
   ],
   "source": [
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42,\n",
    "                       stratify=labels)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "\n",
    "# np.bincount expects non negative integers:\n",
    "print('All:', np.bincount(labels.astype(int))  / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels.astype(int)) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels.astype(int)) / float(len(test_labels)) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "_merged_merged",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
