{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Learn, Test and Evaluation Data\n",
    "<img class=\"imgright\" width=\"40%\" src=\"../images/data_splitting.webp\" srcset=\"../images/data_splitting_800w.webp 800w,../images/data_splitting_700w.webp 700w,../images/data_splitting_600w.webp 600w,../images/data_splitting_500w.webp 500w,../images/data_splitting_400w.webp 400w,../images/data_splitting_350w.webp 350w,../images/data_splitting_300w.webp 300w\" alt=\"Splitting into train and test sets\" />  \n",
    "\n",
    "You have your data ready and you are eager to start training the classifier? \n",
    "But be careful: When your classifier will be finished, you will need some test data to evaluate your classifier. If you evaluate your classifier with the data used for learning, you may see surprisingly good results. What we actually want to test is the performance of classifying on unknown data.\n",
    "\n",
    "For this purpose, we need to split our data into two parts:\n",
    "\n",
    "1. A training set with which the learning algorithm adapts or learns the model\n",
    "2. A test set to evaluate the generalization performance of the model\n",
    "\n",
    "\n",
    "When you consider how machine learning normally works, the idea of a split between learning and test data makes sense. Really existing systems train on existing data and if other new data (from customers, sensors or other sources) comes in, the trained classifier has to predict or classify this new data. We can simulate this during training with a training and test data set - the test data is a simulation of \"future data\" that will go into the system during production. \n",
    "\n",
    "In this chapter of our Python Machine Learning Tutorial, we will learn how to do the splitting with plain Python. \n",
    "\n",
    "We will see also that doing it manually is not necessary, because the ```train_test_split``` function from the ```model_selection``` module can do it for us.\n",
    "\n",
    "If the dataset is sorted by label, we will have to shuffle it before splitting.\n",
    "\n",
    "<img src = \"../images/splitting_data_learn_test.webp\" width = \"50%\">\n",
    "\n",
    "We separated the dataset into a learn (a.k.a. training) dataset  and a test dataset. Best practice is to split it into a learn, test and an evaluation dataset.\n",
    "\n",
    "We will train our model (classifier) step by step and each time the result needs to be tested. If we just have a test dataset. The results of the testing might get into the model. So we will use an evaluation dataset for the complete learning phase. \n",
    "When our classifier is finished, we will check it with the test dataset, which it has not \"seen\" before!\n",
    "\n",
    "<img src = \"../images/splitting_learn_evaluation_testdata.webp\" width = \"25%\">\n",
    "\n",
    "Yet, during our tutorial, we will only use splitings into learn and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Example: Iris Data Set\n",
    "\n",
    "We will demonstrate the previously discussed topics with the Iris Dataset.\n",
    "\n",
    "The 150 data sets of the Iris data set are sorted, i.e. the first 50 data correspond to the first flower class (0 = Setosa), the next 50 to the second flower class (1 = Versicolor) and the remaining data correspond to the last class (2 = Virginica).\n",
    "\n",
    "If we were to split our data in the ratio 2/3 (learning set) and 1/3 (test set), the learning set would contain all the flowers of the first two classes and the test set all the flowers of the third flower class. The classifier could only learn two classes and the third class would be completely unknown. So we urgently need to mix the data.\n",
    "\n",
    "Assuming all samples are independent of each other, we want to shuffle the data set **randomly before we split the data set** as shown above.\n",
    "\n",
    "In the following we split the data manually: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the labels of ```iris.target``` shows us that the data is sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is rearrange the data so that it is not sorted anymore.\n",
    "For this purpose, we will use the permutation function of the random submodul of Numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 98,  56,  37,  60,  94, 142, 117, 121,  10,  15,  89,  85,  66,\n",
       "        29,  44, 102,  24, 140,  58,  25,  19, 100,  83, 126,  28, 118,\n",
       "        50, 127,  72,  99,  74,   0, 128,  11,  45, 143,  54,  79,  34,\n",
       "        32,  95,  92,  46, 146,   3,   9,  73, 101,  23,  77,  39,  87,\n",
       "       111, 129, 148,  67,  75, 147,  48,  76,  43,  30, 144,  27, 104,\n",
       "        35,  93, 125,   2,  69,  63,  40, 141,   7, 133,  18,   4,  12,\n",
       "       109,  33,  88,  71,  22, 110,  42,   8, 134,   5,  97, 114, 135,\n",
       "       108,  91,  14,   6, 137, 124, 130, 145,  55,  17,  80,  36,  61,\n",
       "        49,  62,  90,  84,  64, 139, 107, 112,   1,  70, 123,  38, 132,\n",
       "        31,  16,  13,  21, 113, 120,  41, 106,  65,  20, 116,  86,  68,\n",
       "        96,  78,  53,  47, 105, 136,  51,  57, 131, 149, 119,  26,  59,\n",
       "       138, 122,  81, 103,  52, 115,  82])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.permutation(len(iris.data))\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 2.5 3.  1.1]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [5.  2.  3.5 1. ]] [1 1 0 1]\n",
      "[[7.9 3.8 6.4 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  3.4 1.6 0.4]] [2 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "n_test_samples = 12\n",
    "learnset_data = iris.data[indices[:-n_test_samples]]\n",
    "learnset_labels = iris.target[indices[:-n_test_samples]]\n",
    "testset_data = iris.data[indices[-n_test_samples:]]\n",
    "testset_labels = iris.target[indices[-n_test_samples:]]\n",
    "print(learnset_data[:4], learnset_labels[:4])\n",
    "print(testset_data[:4], testset_labels[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits with Sklearn\n",
    "\n",
    "Even though it was not difficult to split the data manually into a learn (train) and an evaluation (test) set, we don't have to do the splitting manually as shown above. Since this is often required in machine learning, scikit-learn has a predefined function for dividing data into training and test sets.\n",
    "\n",
    "\n",
    "We will demonstrate this below. We will use 80% of the data as training and 20% as test data. We could just as well have taken 70% and 30%, because there are no hard and fast rules. The most important thing is that you rate your system fairly based on data it * did not * see during exercise! In addition, there must be enough data in both data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 7 data sets:\n",
      "[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.6 2.9 3.6 1.3]]\n",
      "The corresponding 7 labels:\n",
      "[1 0 2 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42)\n",
    "train_data, test_data, train_labels, test_labels = res    \n",
    "\n",
    "n = 7\n",
    "print(f\"The first {n} data sets:\")\n",
    "print(test_data[:7])\n",
    "print(f\"The corresponding {n} labels:\")\n",
    "print(test_labels[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified random sample\n",
    "\n",
    "Especially with relatively small amounts of data, it is better to stratify the division. Stratification means that we keep the original class proportion of the data set in the test and training sets. We calculate the class proportions of the previous split in percent using the following code. To calculate the number of occurrences of each class, we use the numpy function 'bincount'. It counts the number of occurrences of each value in the array of non-negative integers passed as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [33.33333333 33.33333333 33.33333333]\n",
      "Training: [33.33333333 34.16666667 32.5       ]\n",
      "Test: [33.33333333 30.         36.66666667]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('All:', np.bincount(labels) / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels) / float(len(test_labels)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stratify the division, we can pass the label array as an additional argument to the train_test_split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [33.33333333 33.33333333 33.33333333]\n",
      "Training: [33.33333333 33.33333333 33.33333333]\n",
      "Test: [33.33333333 33.33333333 33.33333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42,\n",
    "                       stratify=labels)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "\n",
    "print('All:', np.bincount(labels) / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels) / float(len(test_labels)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a stupid example to test the stratified random sample, because the Iris data set has the same proportions, i.e. each class 50 elements.\n",
    "\n",
    "We will work now with the file ```strange_flowers.txt``` of the directory ```data```. This data set is created in the chapter [Generate Datasets in Python](machine_learning_create_datasets.php) \n",
    "The classes in this dataset have different numbers of items. First we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = np.loadtxt(\"data/strange_flowers.txt\", delimiter=\" \")\n",
    "data = content[:, :-1]    # cut of the target column\n",
    "labels = content[:, -1]\n",
    "labels.dtype\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n",
      "Training: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n",
      "Test: [ 0.         23.89937107 25.78616352 28.93081761 21.3836478 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42,\n",
    "                       stratify=labels)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "\n",
    "# np.bincount expects non negative integers:\n",
    "print('All:', np.bincount(labels.astype(int))  / float(len(labels)) * 100.0)\n",
    "print('Training:', np.bincount(train_labels.astype(int)) / float(len(train_labels)) * 100.0)\n",
    "print('Test:', np.bincount(test_labels.astype(int)) / float(len(test_labels)) * 100.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
