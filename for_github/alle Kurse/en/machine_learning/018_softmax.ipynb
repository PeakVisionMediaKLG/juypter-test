{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax as Activation Function\n",
    "<br><br>\n",
    "\n",
    "### Softmax\n",
    "\n",
    "<img width=\"40%\" class=\"imgright\" src=\"../images/softmax_broken_one_on_marble.webp\" srcset=\"../images/softmax_broken_one_on_marble_600w.webp 600w,../images/softmax_broken_one_on_marble_500w.webp 500w,../images/softmax_broken_one_on_marble_400w.webp 400w,../images/softmax_broken_one_on_marble_350w.webp 350w,../images/softmax_broken_one_on_marble_300w.webp 300w\" alt=\"softmax formula with broken one\" />\n",
    "\n",
    "The previous implementations of neural networks in our tutorial returned float values in the open interval (0, 1). To make a final decision we had to interprete the results of the output neurons. The one with the highest value is a likely candidate but we also have to see it in relation to the other results. It should be obvious that in a two classes case ($c_1$ and $c_2$) a result (0.013, 0.95) is a clear vote for the class $c_2$ but (0.73, 0.89) on the other hand is a different thing. We could say  in this situation '$c_2$ is more likely than $c_1$, but $c_1$ has still a high likelihood'. Talking about likelihoods: The return values are not probabilities. It would be a lot better to have a normalized output with a probability function.\n",
    "Here comes the softmax function into the picture. The softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector of n real numbers, and normalizes it into a probability distribution consisting of n probabilities proportional to the exponentials of the input vector. A probability distribution implies that the result vector sums up to 1. Needless to say, if some components of the input vector are negative or greater than one, they will be in the range (0, 1) after applying Softmax . The Softmax function is often used in neural networks, to map the results of the output layer, which is non-normalized,  to a probability distribution over predicted output classes. \n",
    "\n",
    "The softmax function $\\sigma$ is defined by the following formula:\n",
    "\n",
    "$\\sigma(o_i) = \\frac{e^{o_i}}{\\sum_{j=1}^{n} e^{o_j}}$ \n",
    "\n",
    "where the index i is in (0, ..., n-1) and o is the output vector of the network\n",
    "\n",
    "$o = (o_0, o_1, \\ldots, o_{n-1})$\n",
    "\n",
    "We can implement the softmax function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01578405, 0.00580663, 0.11662925, 0.86178007]),\n",
       " array([0.11111111, 0.        , 0.33333333, 0.55555556]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" applies softmax to an input x\"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "x = np.array([1, 0, 3, 5])\n",
    "y = softmax(x)\n",
    "y, x / x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoiding underflow or overflow errors due to floating point instability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01578405, 0.00580663, 0.11662925, 0.86178007])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" applies softmax to an input x\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.array([0.3, 0.4, 0.00005], np.float64)\n",
    "print(softmax(x))\n",
    "print(x / x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivate of softmax Function\n",
    "\n",
    "The softmax function can be written as\n",
    "$$\n",
    "S(o):  \n",
    "\\begin{bmatrix}\n",
    "    o_{1}\\\\\n",
    "    o_{2}\\\\\n",
    "    \\cdots\\\\\n",
    "    o_{n}\\\\\n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{bmatrix}\n",
    "    s_{1}\\\\\n",
    "    s_{2}\\\\\n",
    "    \\cdots\\\\\n",
    "    s_{n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Per element it looks like this:\n",
    "\n",
    "$$s_j(o) = \\frac{e^{o_j}}{\\sum\\limits_{k=1}^{n}{e^{o_k}}}, \\forall k=1, \\cdots, n $$\n",
    "\n",
    "\n",
    "The derivative of softmax can be calculated like this:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial O} = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial s_1}{\\partial o_1} & \\cdots & \\frac{\\partial s_1}{\\partial o_n} \\\\\n",
    "    \\cdots \\\\\n",
    "    \\frac{\\partial s_n}{\\partial o_1} & \\cdots & \\frac{\\partial s_n}{\\partial o_n} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The partial derivatives can be solved for every i and j:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial s_i}{\\partial o_j} = \\frac{\\partial \\frac{e^{o_i}}{\\sum_{k=1}^{n}{e^{o_k}}}}{\\partial o_j}$$\n",
    "\n",
    "We will use the quotien rule, i.e.\n",
    "\n",
    "the derivative of\n",
    "\n",
    "$$f(x) = \\frac{g(x)}{h(x)}$$\n",
    "\n",
    "is\n",
    "\n",
    "$$f'(x) = \\frac{g'(x)\\cdot h(x) - g(x) \\cdot h'(x)}{(h(x)^2}$$\n",
    "\n",
    "We can set $g(x)$ to   $e^{o_i}$ and  $h(x)$ to $\\sum_{k=1}^{n}{e^{o_k}}$ \n",
    "\n",
    "The derivative of $g(x)$ is \n",
    "\n",
    "$$g'(x) = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    e^{o_i}, & \\text{if}\\ i=j \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.$$\n",
    "\n",
    "and the derivative of $h(x)$ is\n",
    "\n",
    "$$h'(x) = e^{o_j}, \\forall k=1, \\cdots, n$$ \n",
    "\n",
    "Let's apply the quotient rule by case differentiation now:\n",
    "\n",
    "1. case: $i = j$:\n",
    "\n",
    "$$\\frac{e^{o_i} \\cdot \\sum_{k=1}^{n}{e^{o_k}} -  e^{o_i} \\cdot e^{o_j}}{(\\sum_{k=1}^{n}{e^{o_k}})^2}$$\n",
    "\n",
    "We can rewrite this expression as\n",
    "\n",
    "$$\\frac{e^{o_i}}{\\sum_{k=1}^{n}{e^{o_k}}} \\cdot \\frac{\\sum_{k=1}^{n}{e^{o_k} - e^{o_j}}}{\\sum_{k=1}^{n}{e^{o_k}}}$$\n",
    "\n",
    "\n",
    "Now we can reduce the second quotient:\n",
    "\n",
    "$$\\frac{e^{o_i}}{\\sum_{k=1}^{n}{e^{o_k}}} \\cdot (1 - \\frac{e^{o_j}}{\\sum_{k=1}^{n}{e^{o_k}}})$$\n",
    "\n",
    "If we compare this expression with the Definition of $s_i$, we can rewrite it to:\n",
    "\n",
    "$$s_i \\cdot (1 - s_j)$$\n",
    "\n",
    "which is the same as $$s_i \\cdot (1 - s_i)$$ because $i=j$.\n",
    "\n",
    "\n",
    "2. case: $i \\neq j$:\n",
    "\n",
    "$$\\frac{0 \\cdot \\sum_{k=1}^{n}{e^{o_k}} -  e^{o_i} \\cdot e^{o_j}}{(\\sum_{k=1}^{n}{e^{o_k}})^2}$$\n",
    "\n",
    "this can be rewritten as:\n",
    "\n",
    "$$- \\frac{e^{o_i}}{\\sum_{k=1}^{n}{e^{o_k}}}\\cdot \\frac{e^{o_j}}{\\sum_{k=1}^{n}{e^{o_k}}}$$\n",
    "\n",
    "this gives us finally:\n",
    "\n",
    "$$-s_i \\cdot s_j$$\n",
    "\n",
    "We can summarize these two cases and write the derivative as:\n",
    "\n",
    "$$g'(x) = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    s_i \\cdot (1 - s_i), & \\text{if}\\ i=j \\\\\n",
    "    -s_i \\cdot s_j, & \\text{otherwise}\n",
    "  \\end{array}\\right.$$\n",
    "\n",
    "If we use the Kronecker delta function<sup>1</sup>, we can get rid of the case differentiation, i.e. we \"let the Kronecker delta do this work\":\n",
    "\n",
    "$$\\frac{\\partial s_i}{\\partial o_j} = s_i(\\delta_{ij} - s_j)$$\n",
    "\n",
    "Finally we can calculate the derivative of softmax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial O} = \n",
    "\\begin{bmatrix}\n",
    "    s_1(\\delta_{11} - s_1)  & s_1(\\delta_{12} - s_2) & \\cdots & s_1(\\delta_{1n} - s_n) \\\\\n",
    "    s_2(\\delta_{21} - s_1)  & s_2(\\delta_{22} - s_2) & \\cdots & s_2(\\delta_{2n} - s_n) \\\\\n",
    "    \\cdots \\\\\n",
    "    s_n(\\delta_{n1} - s_1)  & s_n(\\delta_{n2} - s_2) & \\cdots & s_n(\\delta_{nn} - s_n) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00490169 0.26762315 0.72747516]\n",
      "[[-2.40265555e-05 -1.31180548e-03 -3.56585701e-03]\n",
      " [-1.31180548e-03 -7.16221526e-02 -1.94689196e-01]\n",
      " [-3.56585701e-03 -1.94689196e-01 -5.29220104e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00487766, -0.00131181, -0.00356586],\n",
       "       [-0.00131181,  0.196001  , -0.1946892 ],\n",
       "       [-0.00356586, -0.1946892 ,  0.19825505]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "s = softmax(np.array([0, 4, 5]))\n",
    "\n",
    "si_sj = - s * s.reshape(3, 1)\n",
    "print(s)\n",
    "print(si_sj)\n",
    "s_der = np.diag(s) + si_sj\n",
    "s_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%writefile neural_networks_softmax.py\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 learning_rate,\n",
    "                 softmax=True):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate \n",
    "        self.softmax = softmax\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n",
    "                                       self.no_of_in_nodes))\n",
    "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n",
    "                                        self.no_of_hidden_nodes))\n",
    "        \n",
    "    \n",
    "    def train(self, input_vector, target_vector):\n",
    "        \"\"\"\n",
    "        input_vector and target_vector can be tuples, lists or ndarrays\n",
    "        \"\"\"\n",
    "        # make sure that the vectors have the right shape\n",
    "        input_vector = np.array(input_vector)\n",
    "        input_vector = input_vector.reshape(input_vector.size, 1)\n",
    "        target_vector = np.array(target_vector).reshape(target_vector.size, 1)\n",
    "\n",
    "        output_vector_hidden = sigmoid(self.weights_in_hidden @ input_vector)\n",
    "        if self.softmax:\n",
    "            output_vector_network = softmax(self.weights_hidden_out @ output_vector_hidden)\n",
    "        else:\n",
    "            output_vector_network = sigmoid(self.weights_hidden_out @ output_vector_hidden)\n",
    "            \n",
    "\n",
    "        output_error = target_vector - output_vector_network\n",
    "        if self.softmax:\n",
    "            ovn = output_vector_network.reshape(output_vector_network.size,)\n",
    "            si_sj = - ovn * ovn.reshape(self.no_of_out_nodes, 1)\n",
    "            s_der = np.diag(ovn) + si_sj\n",
    "            tmp =  s_der @ output_error   \n",
    "            self.weights_hidden_out += self.learning_rate  * (tmp @ output_vector_hidden.T)\n",
    "        else:    \n",
    "            tmp = output_error * output_vector_network * (1.0 - output_vector_network)    \n",
    "            self.weights_hidden_out += self.learning_rate  * (tmp @ output_vector_hidden.T)\n",
    "            \n",
    "            \n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = self.weights_hidden_out.T @ output_error\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
    "        self.weights_in_hidden += self.learning_rate * (tmp @ input_vector.T)  \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        \"\"\"\n",
    "        running the network with an input vector 'input_vector'. \n",
    "        'input_vector' can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    "        # make sure that input_vector is a column vector:\n",
    "        input_vector = np.array(input_vector)\n",
    "        input_vector = input_vector.reshape(input_vector.size, 1)\n",
    "        input4hidden = sigmoid(self.weights_in_hidden @ input_vector)\n",
    "        if self.softmax:\n",
    "            output_vector_network = softmax(self.weights_hidden_out @ input4hidden)\n",
    "        else:\n",
    "            output_vector_network = sigmoid(self.weights_hidden_out @ input4hidden)\n",
    "\n",
    "        return output_vector_network\n",
    "            \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n_samples = 300\n",
    "samples, labels = make_blobs(n_samples=n_samples, \n",
    "                             centers=([2, 6], [6, 2]), \n",
    "                             random_state=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "colours = ('green', 'red', 'blue', 'magenta', 'yellow', 'cyan')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "for n_class in range(2):\n",
    "    ax.scatter(samples[labels==n_class][:, 0], samples[labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n",
    "    \n",
    "size_of_learn_sample = int(n_samples * 0.8)\n",
    "learn_data = samples[:size_of_learn_sample]\n",
    "test_data = samples[-size_of_learn_sample:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_networks_softmax import NeuralNetwork\n",
    "\n",
    "simple_network = NeuralNetwork(no_of_in_nodes=2, \n",
    "                               no_of_out_nodes=2, \n",
    "                               no_of_hidden_nodes=5,\n",
    "                               learning_rate=0.3,\n",
    "                               softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4) [[0.53325729]\n",
      " [0.46674271]] 1.0\n",
      "(2, 6) [[0.50669849]\n",
      " [0.49330151]] 1.0\n",
      "(3, 3) [[0.53050147]\n",
      " [0.46949853]] 1.0\n",
      "(6, 2) [[0.52530293]\n",
      " [0.47469707]] 1.0\n"
     ]
    }
   ],
   "source": [
    "for x in [(1, 4), (2, 6), (3, 3), (6, 2)]:\n",
    "    y = simple_network.run(x)\n",
    "    print(x, y, s.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_one_hot = (np.arange(2) == labels.reshape(labels.size, 1))\n",
    "labels_one_hot = labels_one_hot.astype(np.float)\n",
    "\n",
    "for i in range(size_of_learn_sample):\n",
    "    #print(learn_data[i], labels[i], labels_one_hot[i])\n",
    "    simple_network.train(learn_data[i], \n",
    "                         labels_one_hot[i])\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "evaluation = Counter()\n",
    "simple_network.evaluate(learn_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "<sup>1</sup> Kronecker delta:\n",
    "$$\\delta_{ij} = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    1, & \\text{if}\\ i=j \\\\\n",
    "    0, & \\text{if}\\ i \\neq j\\\\\n",
    "  \\end{array}\\right.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
