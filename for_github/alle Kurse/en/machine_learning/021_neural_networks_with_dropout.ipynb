{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Neural Networks\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<img width=\"50%\" class=\"imgright\" src=\"../images/dropout_lightbulbs.webp\" srcset=\"../images/dropout_lightbulbs_800w.webp 800w,../images/dropout_lightbulbs_700w.webp 700w,../images/dropout_lightbulbs_600w.webp 600w,../images/dropout_lightbulbs_500w.webp 500w,../images/dropout_lightbulbs_400w.webp 400w,../images/dropout_lightbulbs_350w.webp 350w,../images/dropout_lightbulbs_300w.webp 300w\" alt=\"dropout neural network with lightbulbs\" />\n",
    "\n",
    "\n",
    "The term \"dropout\" is used for a technique which drops out some nodes of the network. Dropping out can be seen as temporarily deactivating or ignoring neurons of the network. This technique is applied in the training phase  to reduce overfitting effects. Overfitting is an error which occurs when a network is too closely fit to a limited set of input samples.\n",
    "\n",
    "The basic idea behind dropout neural networks is to dropout nodes so that the network can concentrate on other features. Think about it like this. You watch lots of films from your favourite actor. At some point you listen to the radio and here somebody in an interview. You don't recognize your favourite actor, because you have seen only movies and your are a visual type. Now, imagine that you can only listen to the audio tracks of the films. In this case you will have to learn to differentiate the voices of the actresses and actors. So by dropping out the visual part you are forced tp focus on the sound features! \n",
    "\n",
    "\n",
    "This technique has been first proposed in a paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\" by Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov in 2014\n",
    "\n",
    "We will implement in our tutorial on machine learning in Python a Python class which is capable of dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Weight Arrays\n",
    "\n",
    "If we deactivate a node, we have to modify the weight arrays accordingly. To demonstrate how this can be accomplished, we will use a network with three input nodes, four hidden and two output nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuronal Network with 3 input, 4 hidden und 2 output nodes](../images/example_network_3_4_2_without_bias.webp \"Neuronal Network with 3 input, 4 hidden und 2 output nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we will have a look at the weight array between the input and the hidden layer. We called this array 'wih' (weights between input and hidden layer). \n",
    "\n",
    "Let's deactivate (drop out) the node $i_2$. We can see in the following diagram what's happening:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuronal Network with one input dropout node](../images/weights_input2hidden_dropout_i2.webp \"Neuronal Network with one input dropout node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have to take out every second product of the summation, which means that we have to delete the whole second column of the matrix. The second element from the input vector has to be deleted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuronal Network with dropped out input node](../images/weight_matrix_input_dropout_i2.webp \"Neuronal Network with dropped out input node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will examine what happens if we take out a hidden node. We take out the first hidden node, i.e. $h_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/weights_input2hidden_dropout_h1.webp\" \n",
    "alt=\"Neuronal Network with one hidden dropout node\" width=80% max-width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can remove the complete first line of our weight matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Weight Matrix from Neuronal Network with dropped out input node](../images/weight_matrix_input_dropout_h1.webp \"Weight Matrix from  Neuronal Network with dropped out input node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking out a hidden node affects the next weight matrix as well. Let's have a look at what is happening in the network graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/weights_hidden2output_dropout_h1.webp\" \n",
    "alt=\"Neuronal Network with one hidden dropout node\" width=80% max-width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see that the first column of the who weight matrix has to be removed again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Weight Matrix from Neuronal Network with dropped out hidden node](../images/weight_matrix_hidden_dropout_h1.webp \"Weight Matrix from  Neuronal Network with dropped out hidden node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have arbitrarily chosen one node to deactivate. The dropout approach means that we randomly choose a certain number of nodes from the input and the hidden layers, which remain active and turn off the other nodes of these layers. After this we can train a part of our learn set with this network. The next step consists in activating all the nodes again and randomly chose other nodes. It is also possible to train the whole training set with the randomly created dropout networks.\n",
    "\n",
    "We present three possible randomly chosen dropout networks in the following three diagrams:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/example_network_3_4_2_dropout_examples1.webp\" \n",
    "alt=\"Randomly chosen active nodes in dropout network, first example\" width=80% max-width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/example_network_3_4_2_dropout_examples2.webp\" \n",
    "alt=\"Randomly chosen active nodes in dropout network, second example\" width=80% max-width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/example_network_3_4_2_dropout_examples3.webp\" \n",
    "alt=\"Randomly chosen active nodes in dropout network, example example\" width=80% max-width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to think about a possible Python implementation.\n",
    "\n",
    "We will start with the weight matrix between input and hidden layer. We will randomly create a weight matrix for 10 input nodes and 5 hidden nodes. We fill our matrix with random numbers between -10 and 10, which are not proper weight values, but this way we can see better what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6,  -8,  -3,  -7,   2,  -9,  -3,  -5,  -6,   4],\n",
       "       [  5,   3,   7,  -4,   4,   8,  -2,  -4,   7,   7],\n",
       "       [  9,  -7,   4,   0,   4,   0,  -3,  -6,  -2,   7],\n",
       "       [ -8,  -9,  -4,  -5,  -9,   8,  -8,  -8,  -2,  -3],\n",
       "       [  3, -10,   0,  -3,   4,   0,   0,   2,  -7,  -9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "input_nodes = 10\n",
    "hidden_nodes = 5\n",
    "output_nodes = 7\n",
    "\n",
    "wih = np.random.randint(-10, 10, (hidden_nodes, input_nodes))\n",
    "wih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose now the active nodes for the input layer. We calculate random indices for the active nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 5, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_input_percentage = 0.7\n",
    "active_input_nodes = int(input_nodes * active_input_percentage)\n",
    "active_input_indices = sorted(random.sample(range(0, input_nodes), \n",
    "                              active_input_nodes))\n",
    "active_input_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned above that we have to remove the column $j$, if the node $i_j$ is removed. We can easily accomplish this for all deactived nodes by using the slicing operator with the active nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6,  -8,  -3,  -9,  -5,  -6,   4],\n",
       "       [  5,   3,   7,   8,  -4,   7,   7],\n",
       "       [  9,  -7,   4,   0,  -6,  -2,   7],\n",
       "       [ -8,  -9,  -4,   8,  -8,  -2,  -3],\n",
       "       [  3, -10,   0,   0,   2,  -7,  -9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wih_old = wih.copy()\n",
    "wih = wih[:, active_input_indices]\n",
    "wih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have mentioned before, we will have to modify both the 'wih' and the 'who' matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3   6  -3  -9   4]\n",
      " [-10   1   2   5   7]\n",
      " [ -8   1  -3   6   3]\n",
      " [ -3  -3   6  -5  -3]\n",
      " [ -4  -9   8  -3   5]\n",
      " [  8   4  -8   2   7]\n",
      " [ -2   2   3  -8  -5]]\n",
      "[0, 2, 3]\n",
      "[[  3  -3  -9]\n",
      " [-10   2   5]\n",
      " [ -8  -3   6]\n",
      " [ -3   6  -5]\n",
      " [ -4   8  -3]\n",
      " [  8  -8   2]\n",
      " [ -2   3  -8]]\n"
     ]
    }
   ],
   "source": [
    "who = np.random.randint(-10, 10, (output_nodes, hidden_nodes))\n",
    "\n",
    "print(who)\n",
    "active_hidden_percentage = 0.7\n",
    "active_hidden_nodes = int(hidden_nodes * active_hidden_percentage)\n",
    "active_hidden_indices = sorted(random.sample(range(0, hidden_nodes), \n",
    "                             active_hidden_nodes))\n",
    "print(active_hidden_indices)\n",
    "\n",
    "who_old = who.copy()\n",
    "who = who[:, active_hidden_indices]\n",
    "print(who)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to change wih accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6, -8, -3, -9, -5, -6,  4],\n",
       "       [ 9, -7,  4,  0, -6, -2,  7],\n",
       "       [-8, -9, -4,  8, -8, -2, -3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wih = wih[active_hidden_indices]\n",
    "wih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code summarizes the sniplets from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wih: \n",
      " [[ -4   9   3   5  -9   5  -3   0   9   1]\n",
      " [  4   7  -7   3  -4   7   4  -5   6   2]\n",
      " [  5   8   1 -10  -8  -6   7  -4  -6   8]\n",
      " [  6  -3   7   4  -7  -4   0   8   9   1]\n",
      " [  6  -1   4  -3   5  -5  -5   5   4  -7]]\n",
      "who:\n",
      " [[ -6   2  -2   4   0]\n",
      " [ -5  -3   3  -4 -10]\n",
      " [  4   6  -7  -7  -1]\n",
      " [ -4  -1 -10   0  -8]\n",
      " [  8  -2   9  -8  -9]\n",
      " [ -6   0  -2   1  -8]\n",
      " [  1  -4  -2  -6  -5]]\n",
      "\n",
      "active input indices:  [1, 3, 4, 5, 7, 8, 9]\n",
      "active hidden indices:  [0, 1, 2]\n",
      "\n",
      "wih after deactivating input nodes:\n",
      " [[  9   5  -9   5   0   9   1]\n",
      " [  7   3  -4   7  -5   6   2]\n",
      " [  8 -10  -8  -6  -4  -6   8]\n",
      " [ -3   4  -7  -4   8   9   1]\n",
      " [ -1  -3   5  -5   5   4  -7]]\n",
      "\n",
      "wih after deactivating hidden nodes:\n",
      " [[  9   5  -9   5   0   9   1]\n",
      " [  7   3  -4   7  -5   6   2]\n",
      " [  8 -10  -8  -6  -4  -6   8]]\n",
      "\n",
      "wih after deactivating hidden nodes:\n",
      " [[ -6   2  -2]\n",
      " [ -5  -3   3]\n",
      " [  4   6  -7]\n",
      " [ -4  -1 -10]\n",
      " [  8  -2   9]\n",
      " [ -6   0  -2]\n",
      " [  1  -4  -2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "input_nodes = 10\n",
    "hidden_nodes = 5\n",
    "output_nodes = 7\n",
    "\n",
    "wih = np.random.randint(-10, 10, (hidden_nodes, input_nodes))\n",
    "print(\"wih: \\n\", wih)\n",
    "who = np.random.randint(-10, 10, (output_nodes, hidden_nodes))\n",
    "print(\"who:\\n\", who)\n",
    "\n",
    "active_input_percentage = 0.7\n",
    "active_hidden_percentage = 0.7\n",
    "\n",
    "active_input_nodes = int(input_nodes * active_input_percentage)\n",
    "active_input_indices = sorted(random.sample(range(0, input_nodes), \n",
    "                              active_input_nodes))\n",
    "print(\"\\nactive input indices: \", active_input_indices)\n",
    "active_hidden_nodes = int(hidden_nodes * active_hidden_percentage)\n",
    "active_hidden_indices = sorted(random.sample(range(0, hidden_nodes), \n",
    "                             active_hidden_nodes))\n",
    "print(\"active hidden indices: \", active_hidden_indices)\n",
    "\n",
    "wih_old = wih.copy()\n",
    "wih = wih[:, active_input_indices]\n",
    "print(\"\\nwih after deactivating input nodes:\\n\", wih)\n",
    "wih = wih[active_hidden_indices]\n",
    "print(\"\\nwih after deactivating hidden nodes:\\n\", wih)\n",
    "\n",
    "\n",
    "who_old = who.copy()\n",
    "who = who[:, active_hidden_indices]\n",
    "print(\"\\nwih after deactivating hidden nodes:\\n\", who)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 learning_rate,\n",
    "                 bias=None\n",
    "                ):  \n",
    "\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes, \n",
    "                                                   self.no_of_in_nodes + bias_node))\n",
    "\n",
    "        n = (self.no_of_hidden_nodes + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes + bias_node)))\n",
    "\n",
    "    def dropout_weight_matrices(self,\n",
    "                                active_input_percentage=0.70,\n",
    "                                active_hidden_percentage=0.70):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        self.no_of_hidden_nodes_orig = self.no_of_hidden_nodes\n",
    "        self.who_orig = self.who.copy()\n",
    "        \n",
    "\n",
    "        active_input_nodes = int(self.no_of_in_nodes * active_input_percentage)\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), \n",
    "                                      active_input_nodes))\n",
    "        active_hidden_nodes = int(self.no_of_hidden_nodes * active_hidden_percentage)\n",
    "        active_hidden_indices = sorted(random.sample(range(0, self.no_of_hidden_nodes), \n",
    "                                       active_hidden_nodes))\n",
    "        \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices]       \n",
    "        self.who = self.who[:, active_hidden_indices]\n",
    "        \n",
    "        self.no_of_hidden_nodes = active_hidden_nodes\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        return active_input_indices, active_hidden_indices\n",
    "    \n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices):\n",
    "        \n",
    "        \"\"\"\n",
    "        self.wih and self.who contain the newly adapted values from the active nodes.\n",
    "        We have to reconstruct the original weight matrices by assigning the new values \n",
    "        from the active nodes\n",
    "        \"\"\"\n",
    " \n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "\n",
    "        self.who_orig[:, active_hidden_indices] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes = self.no_of_hidden_nodes_orig\n",
    " \n",
    "           \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        \"\"\" \n",
    "        input_vector and target_vector can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    " \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "\n",
    "        output_vector1 = np.dot(self.wih, input_vector)\n",
    "        output_vector_hidden = activation_function(output_vector1)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, output_vector_hidden)\n",
    "        output_vector_network = activation_function(output_vector2)\n",
    "        \n",
    "        output_errors = target_vector - output_vector_network\n",
    "        # update the weights:\n",
    "        tmp = output_errors * output_vector_network * (1.0 - output_vector_network)     \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n",
    "        self.who += tmp\n",
    "\n",
    "\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.wih += self.learning_rate * x\n",
    "\n",
    "            \n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              active_input_percentage=0.70,\n",
    "              active_hidden_percentage=0.70,\n",
    "              no_of_dropout_tests = 10):\n",
    "\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "            for start in range(0, len(data_array), partition_length):\n",
    "                active_in_indices, active_hidden_indices = \\\n",
    "                           self.dropout_weight_matrices(active_input_percentage,\n",
    "                                                        active_hidden_percentage)\n",
    "                for i in range(start, start + partition_length):\n",
    "                    self.train_single(data_array[i][active_in_indices], \n",
    "                                     labels_one_hot_array[i]) \n",
    "                    \n",
    "                self.weight_matrices_reset(active_in_indices, active_hidden_indices)\n",
    "\n",
    "      \n",
    "\n",
    "    \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = {}\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            if (target, res_max) in cm:\n",
    "                cm[(target, res_max)] += 1\n",
    "            else:\n",
    "                cm[(target, res_max)] = 1\n",
    "        return cm\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "        \n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        output_vector = np.dot(self.wih, input_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "        \n",
    "        if self.bias:\n",
    "            output_vector = np.concatenate( (output_vector, [[self.bias]]) )\n",
    "            \n",
    "\n",
    "        output_vector = np.dot(self.who, output_vector)\n",
    "        output_vector = activation_function(output_vector)\n",
    "    \n",
    "        return output_vector\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/mnist/pickled_mnist.pkl\", \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "0 6000\n",
      "6000 12000\n",
      "12000 18000\n",
      "18000 24000\n",
      "24000 30000\n",
      "30000 36000\n",
      "36000 42000\n",
      "42000 48000\n",
      "48000 54000\n",
      "54000 60000\n"
     ]
    }
   ],
   "source": [
    "parts = 10\n",
    "partition_length = int(len(train_imgs) / parts)\n",
    "print(partition_length)\n",
    "\n",
    "start = 0\n",
    "for start in range(0, len(train_imgs), partition_length):\n",
    "    print(start, start + partition_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "simple_network = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                               no_of_out_nodes = 10, \n",
    "                               no_of_hidden_nodes = 100,\n",
    "                               learning_rate = 0.1)\n",
    "    \n",
    "    \n",
    " \n",
    "simple_network.train(train_imgs, \n",
    "                     train_labels_one_hot, \n",
    "                     active_input_percentage=1,\n",
    "                     active_hidden_percentage=1,\n",
    "                     no_of_dropout_tests = 100,\n",
    "                     epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accruracy train:  0.9317833333333333\n",
      "accruracy: test 0.9296\n"
     ]
    }
   ],
   "source": [
    "corrects, wrongs = simple_network.evaluate(train_imgs, train_labels)\n",
    "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = simple_network.evaluate(test_imgs, test_labels)\n",
    "print(\"accuracy: test\", corrects / ( corrects + wrongs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
