{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Categorization and Classification\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "\n",
    "<img width=\"30%\" class=\"imgright\" src=\"../images/prisma.webp\" srcset=\"../images/prisma_350w.webp 350w,../images/prisma_300w.webp 300w\" alt=\"Prisma as a symbol of Text classification\" />\n",
    "\n",
    "\n",
    "Document classification/categorization is a topic in information science, a science dealing with the \n",
    "collection, analysis, classification, categorization, manipulation, retrieval, storage and propagation of information.\n",
    "<br><br>\n",
    "This might sound very abstract, but there are lots of situations nowadays,\n",
    "where companies are in need of automatic classification or categorization of documents. Just think\n",
    "about a large company with thousands of incoming mail pieces per day, both electronic or paper based.\n",
    "Lot's of these mail pieces are without specific addressee names or departments. \n",
    "Somebody has to read these texts and has to decide what kind of a letter it is (\"change of address\", \n",
    "\"complaints letter\", \"inquiry about products\", and so on) and to whom the document should be proceeded.\n",
    "This \"somebody\" can be an automated text classification system.  \n",
    "<br><br>\n",
    "\n",
    "<img class=\"imgright\" src=\"../images/supervised_learning.webp\" srcset=\"../images/supervised_learning_400w.webp 400w,../images/supervised_learning_350w.webp 350w,../images/supervised_learning_300w.webp 300w\" alt=\"Supervised Learning and Prediction\">\n",
    "\n",
    "\n",
    "Automated text classification, also called categorization of texts, has a history, which dates back to the beginning of the 1960s. But the incredible increase in available online documents in the last two decades, due to the expanding internet, has intensified and renewed the interest in automated document classification and data mining. In the beginning text classification focussed on heuristic methods, i.e. solving the task by applying a set of rules based on expert knowledge. This approach proved to be highly inefficient, so nowadays the focus has turned to fully automatic learning and clustering methods.\n",
    "\n",
    "\n",
    "The task of text classification consists in assigning a document to one or more categories, based on the semantic content of the document. Document (or text) classification runs in two modes:\n",
    "\n",
    "- The training phase and the\n",
    "- prediction (or classification) phase. \n",
    "\n",
    "\n",
    "The training phase can be divided into three kinds:\n",
    "\n",
    "- supervised document classification is performed by an external mechanism, usually human feedback, which provides the necessary information for the correct classification of documents,\n",
    "- semi-supervised document classification, a mixture between supervised and unsupervised classification: some documents or parts of documents are labelled by external assistance,\n",
    "- unsupervised document classification is entirely executed without reference to external information.\n",
    "\n",
    "\n",
    "We will implement a text classifier in Python using Naive Bayes. Naive Bayes is the most commonly used text classifier and it is the focus of research in text classification. A Naive Bayes classifier is based on the application of Bayes' theorem with strong independence assumptions. \"Strong independence\" means: the presence or absence of a particular feature of a class is unrelated to the presence or absence of any other feature. Naive Bayes is well suited for multiclass text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "<p>\n",
    "Let C = { c<sub>1</sub>,\n",
    "c<sub>2</sub>, ... c<sub>m</sub>} be a set of categories (classes)\n",
    "and D = { d<sub>1</sub>, d<sub>2</sub>,  ... d<sub>n</sub>} a set of\n",
    "documents. \n",
    "</p>\n",
    "\n",
    "<p style=\"margin-bottom: 0cm\">\n",
    "The task of the text classification consists in assigning to each\n",
    "pair ( c<sub>i</sub>, d<sub>j </sub>) of  C x D (with 1 ≤\n",
    "i ≤ m  and 1 ≤\n",
    "j ≤ n) a value of 0 or 1, i.e. the\n",
    "value 0, if the document d<sub>j</sub> doesn't belong to c<sub>i</sub></p>\n",
    "<br>\n",
    "This\n",
    "mapping is sometimes referred to as the decision matrix:\n",
    "<br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | $d_1$  | ... | $d_j$  | ... | $d_n$  |\n",
    "|-----|------|-----|------|-----|------|\n",
    "| $c_1$ | $a_{11}$ | ... | $a_{1j}$ | ... | $a_{1n}$ |\n",
    "| ... | ...  | ... | ...  | ... | ...  |\n",
    "| $c_i$ | $a_{i1}$ | ... | $a_{ij}$ | ... | $a_{in}$ |\n",
    "| ... | ...  | ... | ...  | ... | ...  |\n",
    "| $c_m$ | $a_{m1}$ | ... | $a_{mj}$ | ... | $a_{mn}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main approaches to solve this task are:\n",
    "\n",
    "- Naive Bayes\n",
    "- Support Vector Machine\n",
    " - Nearest Neighbour\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "A Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naïve) independence assumptions, i.e. an \"independent feature model\".\n",
    "In other words: A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature. \n",
    "\n",
    "#### Formal Derivation of the Naive Bayes Classifier:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"imgright\" src=\"../images/text_classification_sphere.webp\" srcset=\"../images/text_classification_sphere_300w.webp 300w\" alt=\"Text classification formula as a sphere\">\n",
    "\n",
    "Let C = { c<sub>1</sub>,\n",
    "c<sub>2</sub>, ... c<sub>m</sub>} be a set of classes or categories \n",
    "and D = { d<sub>1</sub>, d<sub>2</sub>,  ... d<sub>n</sub>} be a set of\n",
    "documents. <br>\n",
    "Each document is labeled with a class.\n",
    "<br>\n",
    "The set D of documents is used to train the classifier.\n",
    "<br>\n",
    "Classification consists in selecting the most probable class for an unknown document.  \n",
    "<br><br>\n",
    "The number of times a word w<sub>t</sub> occurs within a document d<sub>i</sub> will be denoted as N<sub>it</sub>.\n",
    "N<sub>t</sub><sup>C</sup>     denotes the number of times a word w<sub>t</sub> ocurs in all documents of a given \n",
    "class C.\n",
    "<br>\n",
    "P(d<sub>i</sub>|c<sub>j</sub>) is 1, if d<sub>i</sub> is labelled as c<sub>j</sub>, 0 otherwise \n",
    "<br><br>\n",
    "The probability for a word $w_t$ given a class $c_j$:\n",
    "<br><br>\n",
    "<img class=\"img\" src=\"../images/naive_bayes_probability1.webp\" alt=\"Probability of a word given a class\" height=\"60\">\n",
    "<br>\n",
    "The probability for a class c<sub>j</sub> is the quotient of the number of Documents of c<sub>j</sub> and the \n",
    "number of documents of all classes, i.e. the learn set:\n",
    "<br><br>\n",
    "<img class=\"img\" src=\"../images/naive_bayes_probability2.webp\" alt=\"Probability of a document class\" height=\"60\">\n",
    "<br><br>\n",
    "Finally, we come to the formula we need to classify an unknown document, i.e. the probability for a class \n",
    "c<sub>j</sub> given a document d<sub>i</sub>:\n",
    "<br><br>\n",
    "<img class=\"img\" src=\"../images/naive_bayes_probability3.webp\" alt=\"The classification formula: the probability of a class given a document\" height=\"80\">\n",
    "<br>\n",
    "Unfortunately, the formula of P(c|d<sub>i</sub>) we have just given is numerically not stable, because the \n",
    "denominator can be zero due to rounding errors. We change this by calculating the reciprocal and reformulate the\n",
    "expression as a sum of stable quotients:\n",
    "<br><br>\n",
    "<img class=\"img\" src=\"../images/naive_bayes_formula.webp\" srcset=\"../images/naive_bayes_formula_600w.webp 600w,../images/naive_bayes_formula_500w.webp 500w,../images/naive_bayes_formula_400w.webp 400w,../images/naive_bayes_formula_350w.webp 350w,../images/naive_bayes_formula_300w.webp 300w\" alt=\"The classification formula: Numerically stable\" height=\"170\">\n",
    "<br><br>\n",
    "We can rewrite the previous formula into the following form, our final Naive Bayes classification formula, the\n",
    "one we will use in our Python implementation in the following chapter:\n",
    "<br><br>\n",
    "<img class=\"img\" src=\"../images/naive_bayes_formula_final.webp\" srcset=\"../images/naive_bayes_formula_final_600w.webp 600w,../images/naive_bayes_formula_final_500w.webp 500w,../images/naive_bayes_formula_final_400w.webp 400w,../images/naive_bayes_formula_final_350w.webp 350w,../images/naive_bayes_formula_final_300w.webp 300w\" alt=\"The classification formula: Numerically stable\" height=\"110\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "There are lots of articles on text classification. We just name a few, which we have used for our work:\n",
    "<ul>\n",
    "<li>Fabrizio Sebastiani. A tutorial on automated text categorisation. In Analia Amandi and Alejandro Zunino (eds.), Proceedings of the 1st Argentinian Symposium on Artificial Intelligence (ASAI'99), Buenos Aires, AR, 1999, pp. 7-35. </li>\n",
    "<li>Lewis, David D., Naive (Bayes) at Forty: The independence assumption in informal retrieval, Lecture Notes in Computer Science (1998), 1398, Issue: 1398, Publisher: Springer, Pages: 4-15</li>\n",
    "<li>K. Nigam, A. McCallum, S. Thrun and T. Mitchell, Text classification from labeled and unlabeled documents \n",
    "using EM, Machine Learning 39 (2000) (2/3), pp. 103-134.</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
