{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text for Machine Learning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<img width=40% class=\"imgright\" src=\"../images/bookshelf.webp\" srcset=\"../images/bookshelf_800w.webp 800w,../images/bookshelf_700w.webp 700w,../images/bookshelf_600w.webp 600w,../images/bookshelf_500w.webp 500w,../images/bookshelf_400w.webp 400w,../images/bookshelf_350w.webp 350w,../images/bookshelf_300w.webp 300w\" alt=\"bookshelf\" />\n",
    "\n",
    "We mentioned in the introductory chapter of our tutorial that a spam filter for emails is a typical example of machine learning. Emails are based on text, which is why a classifier to classify emails must be able to process text as input. If we look at the previous examples with neural networks, they always run directly with numerical values and have a fixed input length. In the end, the characters of a text also consist of numerical values, but it is obvious that we cannot simply use a text as it is as input for a neural network.  This means that the text have to be converted into a numerical representation, e.g. vectors or arrays of numbers.\n",
    "\n",
    "We will learn in this tutorial how to encode text in a way which is suitable for machine processing. \n",
    "\n",
    "### Bag-of-Words Model\n",
    "\n",
    "\n",
    "If we want to use texts in machine learning, we need a representation of the text which is usable for Machine Learning purposes. This means we need a numerical representation. We cannot use texts directly. \n",
    "\n",
    "\n",
    "<img width=\"60%\"  src=\"../images/bag_of_words_visualization.webp\" srcset=\"../images/bag_of_words_visualization_800w.webp 800w,../images/bag_of_words_visualization_700w.webp 700w,../images/bag_of_words_visualization_600w.webp 600w,../images/bag_of_words_visualization_500w.webp 500w,../images/bag_of_words_visualization_400w.webp 400w,../images/bag_of_words_visualization_350w.webp 350w,../images/bag_of_words_visualization_300w.webp 300w\" alt=\"Bag of Words\" />\n",
    "\n",
    "In natural language processing and information retrievel the bag-of-words model is of crucial importance. The bag-of-words model can be used to represent text data in a way which is suitable for machine learning algorithms. Furthermore, this model is easy and efficient to implement. In the bag-of-words model, a text (such as a sentence or a document) is represented as the so-called bag (a set or multiset) of its words. Grammar and word order are ignored. \n",
    "\n",
    "We will use in the following a list of three strings to demonstrate the bag-of-words approach. In linguistics, the collection of texts used for the experiments or tests is usually called a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"To be, or not to be, that is the question:\",\n",
    "          \"Whether 'tis nobler in the mind to suffer\",\n",
    "          \"The slings and arrows of outrageous fortune,\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the submodule ```text``` from ```sklearn.feature_extraction```. This module contains utilities to build feature vectors from text documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```CountVectorizer``` is a class in the module ```sklearn.feature_extraction.text```. It's a class useful for building a corpus vocabulary. In addition, it produces the numerical representation of text that we need, i.e. Numpy vectors.\n",
    "\n",
    "First we need an instance of this class. When we instantiate a CountVectorizer, we can pass some optional parameters, but it is possible to call it with no arguments, as we will do in the following. Printing the ```vectorizer``` gives us useful information about the default values used when the instance was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer()\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now an instance of CountVectorizer, but it has not seen any texts so far. We will use the method ```fit``` to process our previously defined corpus. We learn a vocabulary dictionary of all the tokens (strings) of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```fit```  created the vocabulary structure ```vocabulary_```. This contains the words of the text as keys and a unique integer value for each word. As the default value for the parameter ```lowercase``` is set to ```True```, the ```To``` in the beginning of the text has been turned into ```to```. You may also notice that the vocabulary contains only words without any punctuation or special character. You can change this behaviour by assigning a regular expression to the keyword parameter ```token_pattern``` of the ```fit``` method. The default is set to ```(?u)\\\\b\\\\w\\\\w+\\\\b```. The ```(?u)```  part of this regular expression is not necessary because it switches on the ```re.U``` (```re.UNICODE```) flag for this expression, which is the default in Python anyway. The minimal word length will be two characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'to': 18, 'be': 2, 'or': 10, 'not': 8, 'that': 15, 'is': 5, 'the': 16, 'question': 12, 'whether': 19, 'tis': 17, 'nobler': 7, 'in': 4, 'mind': 6, 'suffer': 14, 'slings': 13, 'and': 0, 'arrows': 1, 'of': 9, 'outrageous': 11, 'fortune': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary: \", vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to see the words without the indices, you can your the method ```feature_names```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'arrows', 'be', 'fortune', 'in', 'is', 'mind', 'nobler', 'not', 'of', 'or', 'outrageous', 'question', 'slings', 'suffer', 'that', 'the', 'tis', 'to', 'whether']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can apply ```keys``` to the vocaulary to keep the ordering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'be', 'or', 'not', 'that', 'is', 'the', 'question', 'whether', 'tis', 'nobler', 'in', 'mind', 'suffer', 'slings', 'and', 'arrows', 'of', 'outrageous', 'fortune']\n"
     ]
    }
   ],
   "source": [
    "print(list(vectorizer.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the aid of ```transform``` we will extract the token counts out of the raw text documents. The call will use the vocabulary which we created with ```fit```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 19)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "token_count_matrix = vectorizer.transform(corpus)\n",
    "print(token_count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection between the corpus, the Vocabulary ```vocabulary_``` and the vector created by ```transform``` can be seen in the following image:\n",
    "\n",
    "<img width=\"70%\"  src=\"../images/corpus_words_transform_bag_of_words.webp\" srcset=\"../images/corpus_words_transform_bag_of_words_800w.webp 800w,../images/corpus_words_transform_bag_of_words_700w.webp 700w,../images/corpus_words_transform_bag_of_words_600w.webp 600w,../images/corpus_words_transform_bag_of_words_500w.webp 500w,../images/corpus_words_transform_bag_of_words_400w.webp 400w,../images/corpus_words_transform_bag_of_words_350w.webp 350w,../images/corpus_words_transform_bag_of_words_300w.webp 300w\" alt=\"BoW, Corpus and Transform vector\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the method ```toarray``` on our object ```token_count_matrix```. It will return a dense ndarray representation of this matrix.\n",
    "\n",
    "Just in case: You might see that people use sometimes ```todense``` instead of ```toarray```.  \n",
    "Do not use todense!<a href=\"#footnote1\"><sup>1</sup></a><a name=\"footnote1_ret\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 2, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_tcm = token_count_matrix.toarray()\n",
    "dense_tcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of this array correspond to the strings of our corpus. The length of a row corresponds to the length of the vocabulary. The i'th value in a row corresponds to the i'th entry of the list returned by CountVectorizer method get_feature_names. If the value of ```dense_tcm[i][j]``` is equal to ```k```, we know the word with the index```j```  in the vocabulary occurs ```k``` times in the string with the index ```i``` in the corpus. \n",
    "\n",
    "This is visualized in the following diagram:\n",
    "\n",
    "<img width=\"80%\"  src=\"../images/dense_tcm_words.webp\" srcset=\"../images/dense_tcm_words_800w.webp 800w,../images/dense_tcm_words_700w.webp 700w,../images/dense_tcm_words_600w.webp 600w,../images/dense_tcm_words_500w.webp 500w,../images/dense_tcm_words_400w.webp 400w,../images/dense_tcm_words_350w.webp 350w,../images/dense_tcm_words_300w.webp 300w\" alt=\"Dense Array and words of vocabulary\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "be\n",
      "or\n",
      "not\n",
      "that\n",
      "is\n",
      "the\n",
      "question\n",
      "whether\n",
      "tis\n",
      "nobler\n",
      "in\n",
      "mind\n",
      "suffer\n",
      "slings\n",
      "and\n",
      "arrows\n",
      "of\n",
      "outrageous\n",
      "fortune\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "for el in vectorizer.vocabulary_:\n",
    "    print(el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>arrows</th>\n",
       "      <th>be</th>\n",
       "      <th>fortune</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>mind</th>\n",
       "      <th>nobler</th>\n",
       "      <th>not</th>\n",
       "      <th>of</th>\n",
       "      <th>or</th>\n",
       "      <th>outrageous</th>\n",
       "      <th>question</th>\n",
       "      <th>slings</th>\n",
       "      <th>suffer</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>tis</th>\n",
       "      <th>to</th>\n",
       "      <th>whether</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>corpus_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpus_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpus_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          and  arrows  be  fortune  in  is  mind  nobler  not  of  or  \\\n",
       "corpus_0    0       0   2        0   0   1     0       0    1   0   1   \n",
       "corpus_1    0       0   0        0   1   0     1       1    0   0   0   \n",
       "corpus_2    1       1   0        1   0   0     0       0    0   1   0   \n",
       "\n",
       "          outrageous  question  slings  suffer  that  the  tis  to  whether  \n",
       "corpus_0           0         1       0       0     1    1    0   2        0  \n",
       "corpus_1           0         0       0       1     0    1    1   1        1  \n",
       "corpus_2           1         0       1       0     0    1    0   0        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data=dense_tcm, \n",
    "             index=['corpus_0', 'corpus_1', 'corpus_2'],\n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of times 'be' occurs in:\n",
      "    'To be, or not to be, that is the question:': 2\n",
      "    'Whether 'tis nobler in the mind to suffer': 0\n",
      "    'The slings and arrows of outrageous fortune,': 0\n"
     ]
    }
   ],
   "source": [
    "word = \"be\"\n",
    "i = 1\n",
    "j = vectorizer.vocabulary_[word]\n",
    "print(\"number of times '\" + word + \"' occurs in:\")\n",
    "for i in range(len(corpus)):\n",
    "     print(\"    '\" + corpus[i] + \"': \" + str(dense_tcm[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the token counts out of new text documents. Let's use a literally doubtful variation of Hamlet's famous monologue and check what ```transform``` has to say about it.  ```transform``` will use the vocabulary which was previously fitted with fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"That is the question and it is nobler in the mind.\"\n",
    "vectorizer.transform([txt]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'arrows', 'be', 'fortune', 'in', 'is', 'mind', 'nobler', 'not', 'of', 'or', 'outrageous', 'question', 'slings', 'suffer', 'that', 'the', 'tis', 'to', 'whether']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 18, 'be': 2, 'or': 10, 'not': 8, 'that': 15, 'is': 5, 'the': 16, 'question': 12, 'whether': 19, 'tis': 17, 'nobler': 7, 'in': 4, 'mind': 6, 'suffer': 14, 'slings': 13, 'and': 0, 'arrows': 1, 'of': 9, 'outrageous': 11, 'fortune': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Importance\n",
    "\n",
    "\n",
    "If you look at words like \"the\", \"and\" or \"of\", you will see see that they will occur in nearly all English texts.  If you keep in mind that our ultimate goal will be to differentiate between texts and attribute them to classes, words like the previously mentioned ones will bear hardly any meaning. If you look at the following corpus, you can see words like \"you\", \"I\" or important words like \"Python\", \"lottery\" or \"Programmer\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "corpus = [\"It does not matter what you are doing, just do it!\",\n",
    "          \"Would you work if you won the lottery?\",\n",
    "          \"You like Python, he likes Python, we like Python, everybody loves Python!\"\n",
    "          \"You said: 'I wish I were a Python programmer'\",\n",
    "          \"You can stay here, if you want to. I would, if I were you.\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t2\n",
      "  (0, 10)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 31)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 31)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 11)\t2\n",
      "  (2, 12)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 18)\t5\n",
      "  (2, 19)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 25)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 31)\t2\n",
      "  (3, 1)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 8)\t2\n",
      "  (3, 20)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 23)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 31)\t3\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "token_count_matrix = vectorizer.transform(corpus)\n",
    "print(token_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.51082562, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.51082562, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.51082562, 1.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf_idf = text.TfidfTransformer()\n",
    "tf_idf.fit(token_count_matrix)\n",
    "\n",
    "tf_idf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.916290731874155"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.idf_[vectorizer.vocabulary_['python']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = vectorizer.transform(corpus).toarray()\n",
    "i = 0\n",
    "\n",
    "# check how often the word 'would' occurs in the the i'th sentence:\n",
    "#vectorizer.vocabulary_['would']\n",
    "word_ind = vectorizer.vocabulary_['would']\n",
    "da[i][word_ind]\n",
    "da[:,word_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you            : 1.000\n",
      "if             : 1.511\n",
      "were           : 1.511\n",
      "would          : 1.511\n",
      "are            : 1.916\n",
      "can            : 1.916\n",
      "do             : 1.916\n",
      "does           : 1.916\n",
      "doing          : 1.916\n",
      "everybody      : 1.916\n",
      "he             : 1.916\n",
      "here           : 1.916\n",
      "it             : 1.916\n",
      "just           : 1.916\n",
      "like           : 1.916\n",
      "likes          : 1.916\n",
      "lottery        : 1.916\n",
      "loves          : 1.916\n",
      "matter         : 1.916\n",
      "not            : 1.916\n",
      "programmer     : 1.916\n",
      "python         : 1.916\n",
      "said           : 1.916\n",
      "stay           : 1.916\n",
      "the            : 1.916\n",
      "to             : 1.916\n",
      "want           : 1.916\n",
      "we             : 1.916\n",
      "what           : 1.916\n",
      "wish           : 1.916\n",
      "won            : 1.916\n",
      "work           : 1.916\n"
     ]
    }
   ],
   "source": [
    "word_weight_list = list(zip(vectorizer.get_feature_names(), tf_idf.idf_))\n",
    "\n",
    "word_weight_list.sort(key=lambda x:x[1])  # sort list by the weights (2nd component)\n",
    "for word, idf_weight in word_weight_list:\n",
    "    print(f\"{word:15s}: {idf_weight:4.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "corpus = [\"It does not matter what you are doing, just do it!\",\n",
    "          \"Would you work if you won the lottery?\",\n",
    "          \"You like Python, he likes Python, we like Python, everybody loves Python!\"\n",
    "          \"You said: 'I wish I were a Python programmer'\",\n",
    "          \"You can stay here, if you want to. I would, if I were you.\"\n",
    "         ]\n",
    "\n",
    "n = len(corpus)\n",
    "\n",
    "\n",
    "# the following variables are used globally (as free variables) in the functions :-(\n",
    "vectorizer = text.CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "da = vectorizer.transform(corpus).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency\n",
    "\n",
    "We will first define a function for the term frequency.\n",
    "\n",
    "Some notations: \n",
    "\n",
    "- $f_{t, d}$ denotes the number of times that a term t occurs in document d\n",
    "- ${wc}_d$ denotes the number of words in a document $d$\n",
    "\n",
    "The simplest choice to define tf(t,d) is to use the raw count of a term in a document, i.e., the number of times that term t occurs in document d, which we can denote as $f_{t, d}$\n",
    "\n",
    "We can define tf(t, d) in different ways:\n",
    "\n",
    "- raw count of a term: $tf(t, d) = f_{t, d}$\n",
    "- term frequency adjusted for document length: $tf(t, d) = \\frac{f_{t, d}}{{wc}_d}$  \n",
    "- logarithmically scaled frequency: $tf(t, d) = \\log(1 + f_{t, d})$  \n",
    "- augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency of the term divided by the raw frequency of the most occurring term in the document: \n",
    "$tf(t, d) = 0.5 + 0.5 \\cdot \\frac{f_{t, d}}{\\max_{t' \\in d}{\\{f_{t', d}\\}} }$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(t, d, mode=\"raw\"):\n",
    "    \"\"\" The Term Frequency 'tf' calculates how often a term 't' \n",
    "        occurs in a document 'd'.  ('d': document index)\n",
    "        If t_in_d =  Number of times a term t appears in a document d\n",
    "        and no_terms_d = Total number of terms in the document, \n",
    "        tf(t, d) = t_in_d / no_terms_d\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if t in vectorizer.vocabulary_:\n",
    "        word_ind = vectorizer.vocabulary_[t]\n",
    "        t_occurences = da[d, word_ind]    # 'd' is the document index\n",
    "    else:\n",
    "        t_occurences = 0\n",
    "    if mode == \"raw\":\n",
    "        result =  t_occurences\n",
    "    elif mode == \"length\":\n",
    "        all_terms = (da[d] > 0).sum()  # calculate number of different terms in d\n",
    "        result = t_occurences / all_terms\n",
    "    elif mode == \"log\":\n",
    "        result = log(1 + t_occurences)\n",
    "    elif mode == \"augfreq\":\n",
    "        result = 0.5 + 0.5 * t_occurences / da[d].max()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the word frequencies for some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   raw    length log   augmented freq\n",
      "\n",
      "'matter' in 'It does not matter what you are doing, just do it!''\n",
      "   1.00   0.10   0.69   0.75\n",
      "'matter' in 'Would you work if you won the lottery?''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'matter' in 'You like Python, he likes Python, we like Python, everybody loves Python!You said: 'I wish I were a Python programmer'''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'matter' in 'You can stay here, if you want to. I would, if I were you.''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'python' in 'It does not matter what you are doing, just do it!''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'python' in 'Would you work if you won the lottery?''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'python' in 'You like Python, he likes Python, we like Python, everybody loves Python!You said: 'I wish I were a Python programmer'''\n",
      "   5.00   0.42   1.79   1.00\n",
      "'python' in 'You can stay here, if you want to. I would, if I were you.''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'would' in 'It does not matter what you are doing, just do it!''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'would' in 'Would you work if you won the lottery?''\n",
      "   1.00   0.14   0.69   0.75\n",
      "'would' in 'You like Python, he likes Python, we like Python, everybody loves Python!You said: 'I wish I were a Python programmer'''\n",
      "   0.00   0.00   0.00   0.50\n",
      "'would' in 'You can stay here, if you want to. I would, if I were you.''\n",
      "   1.00   0.11   0.69   0.67"
     ]
    }
   ],
   "source": [
    "print(\"   raw    length log   augmented freq\")\n",
    "for term in ['matter', 'python', 'would']:\n",
    "    for docu_index in range(len(corpus)):\n",
    "        d = corpus[docu_index]\n",
    "        print(f\"\\n'{term}' in '{d}''\")\n",
    "        for mode in ['raw', 'length', 'log', 'augfreq']:\n",
    "            x = tf(term, docu_index, mode=mode)\n",
    "            print(f\"{x:7.2f}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Frequency\n",
    "\n",
    "The document frequency df of a term t is defined as the number of documents in the document set that contain the term t.\n",
    "\n",
    "$df(t) = |{\\{d \\in D: t \\in d\\}}|$\n",
    "\n",
    "#### Inverse Document Frequency\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the document frequency. The effect of adding ```1``` to the idf in the equation above is that terms with zero idf, i.e., terms  that occur in all documents in a training set, will not be entirely ignored.\n",
    "\n",
    "$idf(t) = log( \\frac{n}{df(t)} ) + 1$\n",
    "\n",
    "\n",
    "\n",
    "n is the number of documents in the corpus $n = |D|$\n",
    "\n",
    "(Note that the idf formula above differs from the standard textbook notation that defines the idf as \n",
    "$idf(t) = log( \\frac{n}{df(t) + 1})$.)\n",
    "\n",
    "The formula above is used, when TfidfTransformer() is called with ```smooth_idf=False```!\n",
    "If it is called with ```smooth_idf=True``` (the default) the constant ```1``` is added to the numerator and denominator of the idf as if an extra document was seen  containing every term in the collection exactly once, which prevents zero divisions:\n",
    "\n",
    "$idf(t) = log( \\frac{n + 1}{df(t) + 1} ) + 1$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Term frequency–Inverse document frequency \n",
    "\n",
    "$tf_idf$ is calculated as the product of $tf(t, d)$ and $idf(t)$:\n",
    "\n",
    "$tf_idf(t, d) = tf(t, d) \\cdot idf(t)$\n",
    "\n",
    "A high value of tf–idf means that the term has a high \"term frequency\" in the given document and a low \"document frequency\" in the other documents of the corpus. This means that this wieght can be used to filter out common terms. \n",
    "\n",
    "We will program the tf_idf function now:\n",
    "\n",
    "The helpfile of ```text.TfidfTransformer``` explains how tf_idf is calculated:\n",
    "\n",
    "\n",
    "We will manually program these functions in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 1.0]\n",
      "['if', 1.5108256237659907]\n",
      "['were', 1.5108256237659907]\n",
      "['would', 1.5108256237659907]\n",
      "['are', 1.916290731874155]\n",
      "['can', 1.916290731874155]\n",
      "['do', 1.916290731874155]\n",
      "['does', 1.916290731874155]\n",
      "['doing', 1.916290731874155]\n",
      "['everybody', 1.916290731874155]\n",
      "['he', 1.916290731874155]\n",
      "['here', 1.916290731874155]\n",
      "['it', 1.916290731874155]\n",
      "['just', 1.916290731874155]\n",
      "['like', 1.916290731874155]\n",
      "['likes', 1.916290731874155]\n",
      "['lottery', 1.916290731874155]\n",
      "['loves', 1.916290731874155]\n",
      "['matter', 1.916290731874155]\n",
      "['not', 1.916290731874155]\n",
      "['programmer', 1.916290731874155]\n",
      "['python', 1.916290731874155]\n",
      "['said', 1.916290731874155]\n",
      "['stay', 1.916290731874155]\n",
      "['the', 1.916290731874155]\n",
      "['to', 1.916290731874155]\n",
      "['want', 1.916290731874155]\n",
      "['we', 1.916290731874155]\n",
      "['what', 1.916290731874155]\n",
      "['wish', 1.916290731874155]\n",
      "['won', 1.916290731874155]\n",
      "['work', 1.916290731874155]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def df(t):\n",
    "    \"\"\" df(t) is the document frequency of t; the document frequency is \n",
    "        the number of documents  in the document set that contain the term t. \"\"\"\n",
    "    \n",
    "    word_ind = vectorizer.vocabulary_[t]\n",
    "\n",
    "    tf_in_docus = da[:, word_ind] # vector with the freqencies of word_ind in all docus\n",
    "    existence_in_docus = tf_in_docus > 0 # binary vector, existence of word in docus\n",
    "    return existence_in_docus.sum()\n",
    "    \n",
    "#df(\"would\", vectorizer) \n",
    "\n",
    "def idf(t, smooth_idf=True):\n",
    "    \"\"\" idf \"\"\"\n",
    "    if smooth_idf:\n",
    "        return log((1 + n) / (1 + df(t)) ) + 1\n",
    "    else:\n",
    "        return log(n / df(t) ) + 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def tf_idf(t, d):\n",
    "    return idf(t) * tf(t, d)\n",
    "    \n",
    "\n",
    "res_idf = []\n",
    "for word  in vectorizer.get_feature_names():\n",
    "    tf_docus = []\n",
    "    res_idf.append([word, idf(word)])\n",
    "\n",
    "res_idf.sort(key=lambda x:x[1])\n",
    "for item  in res_idf:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It does not matter what you are doing, just do it!',\n",
       " 'Would you work if you won the lottery?',\n",
       " \"You like Python, he likes Python, we like Python, everybody loves Python!You said: 'I wish I were a Python programmer'\",\n",
       " 'You can stay here, if you want to. I would, if I were you.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "it          : 0 3.83, 1 0.00, 2 0.00, 3 0.00, \n",
      "does        : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "not         : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "matter      : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "what        : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "you         : 0 1.00, 1 2.00, 2 2.00, 3 3.00, \n",
      "are         : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "doing       : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "just        : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "do          : 0 1.92, 1 0.00, 2 0.00, 3 0.00, \n",
      "would       : 0 0.00, 1 1.51, 2 0.00, 3 1.51, \n",
      "work        : 0 0.00, 1 1.92, 2 0.00, 3 0.00, \n",
      "if          : 0 0.00, 1 1.51, 2 0.00, 3 3.02, \n",
      "won         : 0 0.00, 1 1.92, 2 0.00, 3 0.00, \n",
      "the         : 0 0.00, 1 1.92, 2 0.00, 3 0.00, \n",
      "lottery     : 0 0.00, 1 1.92, 2 0.00, 3 0.00, \n",
      "like        : 0 0.00, 1 0.00, 2 3.83, 3 0.00, \n",
      "python      : 0 0.00, 1 0.00, 2 9.58, 3 0.00, \n",
      "he          : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "likes       : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "we          : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "everybody   : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "loves       : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "said        : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "wish        : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "were        : 0 0.00, 1 0.00, 2 1.51, 3 1.51, \n",
      "programmer  : 0 0.00, 1 0.00, 2 1.92, 3 0.00, \n",
      "can         : 0 0.00, 1 0.00, 2 0.00, 3 1.92, \n",
      "stay        : 0 0.00, 1 0.00, 2 0.00, 3 1.92, \n",
      "here        : 0 0.00, 1 0.00, 2 0.00, 3 1.92, \n",
      "want        : 0 0.00, 1 0.00, 2 0.00, 3 1.92, \n",
      "to          : 0 0.00, 1 0.00, 2 0.00, 3 1.92, "
     ]
    }
   ],
   "source": [
    "for word, word_index in vectorizer.vocabulary_.items():\n",
    "    print(f\"\\n{word:12s}: \", end=\"\")\n",
    "    for d_index in range(len(corpus)):\n",
    "        print(f\"{d_index:1d} {tf_idf(word, d_index):3.2f}, \",  end=\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Simple Example\n",
    "\n",
    "We will use another simple example to illustrate the previously introduced concepts. We use a sentence which contains solely different words. The corpus consists of this sentence and reduced versions of it, i.e. cutting of words from the end of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cold', 'Cold wind', 'Cold wind blows', 'Cold wind blows over', 'Cold wind blows over the', 'Cold wind blows over the cornfields']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "words = \"Cold wind blows over the cornfields\".split()\n",
    "corpus = []\n",
    "for i in range(1, len(words)+1):\n",
    "    corpus.append(\" \".join(words[:i]))\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = text.CountVectorizer()\n",
    "\n",
    "vectorizer = vectorizer.fit(corpus)\n",
    "vectorized_text = vectorizer.transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.33647224, 1.        , 2.25276297, 1.55961579, 1.84729786,\n",
       "       1.15415068])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = text.TfidfTransformer()\n",
    "tf_idf.fit(vectorized_text)\n",
    "\n",
    "tf_idf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold           : 1.000\n",
      "wind           : 1.154\n",
      "blows          : 1.336\n",
      "over           : 1.560\n",
      "the            : 1.847\n",
      "cornfields     : 2.253\n"
     ]
    }
   ],
   "source": [
    "word_weight_list = list(zip(vectorizer.get_feature_names(), tf_idf.idf_))\n",
    "word_weight_list.sort(key=lambda x:x[1])  # sort list by the weights (2nd component)\n",
    "for word, idf_weight in word_weight_list:\n",
    "    print(f\"{word:15s}: {idf_weight:4.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold           : 1.000\n",
      "wind           : 1.154\n",
      "blows          : 1.336\n",
      "over           : 1.560\n",
      "the            : 1.847\n",
      "cornfields     : 2.253\n"
     ]
    }
   ],
   "source": [
    "TfidF = text.TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf = TfidF.fit_transform(vectorized_text)\n",
    "\n",
    "\n",
    "word_weight_list = list(zip(vectorizer.get_feature_names(), tf_idf.idf_))\n",
    "word_weight_list.sort(key=lambda x:x[1])  # sort list by the weights (2nd component)\n",
    "for word, idf_weight in word_weight_list:\n",
    "    print(f\"{word:15s}: {idf_weight:4.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working With Real Data\n",
    "\n",
    "scikit-learn contains a dataset from real newsgroups, which can be used for our purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create our vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Let's fetch all the possible text data\n",
    "newsgroups_data = fetch_20newsgroups()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a closer look at this data. As with all the other data sets in ```sklearn``` we can find the actual data under the attribute ```data```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: \"Proper gun control?\" What is proper gun cont\n",
      "From: kim39@scws8.harvard.edu (John Kim)\n",
      "Organization: Harvard University Science Center\n",
      "Nntp-Posting-Host: scws8.harvard.edu\n",
      "Lines: 17\n",
      "\n",
      "In article <C5JGz5.34J@SSD.intel.com> hays@ssd.intel.com (Kirk Hays) writes:\n",
      ">I'd like to point out that I was in error - \"Terminator\" began posting only \n",
      ">six months before he purchased his first firearm, according to private email\n",
      ">from him.\n",
      ">I can't produce an archived posting of his earlier than January 1992,\n",
      ">and he purchased his first firearm in March 1992.\n",
      ">I guess it only seemed like years.\n",
      ">Kirk Hays - NRA Life, seventh generation.\n",
      "\n",
      "I first read and consulted rec.guns in the summer of 1991.  I\n",
      "just purchased my first firearm in early March of this year.\n",
      "\n",
      " NOt for lack of desire for a firearm, you understand.  I could \n",
      "have purchased a rifle or shotgun but didn't want one.\n",
      "-Case Kim\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_data.data[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the ```vectorizer```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer.fit(newsgroups_data.data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first ```n``` words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 56979\n",
      "lerxst 75358\n",
      "wam 123162\n",
      "umd 118280\n",
      "edu 50527\n",
      "where 124031\n",
      "my 85354\n",
      "thing 114688\n",
      "subject 111322\n",
      "what 123984\n",
      "car 37780\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "n = 10\n",
    "for word, index in vectorizer.vocabulary_.items():\n",
    "    print(word, index)\n",
    "    counter += 1\n",
    "    if counter > n:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn the newsgroup postings into arrays. We do it with the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "a = vectorizer.transform([newsgroups_data.data[0]]).toarray()[0]\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary is huge This is why we see mostly zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130107"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of 'rubbish' words in this vocabulary. ```rubish``` means seen from the perspective of machine learning. For machine learning purposes words like 'Subject', 'From', 'Organization', 'Nntp-Posting-Host', 'Lines' and many others are useless, because they occur in all or in most postings. The technical 'garbage' from the newsgroup can be easily stripped off. We can fetch it differently. Stating that we do not want 'headers', 'footers' and 'quotes':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data_cleaned = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_data_cleaned.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the complete posting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101631"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_cleaned = vectorizer.fit(newsgroups_data_cleaned.data)\n",
    "len(vectorizer_cleaned.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got rid of more than 30000 words, but with more than a 100000 words is it still very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly separate the newsgroup feeds into a train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.6460435475305364\n",
      "F1 score:  0.6203806145034193\n"
     ]
    }
   ],
   "source": [
    "train_data = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# creating a classifier\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(train_data, newsgroups_train.target)\n",
    "\n",
    "test_data = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "accuracy_score = metrics.accuracy_score(newsgroups_test.target, \n",
    "                                        predictions)\n",
    "f1_score = metrics.f1_score(newsgroups_test.target, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score)\n",
    "print(\"F1 score: \", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "<img width=80% src=\"../images/stop_words_cloud_3.webp\" srcset=\"../images/stop_words_cloud_3_800w.webp 800w,../images/stop_words_cloud_3_700w.webp 700w,../images/stop_words_cloud_3_600w.webp 600w,../images/stop_words_cloud_3_500w.webp 500w,../images/stop_words_cloud_3_400w.webp 400w,../images/stop_words_cloud_3_350w.webp 350w,../images/stop_words_cloud_3_300w.webp 300w\" alt=\"Stop words as a cloud\" />\n",
    "\n",
    "So far we added all the words to the vocabulary. However, it is questionable whether words like \"the\", \"am\", \"were\" or similar words should be included at all, since they usually do not provide any significant semantic contribution for a text. In other words: They have limited predictive power. It would therefore make sense to exclude such words from editing, i.e. inclusion in the dictionary. This means we have to provide a list of words which should be neglected, i.e. being filtered out before or after processing text. In natural text recognition such words  are usually called \"stop words\".  There is no single universal list of stop words defined, which could be used by all natural language processing tools. Usually, stop words consist of the most frequently used words in a language. \"Stop words\" can be individually chosen for a given task. \n",
    "\n",
    "By the way, stop words are an idea which is quite old. It goes back to 1959 and Hans Peter Luhn, one of the pioneers in information retrieval. \n",
    "\n",
    "There are different ways to provide stop words in ```sklearn```:\n",
    "\n",
    "- Explicit list of stop words\n",
    "- Automatically created stop words\n",
    "\n",
    "We will start with individual stop words:\n",
    "\n",
    "#### Indivudual Stop Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'horse': 5,\n",
       " 'kingdom': 8,\n",
       " 'sense': 16,\n",
       " 'thing': 18,\n",
       " 'keeps': 7,\n",
       " 'betting': 1,\n",
       " 'people': 13,\n",
       " 'often': 11,\n",
       " 'said': 15,\n",
       " 'nothing': 10,\n",
       " 'better': 0,\n",
       " 'inside': 6,\n",
       " 'man': 9,\n",
       " 'outside': 12,\n",
       " 'spiritually': 17,\n",
       " 'well': 20,\n",
       " 'physically': 14,\n",
       " 'bigger': 2,\n",
       " 'foot': 3,\n",
       " 'heaven': 4,\n",
       " 'welcome': 19}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"A horse, a horse, my kingdom for a horse!\",\n",
    "          \"Horse sense is the thing a horse has which keeps it from betting on people.\"\n",
    "          \"I’ve often said there is nothing better for the inside of the man, than the outside of the horse.\",\n",
    "          \"A man on a horse is spiritually, as well as physically, bigger then a man on foot.\",\n",
    "          \"No heaven can heaven be, if my horse isn’t there to welcome me.\"]\n",
    "\n",
    "cv = CountVectorizer(input=corpus,\n",
    "                     stop_words=[\"my\", \"for\",\"the\", \"has\", \"than\", \"if\", \n",
    "                                 \"from\", \"on\", \"of\", \"it\", \"there\", \"ve\",\n",
    "                                 \"as\", \"no\", \"be\", \"which\", \"isn\", \"to\", \n",
    "                                 \"me\", \"is\", \"can\", \"then\"])\n",
    "count_vector = cv.fit_transform(corpus)\n",
    "count_vector.shape\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sklearn``` contains default stop words, which are implemented as a ```frozenset``` and it can be accessed with ```text.ENGLISH_STOP_WORDS```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 arbitrary words from ENGLISH_STOP_WORDS:\n",
      "over, it, anywhere, all, toward, every, inc, had, been, being, without, thence, mine, whole, by, below, when, beside, nevertheless, at, beforehand, after, several, throughout, eg\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "n = 25\n",
    "print(str(n) + \" arbitrary words from ENGLISH_STOP_WORDS:\")\n",
    "counter = 0\n",
    "for word in text.ENGLISH_STOP_WORDS:\n",
    "    if counter == n - 1:\n",
    "        print(word)\n",
    "        break\n",
    "    print(word, end=\", \")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use stop words in our 20newsgroups classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.6526818906001062\n",
      "F1-score:  0.6268816896587931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS)\n",
    "\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# creating a classifier\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(vectors, newsgroups_train.target)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "predictions = classifier.predict(vectors_test)\n",
    "accuracy_score = metrics.accuracy_score(newsgroups_test.target, \n",
    "                                        predictions)\n",
    "f1_score = metrics.f1_score(newsgroups_test.target, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically Created Stop Words\n",
    "\n",
    "As in many other cases, it is a good idea to look for ways to automatically define a list of stop words. A list that is or should be ideally adapted to the problem.\n",
    "\n",
    "To automatically create a stop word list, we will start with the parameter ```min_df``` of ```CountVectorizer```. When you set this threshold parameter,  terms that have a document frequency strictly lower than the given threshold will be ignored. This value is also called cut-off in the literature. If a float value in the range of [0.0, 1.0] is used, the parameter represents a proportion of documents. An integer will be treated as absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 7,\n",
       " 'you': 9,\n",
       " 'cannot': 0,\n",
       " 'is': 3,\n",
       " 'horse': 2,\n",
       " 'my': 5,\n",
       " 'for': 1,\n",
       " 'on': 6,\n",
       " 'there': 8,\n",
       " 'man': 4}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"\"\"People say you cannot live without love, \n",
    "             but I think oxygen is more important\"\"\",\n",
    "          \"Sometimes, when you close your eyes, you cannot see.\"\n",
    "          \"A horse, a horse, my kingdom for a horse!\",\n",
    "          \"\"\"Horse sense is the thing a horse has which \n",
    "          keeps it from betting on people.\"\"\"\n",
    "          \"\"\"I’ve often said there is nothing better for \n",
    "          the inside of the man, than the outside of the horse.\"\"\",\n",
    "          \"\"\"A man on a horse is spiritually, as well as physically, \n",
    "          bigger then a man on foot.\"\"\",\n",
    "          \"\"\"No heaven can heaven be, if my horse isn’t there \n",
    "          to welcome me.\"\"\"]\n",
    "\n",
    "cv = CountVectorizer(input=corpus,\n",
    "                     min_df=2)\n",
    "count_vector = cv.fit_transform(corpus)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardly any words from our corpus text are left. Because we have only few documents (strings) in our corpus and also because these texts are very short, the number of words which occur in less then two documents is very high. We eliminated all the words which occur in less two documents.\n",
    "\n",
    "We can also see the words which have been chosen as stopwords by looking at ```cv.stop_words_```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'as',\n",
       " 'be',\n",
       " 'better',\n",
       " 'betting',\n",
       " 'bigger',\n",
       " 'but',\n",
       " 'can',\n",
       " 'close',\n",
       " 'eyes',\n",
       " 'foot',\n",
       " 'from',\n",
       " 'has',\n",
       " 'heaven',\n",
       " 'if',\n",
       " 'important',\n",
       " 'inside',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'keeps',\n",
       " 'kingdom',\n",
       " 'live',\n",
       " 'love',\n",
       " 'me',\n",
       " 'more',\n",
       " 'no',\n",
       " 'nothing',\n",
       " 'of',\n",
       " 'often',\n",
       " 'outside',\n",
       " 'oxygen',\n",
       " 'physically',\n",
       " 'said',\n",
       " 'say',\n",
       " 'see',\n",
       " 'sense',\n",
       " 'sometimes',\n",
       " 'spiritually',\n",
       " 'than',\n",
       " 'the',\n",
       " 'then',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'to',\n",
       " 've',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'when',\n",
       " 'which',\n",
       " 'without',\n",
       " 'your'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docus, size of vocabulary, stop_words list size\n",
      "         0           42192                   0\n",
      "         1           42192                   0\n",
      "         2           17066               25126\n",
      "         3           10403               31789\n",
      "         4            6637               35555\n",
      "         5            4174               38018\n"
     ]
    }
   ],
   "source": [
    "print(\"number of docus, size of vocabulary, stop_words list size\")\n",
    "for i in range(len(corpus)):\n",
    "    cv = CountVectorizer(input=corpus,\n",
    "                         min_df=i)\n",
    "    count_vector = cv.fit_transform(corpus)\n",
    "    len_voc = len(cv.vocabulary_)\n",
    "    len_stop_words = len(cv.stop_words_)\n",
    "    print(f\"{i:10d} {len_voc:15d} {len_stop_words:19d}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter of ```CountVectorizer``` with which we can create a corpus-specific stop_words_list is ```max_df```. \n",
    "It can be a float values between 0.0 and 1.0 or an integer. the default value is 1.0, i.e. the float value 1.0 and not an integer 1! When building the vocabulary all terms that have a document frequency strictly higher than the given threshold will be ignored. If this parameter is given as a float betwenn 0.0 and 1.0.,  the parameter represents a proportion of documents. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "Let us use again our previous corpus for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jumped',\n",
       " 'remains',\n",
       " 'swart',\n",
       " 'pendant',\n",
       " 'pier',\n",
       " 'felicity',\n",
       " 'senor',\n",
       " 'solidity',\n",
       " 'regularly',\n",
       " 'escape',\n",
       " 'adds',\n",
       " 'dirty',\n",
       " 'struggled',\n",
       " 'meadow',\n",
       " 'differences',\n",
       " 'poser',\n",
       " 'comparative',\n",
       " 'jerkin',\n",
       " 'pleasant',\n",
       " 'principal',\n",
       " 'hangs',\n",
       " 'spiral',\n",
       " 'connection',\n",
       " 'diametrically',\n",
       " 'xxviii',\n",
       " 'magistrate',\n",
       " 'wickedly',\n",
       " 'battened',\n",
       " 'willy',\n",
       " 'breakfasting',\n",
       " 'invented',\n",
       " 'ejaculation',\n",
       " 'confer',\n",
       " 'anderson',\n",
       " 'pupils',\n",
       " '92',\n",
       " 'click',\n",
       " 'alight',\n",
       " 'hoofs',\n",
       " 'disasters',\n",
       " 'monosyllables',\n",
       " 'admirers',\n",
       " 'traffic',\n",
       " 'ushered',\n",
       " 'littleness',\n",
       " 'labors',\n",
       " 'telegraph',\n",
       " 'disembodied',\n",
       " 'delude',\n",
       " 'lawless',\n",
       " 'conduct',\n",
       " 'belie',\n",
       " 'morning',\n",
       " 'deeds',\n",
       " 'manners',\n",
       " 'foot',\n",
       " 'politeness',\n",
       " 'persia',\n",
       " 'ruler',\n",
       " 'divorced',\n",
       " 'vainly',\n",
       " 'opens',\n",
       " 'pellet',\n",
       " 'palace',\n",
       " 'chanson',\n",
       " 'result',\n",
       " 'wipe',\n",
       " 'passed',\n",
       " 'hoot',\n",
       " 'daringly',\n",
       " 'beforehand',\n",
       " 'qualifying',\n",
       " 'gazers',\n",
       " 'exported',\n",
       " 'chuckling',\n",
       " 'shaven',\n",
       " 'prostitute',\n",
       " 'grudging',\n",
       " 'barque',\n",
       " 'companies',\n",
       " 'birthright',\n",
       " 'analysis',\n",
       " 'reserved',\n",
       " 'pre',\n",
       " 'swagger',\n",
       " 'walls',\n",
       " 'unquestionable',\n",
       " 'unutterable',\n",
       " 'drive',\n",
       " 'willingness',\n",
       " 'attempts',\n",
       " 'helplessly',\n",
       " 'serge',\n",
       " 'eaters',\n",
       " 'tear',\n",
       " 'sooty',\n",
       " 'friar',\n",
       " 'insertions',\n",
       " 'prosper',\n",
       " 'pennies',\n",
       " 'tilt',\n",
       " 'christians',\n",
       " 'cultured',\n",
       " 'accursed',\n",
       " 'entrusted',\n",
       " 'coat',\n",
       " 'traced',\n",
       " 'piers',\n",
       " 'healthier',\n",
       " 'garbage',\n",
       " 'tougher',\n",
       " 'jogs',\n",
       " 'glows',\n",
       " 'starved',\n",
       " 'vitiated',\n",
       " 'hails',\n",
       " 'scan',\n",
       " 'measured',\n",
       " 'diamond',\n",
       " 'lot',\n",
       " 'enough',\n",
       " 'predominating',\n",
       " 'unaware',\n",
       " 'embalming',\n",
       " 'abounded',\n",
       " 'jawed',\n",
       " 'ptolemy',\n",
       " 'usefully',\n",
       " 'theatre',\n",
       " 'transports',\n",
       " 'snuffed',\n",
       " 'weeps',\n",
       " 'friendship',\n",
       " 'cloths',\n",
       " 'snowy',\n",
       " 'absorb',\n",
       " 'partnership',\n",
       " 'assurances',\n",
       " 'infanticide',\n",
       " 'wondrously',\n",
       " 'arrogance',\n",
       " 'allegiance',\n",
       " 'feebly',\n",
       " 'temperament',\n",
       " 'operas',\n",
       " 'ample',\n",
       " 'darkening',\n",
       " 'fascination',\n",
       " 'churches',\n",
       " 'whispers',\n",
       " 'highlander',\n",
       " 'protestant',\n",
       " 'ludicrous',\n",
       " 'bravery',\n",
       " 'commented',\n",
       " 'ham',\n",
       " '79',\n",
       " 'hoops',\n",
       " 'turtle',\n",
       " 'pretences',\n",
       " 'bloodiest',\n",
       " 'turnips',\n",
       " 'priest',\n",
       " 'precipitous',\n",
       " 'murmured',\n",
       " 'endless',\n",
       " 'imagining',\n",
       " 'icebergs',\n",
       " 'grounds',\n",
       " 'cruise',\n",
       " 'madame',\n",
       " 'witty',\n",
       " 'implicit',\n",
       " 'squeeze',\n",
       " 'itself',\n",
       " 'splintered',\n",
       " 'waterloo',\n",
       " 'overjoyed',\n",
       " 'undertook',\n",
       " 'vibration',\n",
       " 'distinguishable',\n",
       " 'retirement',\n",
       " 'diverting',\n",
       " 'actions',\n",
       " 'tied',\n",
       " 'academy',\n",
       " 'respectfully',\n",
       " 'asses',\n",
       " 'laugh',\n",
       " 'peas',\n",
       " 'stabs',\n",
       " 'daughters',\n",
       " 'identified',\n",
       " 'unrelenting',\n",
       " 'inverted',\n",
       " 'inn',\n",
       " 'improvement',\n",
       " 'sucklings',\n",
       " 'conceals',\n",
       " 'tiptoe',\n",
       " 'displaced',\n",
       " 'allowing',\n",
       " 'baton',\n",
       " 'superior',\n",
       " 'softly',\n",
       " 'introspective',\n",
       " 'breakers',\n",
       " 'affectionately',\n",
       " 'hamlet',\n",
       " 'blaming',\n",
       " 'bondage',\n",
       " 'card',\n",
       " 'calculations',\n",
       " 'fix',\n",
       " 'manservant',\n",
       " 'muscles',\n",
       " 'armada',\n",
       " 'sacrificed',\n",
       " 'choke',\n",
       " 'invoking',\n",
       " 'freed',\n",
       " 'cricket',\n",
       " 'catalogue',\n",
       " 'oatmeal',\n",
       " 'excursion',\n",
       " 'cans',\n",
       " 'displays',\n",
       " 'bulb',\n",
       " 'ventilated',\n",
       " 'follies',\n",
       " 'filth',\n",
       " 'stunned',\n",
       " 'brasses',\n",
       " 'japanese',\n",
       " 'calling',\n",
       " 'rail',\n",
       " 'possession',\n",
       " 'wrist',\n",
       " 'sustained',\n",
       " 'rammed',\n",
       " 'estate',\n",
       " 'blurted',\n",
       " 'pavements',\n",
       " 'finds',\n",
       " '250',\n",
       " 'steeple',\n",
       " 'enlarged',\n",
       " 'blew',\n",
       " 'throng',\n",
       " 'nasty',\n",
       " 'stiffness',\n",
       " 'landslip',\n",
       " 'wailing',\n",
       " 'past',\n",
       " 'navel',\n",
       " 'bedside',\n",
       " 'slunk',\n",
       " 'lapland',\n",
       " 'carriage',\n",
       " 'victoria',\n",
       " 'adoration',\n",
       " 'narration',\n",
       " 'contraction',\n",
       " 'prelude',\n",
       " 'breaths',\n",
       " 'energetically',\n",
       " 'hail',\n",
       " 'darker',\n",
       " 'bawl',\n",
       " 'reasonably',\n",
       " 'contracting',\n",
       " 'miraculously',\n",
       " '48',\n",
       " 'entertaining',\n",
       " 'consistently',\n",
       " 'fond',\n",
       " 'groaned',\n",
       " 'characteristics',\n",
       " 'smelt',\n",
       " 'buzz',\n",
       " 'gums',\n",
       " 'unmatched',\n",
       " 'get',\n",
       " 'watchful',\n",
       " 'cities',\n",
       " 'suit',\n",
       " 'conference',\n",
       " 'wax',\n",
       " 'preparing',\n",
       " 'overdone',\n",
       " 'wretched',\n",
       " 'striving',\n",
       " 'della',\n",
       " 'drudged',\n",
       " 'stolid',\n",
       " 'pierce',\n",
       " 'sorrowing',\n",
       " 'kink',\n",
       " 'slit',\n",
       " 'audible',\n",
       " 'entertainments',\n",
       " 'gradations',\n",
       " 'excessively',\n",
       " 'indolence',\n",
       " 'ballrooms',\n",
       " 'tolerably',\n",
       " 'midsummer',\n",
       " 'spanish',\n",
       " 'fiendish',\n",
       " 'distraction',\n",
       " 'defect',\n",
       " 'leaps',\n",
       " '21',\n",
       " 'flexible',\n",
       " 'token',\n",
       " 'stammered',\n",
       " 'positively',\n",
       " 'create',\n",
       " 'cobweb',\n",
       " 'thinks',\n",
       " 'started',\n",
       " 'punishments',\n",
       " 'parallel',\n",
       " 'needs',\n",
       " 'alive',\n",
       " 'drudgery',\n",
       " 'protecting',\n",
       " 'generous',\n",
       " 'cant',\n",
       " 'stung',\n",
       " 'fallow',\n",
       " 'iv',\n",
       " 'thunderbolts',\n",
       " 'plainly',\n",
       " 'sounding',\n",
       " 'assist',\n",
       " 'quiver',\n",
       " 'slightly',\n",
       " 'apprehension',\n",
       " 'cheated',\n",
       " 'flippancy',\n",
       " 'essentially',\n",
       " 'suggest',\n",
       " 'startling',\n",
       " 'positive',\n",
       " 'lipped',\n",
       " 'escapes',\n",
       " 'dazzling',\n",
       " 'immensity',\n",
       " 'dining',\n",
       " 'plums',\n",
       " 'creed',\n",
       " 'conventionality',\n",
       " 'lavish',\n",
       " 'retraced',\n",
       " 'resembled',\n",
       " 'forgiveness',\n",
       " 'avis',\n",
       " 'grounded',\n",
       " 'seen',\n",
       " 'recoiled',\n",
       " 'sometime',\n",
       " 'pollen',\n",
       " 'scalding',\n",
       " 'foresaw',\n",
       " 'disorder',\n",
       " 'worst',\n",
       " 'sheepish',\n",
       " 'proportionate',\n",
       " 'immaterial',\n",
       " 'squander',\n",
       " 'occasions',\n",
       " 'pulpy',\n",
       " 'researches',\n",
       " 'chestnut',\n",
       " 'peer',\n",
       " 'muddled',\n",
       " 'prospect',\n",
       " 'sails',\n",
       " 'beat',\n",
       " 'stab',\n",
       " 'settees',\n",
       " 'expectancy',\n",
       " 'thump',\n",
       " 'dizzily',\n",
       " 'lose',\n",
       " 'abode',\n",
       " 'advertising',\n",
       " 'paces',\n",
       " 'st',\n",
       " 'solicited',\n",
       " 'workmen',\n",
       " 'exert',\n",
       " 'discharged',\n",
       " 'relapsed',\n",
       " 'observe',\n",
       " 'implored',\n",
       " 'ter',\n",
       " 'deformed',\n",
       " 'keep',\n",
       " 'dominance',\n",
       " 'journeys',\n",
       " 'buffalo',\n",
       " 'humbly',\n",
       " 'harp',\n",
       " 'wasted',\n",
       " 'grammar',\n",
       " 'err',\n",
       " 'assurance',\n",
       " 'oiled',\n",
       " 'frayed',\n",
       " 'fowls',\n",
       " 'imperatively',\n",
       " 'threatened',\n",
       " 'notepaper',\n",
       " 'unsuccessful',\n",
       " 'practices',\n",
       " 'disagree',\n",
       " 'solomon',\n",
       " 'design',\n",
       " 'graved',\n",
       " 'handing',\n",
       " 'kee',\n",
       " 'sanctity',\n",
       " 'incumbent',\n",
       " 'precipitate',\n",
       " 'approval',\n",
       " 'promoting',\n",
       " 'obliquity',\n",
       " 'comfort',\n",
       " 'lowers',\n",
       " 'escaped',\n",
       " 'withhold',\n",
       " 'stretching',\n",
       " 'lacking',\n",
       " 'policeman',\n",
       " 'grouped',\n",
       " 'opposite',\n",
       " 'arena',\n",
       " 'stubbs',\n",
       " 'honest',\n",
       " 'vestige',\n",
       " 'travellers',\n",
       " 'groan',\n",
       " 'hypothesis',\n",
       " 'persist',\n",
       " 'levers',\n",
       " 'happened',\n",
       " 'pearson',\n",
       " 'snort',\n",
       " 'duly',\n",
       " 'bernard',\n",
       " 'tightly',\n",
       " 'mature',\n",
       " 'balloon',\n",
       " 'obscurity',\n",
       " 'undaunted',\n",
       " 'soiled',\n",
       " 'justify',\n",
       " 'buttered',\n",
       " 'gilbert',\n",
       " 'reversed',\n",
       " 'restrain',\n",
       " 'intellect',\n",
       " 'limitations',\n",
       " 'difference',\n",
       " 'squares',\n",
       " 'tortoise',\n",
       " 'merits',\n",
       " 'jump',\n",
       " 'belvedere',\n",
       " 'brightness',\n",
       " 'coupled',\n",
       " 'objection',\n",
       " 'spruce',\n",
       " 'circuit',\n",
       " 'sunk',\n",
       " 'paused',\n",
       " 'cramped',\n",
       " 'medical',\n",
       " 'gallons',\n",
       " 'hoisted',\n",
       " 'moonlit',\n",
       " 'penned',\n",
       " 'spear',\n",
       " 'obedience',\n",
       " 'uncontrollable',\n",
       " 'blithe',\n",
       " 'feats',\n",
       " 'bony',\n",
       " 'stroll',\n",
       " 'complained',\n",
       " 'ornamented',\n",
       " 'albatrosses',\n",
       " 'baptismal',\n",
       " 'careering',\n",
       " 'hiss',\n",
       " 'certain',\n",
       " 'powers',\n",
       " 'swamped',\n",
       " 'aback',\n",
       " 'margaret',\n",
       " 'characters',\n",
       " 'ragged',\n",
       " 'visitors',\n",
       " 'propriety',\n",
       " 'index',\n",
       " 'mare',\n",
       " 'anew',\n",
       " 'laurel',\n",
       " 'frenzy',\n",
       " 'symbols',\n",
       " 'babyish',\n",
       " 'cheaply',\n",
       " 'meals',\n",
       " 'specially',\n",
       " 'ourselves',\n",
       " 'sounds',\n",
       " 'secret',\n",
       " 'cursing',\n",
       " 'noon',\n",
       " 'archbishop',\n",
       " 'miseries',\n",
       " 'mistakes',\n",
       " 'vaughan',\n",
       " 'flaming',\n",
       " 'meanings',\n",
       " 'shock',\n",
       " 'deepest',\n",
       " 'afterwards',\n",
       " 'bounced',\n",
       " 'caramba',\n",
       " 'conceal',\n",
       " 'delusions',\n",
       " 'worth',\n",
       " 'section',\n",
       " 'fullness',\n",
       " 'privileged',\n",
       " 'barrow',\n",
       " 'compile',\n",
       " 'manage',\n",
       " 'animosity',\n",
       " 'recognise',\n",
       " 'uninteresting',\n",
       " 'systems',\n",
       " 'riches',\n",
       " 'endeavours',\n",
       " 'diddled',\n",
       " 'investigations',\n",
       " 'southerly',\n",
       " 'flats',\n",
       " 'realizing',\n",
       " 'situated',\n",
       " 'proximity',\n",
       " 'stays',\n",
       " 'slogan',\n",
       " 'staring',\n",
       " 'ineffectually',\n",
       " 'burn',\n",
       " 'fickle',\n",
       " 'oath',\n",
       " 'homecoming',\n",
       " 'weekly',\n",
       " 'record',\n",
       " 'likewise',\n",
       " 'winks',\n",
       " 'xxxiv',\n",
       " 'conception',\n",
       " 'haunts',\n",
       " 'athenian',\n",
       " 'nourishment',\n",
       " 'beard',\n",
       " 'audience',\n",
       " 'genesis',\n",
       " 'timely',\n",
       " 'observing',\n",
       " 'entreaty',\n",
       " 'eclipsed',\n",
       " 'reappeared',\n",
       " 'salted',\n",
       " 'shaky',\n",
       " 'virgin',\n",
       " 'majesty',\n",
       " 'alterations',\n",
       " 'masculine',\n",
       " 'strained',\n",
       " 'puddings',\n",
       " 'oxford',\n",
       " 'algebra',\n",
       " 'flannelette',\n",
       " 'shall',\n",
       " 'reckoning',\n",
       " 'newspapers',\n",
       " 'proclaimed',\n",
       " 'lament',\n",
       " 'curdling',\n",
       " 'frustrate',\n",
       " 'professors',\n",
       " 'lectures',\n",
       " 'phrase',\n",
       " 'exacted',\n",
       " 'basso',\n",
       " 'strait',\n",
       " 'climbing',\n",
       " 'avail',\n",
       " 'weather',\n",
       " 'long',\n",
       " 'abroad',\n",
       " 'impassive',\n",
       " 'painted',\n",
       " 'haters',\n",
       " 'philip',\n",
       " 'broken',\n",
       " 'ignoring',\n",
       " 'swore',\n",
       " 'worry',\n",
       " 'extension',\n",
       " 'longest',\n",
       " 'bareheaded',\n",
       " 'bog',\n",
       " 'meet',\n",
       " 'yonder',\n",
       " 'accompany',\n",
       " 'lovable',\n",
       " 'drawn',\n",
       " 'regular',\n",
       " 'demon',\n",
       " 'die',\n",
       " 'wouldst',\n",
       " 'unrest',\n",
       " 'fancied',\n",
       " 'dangled',\n",
       " 'listens',\n",
       " 'list',\n",
       " 'smoked',\n",
       " 'doubtfully',\n",
       " 'masses',\n",
       " 'learned',\n",
       " 'incomprehensible',\n",
       " 'grass',\n",
       " 'loth',\n",
       " 'tract',\n",
       " 'greetings',\n",
       " 'misgiving',\n",
       " 'literature',\n",
       " 'stain',\n",
       " 'trent',\n",
       " 'determination',\n",
       " 'sufficiency',\n",
       " 'bangle',\n",
       " 'hurried',\n",
       " 'spur',\n",
       " 'metropolis',\n",
       " 'king',\n",
       " 'inconsistent',\n",
       " 'clown',\n",
       " 'hopelessness',\n",
       " 'ticked',\n",
       " 'eldest',\n",
       " 'interested',\n",
       " 'suburban',\n",
       " 'lisp',\n",
       " 'youths',\n",
       " 'raptures',\n",
       " 'partitions',\n",
       " 'poverty',\n",
       " 'effigy',\n",
       " 'dawn',\n",
       " 'existence',\n",
       " 'clatter',\n",
       " 'lt',\n",
       " 'tiresome',\n",
       " 'credited',\n",
       " 'howled',\n",
       " 'besides',\n",
       " 'borrow',\n",
       " 'gnawing',\n",
       " 'treason',\n",
       " 'speaking',\n",
       " 'film',\n",
       " 'hysterical',\n",
       " 'razor',\n",
       " 'rabble',\n",
       " 'thirds',\n",
       " 'flour',\n",
       " 'smiled',\n",
       " 'twas',\n",
       " 'beastly',\n",
       " 'feeding',\n",
       " 'female',\n",
       " 'amiable',\n",
       " 'renewed',\n",
       " 'established',\n",
       " 'unmarried',\n",
       " 'railing',\n",
       " 'fluttered',\n",
       " 'stole',\n",
       " 'confinement',\n",
       " 'pouch',\n",
       " 'slay',\n",
       " 'india',\n",
       " 'relentless',\n",
       " 'sweep',\n",
       " 'upbraid',\n",
       " 'disdain',\n",
       " 'broadcloth',\n",
       " 'poet',\n",
       " 'antarctic',\n",
       " 'bottomless',\n",
       " 'accidentally',\n",
       " 'snores',\n",
       " 'imps',\n",
       " 'quarts',\n",
       " 'divert',\n",
       " 'sceptical',\n",
       " 'strength',\n",
       " 'neighbor',\n",
       " 'ends',\n",
       " 'initiated',\n",
       " 'reprimand',\n",
       " 'whaler',\n",
       " 'soothed',\n",
       " 'blimey',\n",
       " 'friends',\n",
       " 'passionate',\n",
       " 'whereupon',\n",
       " 'terrors',\n",
       " 'redoubled',\n",
       " 'kindle',\n",
       " 'finance',\n",
       " 'pico',\n",
       " 'hand',\n",
       " 'excellency',\n",
       " 'drugged',\n",
       " 'inspired',\n",
       " 'warehouses',\n",
       " 'apoplectic',\n",
       " 'expanse',\n",
       " 'furled',\n",
       " 'stronger',\n",
       " 'stretched',\n",
       " 'bursts',\n",
       " 'celebration',\n",
       " 'heathen',\n",
       " 'circumpolar',\n",
       " 'encased',\n",
       " 'twins',\n",
       " 'graham',\n",
       " 'surveys',\n",
       " 'embassy',\n",
       " 'fundamentals',\n",
       " 'author',\n",
       " 'scope',\n",
       " 'eulogy',\n",
       " 'thanking',\n",
       " 'graves',\n",
       " 'steer',\n",
       " 'inhabit',\n",
       " 'solvency',\n",
       " 'talked',\n",
       " 'withdrew',\n",
       " 'risked',\n",
       " 'slanted',\n",
       " 'dane',\n",
       " 'cove',\n",
       " 'obtain',\n",
       " 'belt',\n",
       " 'tasting',\n",
       " 'forfeited',\n",
       " 'ugly',\n",
       " 'term',\n",
       " 'routine',\n",
       " 'curving',\n",
       " 'immaculate',\n",
       " 'instead',\n",
       " 'trophies',\n",
       " 'sunday',\n",
       " 'ridicule',\n",
       " 'skirted',\n",
       " 'launch',\n",
       " 'greasy',\n",
       " 'homely',\n",
       " 'peacock',\n",
       " 'firearms',\n",
       " 'swelling',\n",
       " 'promise',\n",
       " 'cheerfully',\n",
       " 'interest',\n",
       " 'numbers',\n",
       " 'sou',\n",
       " 'whitened',\n",
       " 'distrustful',\n",
       " 'beaker',\n",
       " 'stiffening',\n",
       " 'malt',\n",
       " 'insanity',\n",
       " 'rooms',\n",
       " 'circle',\n",
       " 'rags',\n",
       " 'originals',\n",
       " 'blemish',\n",
       " 'breakfasts',\n",
       " 'butler',\n",
       " 'sugary',\n",
       " 'sheathed',\n",
       " 'scar',\n",
       " 'sew',\n",
       " 'venom',\n",
       " 'chiselled',\n",
       " 'indispensable',\n",
       " 'winning',\n",
       " 'splinter',\n",
       " 'open',\n",
       " 'calamity',\n",
       " 'mendelssohn',\n",
       " 'angelo',\n",
       " 'presses',\n",
       " 'indications',\n",
       " 'infallibly',\n",
       " 'congregational',\n",
       " 'chrysanthemums',\n",
       " 'unexpectedness',\n",
       " 'conceive',\n",
       " 'involves',\n",
       " 'bounds',\n",
       " 'passenger',\n",
       " 'builds',\n",
       " 'duke',\n",
       " 'exceeded',\n",
       " 'yells',\n",
       " 'survived',\n",
       " 'market',\n",
       " 'prize',\n",
       " 'slinking',\n",
       " 'begets',\n",
       " 'british',\n",
       " 'pikes',\n",
       " 'pipes',\n",
       " 'pieties',\n",
       " 'blank',\n",
       " 'least',\n",
       " 'tom',\n",
       " 'burglars',\n",
       " 'sternness',\n",
       " 'crops',\n",
       " 'villainy',\n",
       " 'herring',\n",
       " 'cobbler',\n",
       " 'shallowest',\n",
       " 'lifting',\n",
       " 'reaped',\n",
       " 'respite',\n",
       " 'ganders',\n",
       " 'crow',\n",
       " 'robin',\n",
       " 'rude',\n",
       " 'purely',\n",
       " 'actress',\n",
       " 'surrey',\n",
       " 'fooling',\n",
       " 'dilating',\n",
       " 'lagoons',\n",
       " 'rod',\n",
       " 'chaplain',\n",
       " 'contact',\n",
       " 'blotch',\n",
       " 'unanswerable',\n",
       " 'deplorable',\n",
       " 'arrested',\n",
       " 'azure',\n",
       " 'tottenham',\n",
       " 'confirmation',\n",
       " 'phil',\n",
       " 'gangs',\n",
       " 'mermaids',\n",
       " 'paled',\n",
       " 'quietude',\n",
       " 'moody',\n",
       " 'imperious',\n",
       " 'replacing',\n",
       " 'seized',\n",
       " 'lasted',\n",
       " 'restricted',\n",
       " 'nobody',\n",
       " 'braiding',\n",
       " 'illustrations',\n",
       " 'suspended',\n",
       " 'distinct',\n",
       " 'gilt',\n",
       " 'happen',\n",
       " 'australia',\n",
       " 'lotion',\n",
       " 'absence',\n",
       " 'contradicting',\n",
       " 'note',\n",
       " 'phrased',\n",
       " 'dashing',\n",
       " 'magnifying',\n",
       " 'pursed',\n",
       " 'infinitesimal',\n",
       " 'service',\n",
       " 'gout',\n",
       " 'deciphered',\n",
       " 'furnishing',\n",
       " 'hollow',\n",
       " 'youngest',\n",
       " 'police',\n",
       " 'multitudinous',\n",
       " 'brains',\n",
       " 'flows',\n",
       " 'vernacular',\n",
       " 'virtue',\n",
       " 'nurtured',\n",
       " 'cheeks',\n",
       " 'delivered',\n",
       " 'elderly',\n",
       " 'magical',\n",
       " 'salutes',\n",
       " 'despising',\n",
       " 'moods',\n",
       " 'correctness',\n",
       " 'habit',\n",
       " 'outwardly',\n",
       " 'darwin',\n",
       " 'someone',\n",
       " 'derelict',\n",
       " 'embodied',\n",
       " 'wonderful',\n",
       " 'pussy',\n",
       " '1846',\n",
       " '4d',\n",
       " 'sheep',\n",
       " 'extent',\n",
       " 'wapping',\n",
       " 'bundling',\n",
       " 'smeared',\n",
       " 'toilet',\n",
       " 'inconsiderate',\n",
       " 'bountifully',\n",
       " 'incandescence',\n",
       " 'smoking',\n",
       " 'trust',\n",
       " 'father',\n",
       " 'backwards',\n",
       " 'thee',\n",
       " 'tornado',\n",
       " 'avenger',\n",
       " 'plumped',\n",
       " 'grouse',\n",
       " 'secrets',\n",
       " 'majority',\n",
       " 'staves',\n",
       " 'crutch',\n",
       " 'wakes',\n",
       " 'saddened',\n",
       " 'kine',\n",
       " 'nods',\n",
       " 'indifferently',\n",
       " 'butteries',\n",
       " 'charades',\n",
       " 'feelings',\n",
       " 'locking',\n",
       " 'librarian',\n",
       " 'greying',\n",
       " 'house',\n",
       " 'grudgingly',\n",
       " 'much',\n",
       " 'expound',\n",
       " 'marshalled',\n",
       " 'stillness',\n",
       " 'mirth',\n",
       " 'hours',\n",
       " 'everlasting',\n",
       " 'surf',\n",
       " 'appellation',\n",
       " 'trampled',\n",
       " 'porch',\n",
       " 'looping',\n",
       " 'justification',\n",
       " 'honestly',\n",
       " 'lamentable',\n",
       " 'musical',\n",
       " 'prodding',\n",
       " 'captain',\n",
       " 'procrastination',\n",
       " 'sneaking',\n",
       " 'smiles',\n",
       " 'tranquil',\n",
       " 'preservation',\n",
       " 'navigator',\n",
       " 'technically',\n",
       " 'daisy',\n",
       " 'boredom',\n",
       " 'twisting',\n",
       " 'speed',\n",
       " 'creamy',\n",
       " 'documents',\n",
       " 'tum',\n",
       " '82',\n",
       " 'unwieldy',\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(input=corpus,\n",
    "                     max_df=0.20)\n",
    "count_vector = cv.fit_transform(corpus)\n",
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "In the subdirectory 'books' you will find some books:\n",
    "\n",
    "- <a href=\"books/night_and_day_virginia_wolf.txt\" rel=\"nofollow\">Virginia Woolf: Night and Day</a>\n",
    "- <a href=\"books/the_way_of_all_flash_butler.txt\" rel=\"nofollow\">Samuel Butler: The Way of all Flesh</a>\n",
    "- <a href=\"books/moby_dick_melville.txt\" rel=\"nofollow\">Herman Melville: Moby Dick</a>\n",
    "- <a href=\"books/sons_and_lovers_lawrence.txt\" rel=\"nofollow\">David Herbert Lawrence: Sons and Lovers</a>\n",
    "- <a href=\"books/robinson_crusoe_defoe.txt\" rel=\"nofollow\">Daniel Defoe: The Life and Adventures of Robinson Crusoe</a>\n",
    "- <a href=\"books/james_joyce_ulysses.txt\" rel=\"nofollow\">James Joyce: Ulysses</a>\n",
    "\n",
    "Use these novels as the corpus and create a word count vector.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Turn the previously calculated 'word count vector' into a dense ndarray representation.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Let us have another example with a different corpus. The five strings are famous quotes from \n",
    "    \n",
    "1. William Shakespeare\n",
    "2. W.C. Fields\n",
    "3. Ronald Reagan\n",
    "4. John Steinbeck\n",
    "5. Author unknown\n",
    "\n",
    "Compute the IDF values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = [\"A horse, a horse, my kingdom for a horse!\",\n",
    "          \"Horse sense is the thing a horse has which keeps it from betting on people.\"\n",
    "          \"I’ve often said there is nothing better for the inside of the man, than the outside of the horse.\",\n",
    "          \"A man on a horse is spiritually, as well as physically, bigger then a man on foot.\",\n",
    "          \"No heaven can heaven be, if my horse isn’t there to welcome me.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "#### Solution  to Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of',\n",
       " 'The Project Gutenberg eBook, T',\n",
       " '\\nThe Project Gutenberg EBook o',\n",
       " 'The Project Gutenberg EBook of',\n",
       " 'The Project Gutenberg eBook, T',\n",
       " '\\nThe Project Gutenberg EBook o']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "books = [\"night_and_day_virginia_woolf.txt\",\n",
    "         \"the_way_of_all_flash_butler.txt\",\n",
    "         \"moby_dick_melville.txt\",\n",
    "         \"sons_and_lovers_lawrence.txt\",\n",
    "         \"robinson_crusoe_defoe.txt\",\n",
    "         \"james_joyce_ulysses.txt\"]\n",
    "path = \"books\"\n",
    "\n",
    "corpus = []\n",
    "for book in books:\n",
    "    txt = open(path + \"/\" + book).read()\n",
    "    corpus.append(txt)\n",
    "    \n",
    "[book[:30] for book in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to get rid of the Gutenberg header and footer, because it doesn't belong to the novels. We can see by looking at the texts that the authors works begins after lines of the following kind\n",
    "\n",
    "```***START OF THIS PROJECT GUTENBERG ... ***```\n",
    "\n",
    "The footer of the texts start with this line:\n",
    "\n",
    "```***END OF THIS PROJECT GUTENBERG EBOOK ...***```\n",
    "\n",
    "There may or may not be a space after the first three stars or instead of \"the\" there may be \"this\".\n",
    "\n",
    "We can use regular expressions to find the starting point of the novels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "\n",
    "corpus = []\n",
    "books = [\"night_and_day_virginia_woolf.txt\",\n",
    "         \"the_way_of_all_flash_butler.txt\",\n",
    "         \"moby_dick_melville.txt\",\n",
    "         \"sons_and_lovers_lawrence.txt\",\n",
    "         \"robinson_crusoe_defoe.txt\",\n",
    "         \"james_joyce_ulysses.txt\"]\n",
    "path = \"books\"\n",
    "\n",
    "corpus = []\n",
    "for book in books:\n",
    "    txt = open(path + \"/\" + book).read()\n",
    "    text_begin = re.search(r\"\\*\\*\\* ?START OF (THE|THIS) PROJECT.*?\\*\\*\\*\", txt, re.DOTALL)\n",
    "    text_end = re.search(r\"\\*\\*\\* ?END OF (THE|THIS) PROJECT.*?\\*\\*\\*\", txt, re.DOTALL)\n",
    "    corpus.append(txt[text_begin.end():text_end.start()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t2\n",
      "  (0, 35)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 79)\t1\n",
      "  (0, 131)\t1\n",
      "  (0, 221)\t1\n",
      "  (0, 724)\t6\n",
      "  (0, 731)\t5\n",
      "  (0, 734)\t1\n",
      "  (0, 743)\t5\n",
      "  (0, 761)\t1\n",
      "  (0, 773)\t1\n",
      "  (0, 779)\t1\n",
      "  (0, 780)\t1\n",
      "  (0, 781)\t23\n",
      "  (0, 790)\t1\n",
      "  (0, 804)\t1\n",
      "  (0, 809)\t412\n",
      "  (0, 810)\t36\n",
      "  (0, 817)\t2\n",
      "  (0, 823)\t4\n",
      "  (0, 824)\t19\n",
      "  (0, 825)\t3\n",
      "  (0, 828)\t11\n",
      "  (0, 829)\t1\n",
      "  :\t:\n",
      "  (5, 42156)\t5\n",
      "  (5, 42157)\t1\n",
      "  (5, 42158)\t1\n",
      "  (5, 42159)\t2\n",
      "  (5, 42160)\t2\n",
      "  (5, 42161)\t106\n",
      "  (5, 42165)\t1\n",
      "  (5, 42166)\t2\n",
      "  (5, 42167)\t1\n",
      "  (5, 42172)\t2\n",
      "  (5, 42173)\t4\n",
      "  (5, 42174)\t1\n",
      "  (5, 42175)\t1\n",
      "  (5, 42176)\t1\n",
      "  (5, 42177)\t1\n",
      "  (5, 42178)\t3\n",
      "  (5, 42181)\t1\n",
      "  (5, 42182)\t1\n",
      "  (5, 42183)\t3\n",
      "  (5, 42184)\t1\n",
      "  (5, 42185)\t2\n",
      "  (5, 42186)\t1\n",
      "  (5, 42187)\t1\n",
      "  (5, 42188)\t2\n",
      "  (5, 42189)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "token_count_matrix = vectorizer.transform(corpus)\n",
    "print(token_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary:  42192\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in vocabulary: \", len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to Exercise 2\n",
    "\n",
    "All you have to do is applying the method ```toarray``` to get the ```token_count_matrix```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [19,  0,  0, ...,  0,  0,  0],\n",
       "       [20,  0,  0, ...,  0,  1,  1],\n",
       "       [ 0,  0,  1, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [11,  1,  0, ...,  1,  0,  0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Solution to Exercise 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           : idf_weight\n",
      "horse          : 1.000\n",
      "for            : 1.511\n",
      "is             : 1.511\n",
      "man            : 1.511\n",
      "my             : 1.511\n",
      "on             : 1.511\n",
      "there          : 1.511\n",
      "as             : 1.916\n",
      "be             : 1.916\n",
      "better         : 1.916\n",
      "betting        : 1.916\n",
      "bigger         : 1.916\n",
      "can            : 1.916\n",
      "foot           : 1.916\n",
      "from           : 1.916\n",
      "has            : 1.916\n",
      "heaven         : 1.916\n",
      "if             : 1.916\n",
      "inside         : 1.916\n",
      "isn            : 1.916\n",
      "it             : 1.916\n",
      "keeps          : 1.916\n",
      "kingdom        : 1.916\n",
      "me             : 1.916\n",
      "no             : 1.916\n",
      "nothing        : 1.916\n",
      "of             : 1.916\n",
      "often          : 1.916\n",
      "outside        : 1.916\n",
      "people         : 1.916\n",
      "physically     : 1.916\n",
      "said           : 1.916\n",
      "sense          : 1.916\n",
      "spiritually    : 1.916\n",
      "than           : 1.916\n",
      "the            : 1.916\n",
      "then           : 1.916\n",
      "thing          : 1.916\n",
      "to             : 1.916\n",
      "ve             : 1.916\n",
      "welcome        : 1.916\n",
      "well           : 1.916\n",
      "which          : 1.916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "\n",
    "# our corpus:\n",
    "quotes = [\"A horse, a horse, my kingdom for a horse!\",\n",
    "          \"Horse sense is the thing a horse has which keeps it from betting on people.\"\n",
    "          \"I’ve often said there is nothing better for the inside of the man, than the outside of the horse.\",\n",
    "          \"A man on a horse is spiritually, as well as physically, bigger then a man on foot.\",\n",
    "          \"No heaven can heaven be, if my horse isn’t there to welcome me.\"]\n",
    "\n",
    "vectorizer = text.CountVectorizer()\n",
    "vectorizer.fit(quotes)\n",
    "vectorized_text = vectorizer.fit_transform(quotes)\n",
    "\n",
    "tfidf_transformer = text.TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(vectorized_text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "alternative way to output the data:\n",
    "import pandas as pd\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, \n",
    "                      index=vectorizer.get_feature_names(),\n",
    "                      columns=[\"idf_weight\"]) \n",
    "df_idf.sort_values(by=['idf_weights'])  # sorting data\n",
    "print(df_idf)\n",
    "\n",
    "\"\"\"\n",
    "print(f\"{'word':15s}: idf_weight\")\n",
    "word_weight_list = list(zip(vectorizer.get_feature_names(), tfidf_transformer.idf_))\n",
    "word_weight_list.sort(key=lambda x:x[1])  # sort list by the weights (2nd component)\n",
    "for word, idf_weight in word_weight_list:\n",
    "    print(f\"{word:15s}: {idf_weight:4.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "<a name=\"footnote1\"></a><sub>1</sub>Logically ```toarray``` and ```todense``` are the same thing, but ```toarray``` returns an ```ndarray``` whereas ```todense``` returns a ```matrix```. If you consider, what the official Numpy documentation has to say about the ```numpy.matrix``` class, you shouldn't use ```todense```! \"*It is no longer recommended to use this class, even for linear algebra. Instead use regular arrays. The class may be removed in the future.*\"\n",
    "([numpy.matrix](https://numpy.org/doc/stable/reference/generated/numpy.matrix.html))\n",
    "(<a href=\"#footnote1_ret\">back</a>)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
