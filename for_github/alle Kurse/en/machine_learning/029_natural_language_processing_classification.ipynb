{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing: Classification\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<img width=40% class=\"imgright\" src=\"../images/natural_language_processing.webp\" srcset=\"../images/natural_language_processing_800w.webp 800w,../images/natural_language_processing_700w.webp 700w,../images/natural_language_processing_600w.webp 600w,../images/natural_language_processing_500w.webp 500w,../images/natural_language_processing_400w.webp 400w,../images/natural_language_processing_350w.webp 350w,../images/natural_language_processing_300w.webp 300w\" alt=\"natural language processing\" />\n",
    "\n",
    "\n",
    "One might think that it might not be that difficult to get good text material for examples of text classification. After all, hardly a minute goes by in our daily lives that we are not dealing with written language. Newspapers, books, and most of all, most of the internet is probably still text-based. For our example classifiers, however, the texts must be in machine-readable form and preferably in simple text files, i.e. not formatted in Word or other formats.\n",
    "In addition, the texts may not be protected by copyright.\n",
    "\n",
    "We use our example novels from the Gutenberg project.\n",
    "\n",
    "The first task consists in training a classifier which can predict the author of a paragraph from a novel. \n",
    "\n",
    "The second example will use novels of various languages, i.e. German, Swedish, Danish, Dutch, French, Italian and Spanish.\n",
    "\n",
    "### Author Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We want to demonstrate the concepts of the previous chapter of our Machine Learning tutorial in an extended example. We will use the following novels:\n",
    "\n",
    "- <a href=\"books/night_and_day_virginia_woolf.txt\" rel=\"nofollow\">Virginia Woolf: Night and Day</a>\n",
    "- <a href=\"books/the_way_of_all_flash_butler.txt\" rel=\"nofollow\">Samuel Butler: The Way of all Flesh</a>\n",
    "- <a href=\"books/moby_dick_melville.txt\" rel=\"nofollow\">Herman Melville: Moby Dick</a>\n",
    "- <a href=\"books/sons_and_lovers_lawrence.txt\" rel=\"nofollow\">David Herbert Lawrence: Sons and Lovers</a>\n",
    "- <a href=\"books/robinson_crusoe_defoe.txt\" rel=\"nofollow\">Daniel Defoe: The Life and Adventures of Robinson Crusoe</a>\n",
    "- <a href=\"books/james_joyce_ulysses.txt\" rel=\"nofollow\">James Joyce: Ulysses</a>\n",
    "\n",
    "Will will train a classifier with these novels. This classifier should be able to predict the author from an arbitrary text passage.\n",
    "\n",
    "<img width=80% src=\"../images/author_prediction.webp\" srcset=\"../images/author_prediction_800w.webp 800w,../images/author_prediction_700w.webp 700w,../images/author_prediction_600w.webp 600w,../images/author_prediction_500w.webp 500w,../images/author_prediction_400w.webp 400w,../images/author_prediction_350w.webp 350w,../images/author_prediction_300w.webp 300w\" alt=\"Various books and authors\" />\n",
    "\n",
    "We will segment the books into lists of paragraphs. We will use a function 'text2paragraphs', which we had introduced as an exercise in our [chapter on file handling](python3_file_management.php).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2paragraphs(filename, min_size=1):\n",
    "    \"\"\" A text contained in the file 'filename' will be read \n",
    "    and chopped into paragraphs.\n",
    "    Paragraphs with a string length less than min_size will be ignored.\n",
    "    A list of paragraph strings will be returned\"\"\"\n",
    "    \n",
    "    txt = open(filename).read()\n",
    "    paragraphs = [para for para in txt.split(\"\\n\\n\") if len(para) > min_size]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Virginia Woolf', 'Samuel Butler', 'Herman Melville', \n",
    "          'David Herbert Lawrence', 'Daniel Defoe', 'James Joyce']\n",
    "\n",
    "files = ['night_and_day_virginia_woolf.txt', 'the_way_of_all_flash_butler.txt',\n",
    "         'moby_dick_melville.txt', 'sons_and_lovers_lawrence.txt',\n",
    "         'robinson_crusoe_defoe.txt', 'james_joyce_ulysses.txt']\n",
    "\n",
    "path = \"books/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "targets = []\n",
    "counter = 0\n",
    "for fname in files:\n",
    "    paras = text2paragraphs(path + fname, min_size=150)\n",
    "    data.extend(paras)\n",
    "    targets += [counter] * len(paras)\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell is useless, because train_test_split will do the shuffling!\n",
    "\n",
    "import random\n",
    "\n",
    "data_targets = list(zip(data, targets))\n",
    "# create random permuation on list:\n",
    "data_targets = random.sample(data_targets, len(data_targets))\n",
    "\n",
    "data, targets = list(zip(*data_targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "res = train_test_split(data, targets, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42)\n",
    "train_data, test_data, train_targets, test_targets = res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(train_data), len(test_data), len(train_targets), len(test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Naive Bayes classifiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9123571039738705\n",
      "F1-score:  0.9097752590254707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# creating a classifier\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(vectors, train_targets)\n",
    "\n",
    "vectors_test = vectorizer.transform(test_data)\n",
    "\n",
    "predictions = classifier.predict(vectors_test)\n",
    "accuracy_score = metrics.accuracy_score(test_targets, \n",
    "                                        predictions)\n",
    "f1_score = metrics.f1_score(test_targets, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test this classifier now with a different book of Virginia Woolf. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 5 5 0 5 5 0 2 5 0 0 5 0 5 0 0 0 1 0 1 0 0 5 1 5 0 0 1 0 0 0 5 2 2 5 0\n",
      " 2 2 5 0 0 0 0 0 3 0 0 0 0 0 4 2 5 2 3 0 0 0 0 0 0 5 0 0 2 0 0 0 0 0 5 5 5\n",
      " 0 0 1 0 0 2 2 3 0 2 2 0 5 5 0 5 1 0 0 1 0 5 0 0 5 0 0 3 5 5 0 5 5 5 5 0 5\n",
      " 0 0 0 0 0 0 1 2 0 0 0 5 0 1 2 2 2 5 5 0 0 0 1 3 0 0 5 1 3 0 0 0 0 3 0 0 0\n",
      " 0 0 5 0 5 0 5 5 1 1 1 0 0 0 0 0 0 5 0 1 0 0 0 5 5 5 5 0 2 3 5 0 0 0 0 0 0\n",
      " 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 5 0 0 0 5 5 5 3 0 5 0 0 3 0 0 0 5 0\n",
      " 0 5 2 0 0 0 0 0 3 0 0 0 0 2 0 0 5 3 5 1 0 5 5 0 5 0 5 0 1 1 1 0 0 0 1 1 3\n",
      " 1 0 0 5 0 0 5 2 3 0 0 0 5 0 2 2 0 1 0 0 0 0 0 0 3 0 4 0 0 0 0 1 0 0 0 0 1\n",
      " 1 0 5 5 5 0 5 0 0 0 0 0 5 3 0 0 0 5 3 1 3 0 0 5 0 0 0 0 0 0 3 0 5 5 0 0 0\n",
      " 3 3 5 0 3 3 0 0 1 5 1 0 0 0 0 2 0 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 2 3 0 0\n",
      " 0 1 0 0 0 5 0 0 0 0 0 0 0 0 3 0 0 0 0 0 1 5 0 0 0 0 0 0 0 0]\n",
      "accuracy score:  0.595\n",
      "precision score:  0.595\n",
      "F1-score:  0.12434691745036573\n"
     ]
    }
   ],
   "source": [
    "paras = text2paragraphs(path + \"the_voyage_out_virginia_woolf.txt\", min_size=250)\n",
    "\n",
    "first_para, last_para = 100, 500\n",
    "vectors_test = vectorizer.transform(paras[first_para: last_para])\n",
    "#vectors_test = vectorizer.transform([\"To be or not to be\"])\n",
    "\n",
    "predictions = classifier.predict(vectors_test)\n",
    "print(predictions)\n",
    "targets = [0] * (last_para - first_para)\n",
    "accuracy_score = metrics.accuracy_score(targets, \n",
    "                                        predictions)\n",
    "precision_score = metrics.precision_score(targets, \n",
    "                                          predictions, \n",
    "                                          average='macro')\n",
    "\n",
    "f1_score = metrics.f1_score(targets, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"precision score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.26578058e-004 2.51943113e-002 4.85163038e-008 4.75065393e-005\n",
      "  4.00835263e-014 9.74131556e-001]\n",
      " [7.12081909e-001 4.92957656e-002 5.37096844e-003 1.68824845e-009\n",
      "  4.99835718e-013 2.33251355e-001]\n",
      " [1.11615265e-001 1.70149726e-009 8.02170949e-013 1.93038351e-008\n",
      "  3.38381992e-017 8.88384714e-001]\n",
      " ...\n",
      " [9.99433053e-001 5.66946558e-004 6.87847449e-032 2.49682983e-019\n",
      "  9.56365457e-038 3.61259105e-033]\n",
      " [9.99999991e-001 7.95355880e-009 9.29384687e-029 2.81898441e-033\n",
      "  1.49766211e-060 8.27077882e-010]\n",
      " [1.00000000e+000 2.80028853e-054 1.53409474e-068 4.12917577e-086\n",
      "  3.33829236e-115 1.78467356e-057]]\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict_proba(vectors_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have hoped for a better result and you may be disappointed. Yet, this result is on the other hand quite impressive. In nearly 60 % of all cases we got the label 0, which stand for Virginia Woolf and her novel \"Night and Day\". We can say that our classifier recognized the Woolf writing style just by the words in nearly 60 percent of all the paragraphs, even though it is a different novel.\n",
    "\n",
    "Let us have a look at the first 10 paragraphs which we have tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.26578058e-04 2.51943113e-02 4.85163038e-08 4.75065393e-05\n",
      " 4.00835263e-14 9.74131556e-01] \"That's the painful thing about pets,\" said Mr. Dalloway; \"they die. The\n",
      "first sorrow I can remember was for the death of a dormouse. I regret to\n",
      "say that I sat upon it. Still, that didn't make one any the less sorry.\n",
      "Here lies the duck that Samuel Johnson sat on, eh? I was big for my\n",
      "age.\"\n",
      "[7.12081909e-01 4.92957656e-02 5.37096844e-03 1.68824845e-09\n",
      " 4.99835718e-13 2.33251355e-01] \"Please tell me--everything.\" That was what she wanted to say. He had\n",
      "drawn apart one little chink and showed astonishing treasures. It seemed\n",
      "to her incredible that a man like that should be willing to talk to her.\n",
      "He had sisters and pets, and once lived in the country. She stirred her\n",
      "tea round and round; the bubbles which swam and clustered in the cup\n",
      "seemed to her like the union of their minds.\n",
      "[1.11615265e-01 1.70149726e-09 8.02170949e-13 1.93038351e-08\n",
      " 3.38381992e-17 8.88384714e-01] The talk meanwhile raced past her, and when Richard suddenly stated in a\n",
      "jocular tone of voice, \"I'm sure Miss Vinrace, now, has secret leanings\n",
      "towards Catholicism,\" she had no idea what to answer, and Helen could\n",
      "not help laughing at the start she gave.\n",
      "[1.94979929e-05 4.16423135e-06 1.30402613e-13 4.90014758e-03\n",
      " 1.02628751e-18 9.95076190e-01] However, breakfast was over and Mrs. Dalloway was rising. \"I always\n",
      "think religion's like collecting beetles,\" she said, summing up the\n",
      "discussion as she went up the stairs with Helen. \"One person has a\n",
      "passion for black beetles; another hasn't; it's no good arguing about\n",
      "it. What's _your_ black beetle now?\"\n",
      "[1.00000000e+00 2.88701360e-46 1.83061388e-38 5.54119421e-32\n",
      " 7.87165681e-71 1.33908569e-29] It was as though a blue shadow had fallen across a pool. Their eyes\n",
      "became deeper, and their voices more cordial. Instead of joining them\n",
      "as they began to pace the deck, Rachel was indignant with the prosperous\n",
      "matrons, who made her feel outside their world and motherless, and\n",
      "turning back, she left them abruptly. She slammed the door of her room,\n",
      "and pulled out her music. It was all old music--Bach and Beethoven,\n",
      "Mozart and Purcell--the pages yellow, the engraving rough to the finger.\n",
      "In three minutes she was deep in a very difficult, very classical fugue\n",
      "in A, and over her face came a queer remote impersonal expression of\n",
      "complete absorption and anxious satisfaction. Now she stumbled; now she\n",
      "faltered and had to play the same bar twice over; but an invisible\n",
      "line seemed to string the notes together, from which rose a shape,\n",
      "a building. She was so far absorbed in this work, for it was really\n",
      "difficult to find how all these sounds should stand together, and drew\n",
      "upon the whole of her faculties, that she never heard a knock at the\n",
      "door. It was burst impulsively open, and Mrs. Dalloway stood in the room\n",
      "leaving the door open, so that a strip of the white deck and of the blue\n",
      "sea appeared through the opening. The shape of the Bach fugue crashed to\n",
      "the ground.\n",
      "[3.01049983e-02 2.33225150e-01 1.44790362e-07 2.08470928e-02\n",
      " 1.21445899e-20 7.15822614e-01] \"He wrote awfully well, didn't he?\" said Clarissa; \"--if one likes\n",
      "that kind of thing--finished his sentences and all that. _Wuthering_\n",
      "_Heights_! Ah--that's more in my line. I really couldn't exist without\n",
      "the Brontes! Don't you love them? Still, on the whole, I'd rather live\n",
      "without them than without Jane Austen.\"\n",
      "[8.44480345e-03 4.79211117e-16 5.36229064e-04 1.94962600e-08\n",
      " 1.93352536e-27 9.91018948e-01] How divine!--and yet what nonsense!\" She looked lightly round the room.\n",
      "\"I always think it's _living_, not dying, that counts. I really respect\n",
      "some snuffy old stockbroker who's gone on adding up column after column\n",
      "all his days, and trotting back to his villa at Brixton with some old\n",
      "pug dog he worships, and a dreary little wife sitting at the end of the\n",
      "table, and going off to Margate for a fortnight--I assure you I know\n",
      "heaps like that--well, they seem to me _really_ nobler than poets whom\n",
      "every one worships, just because they're geniuses and die young. But I\n",
      "don't expect _you_ to agree with me!\"\n",
      "[9.99929790e-01 2.75362913e-05 7.08502304e-14 4.80647305e-11\n",
      " 3.30471723e-13 4.26739511e-05] \"When you're my age you'll see that the world is _crammed_ with\n",
      "delightful things. I think young people make such a mistake about\n",
      "that--not letting themselves be happy. I sometimes think that happiness\n",
      "is the only thing that counts. I don't know you well enough to say, but\n",
      "I should guess you might be a little inclined to--when one's young and\n",
      "attractive--I'm going to say it!--_every_thing's at one's feet.\" She\n",
      "glanced round as much as to say, \"not only a few stuffy books and Bach.\"\n",
      "[1.06997945e-10 1.91268645e-22 9.99999647e-01 6.84957708e-12\n",
      " 3.46586775e-07 5.86836045e-09] The shores of Portugal were beginning to lose their substance; but\n",
      "the land was still the land, though at a great distance. They could\n",
      "distinguish the little towns that were sprinkled in the folds of the\n",
      "hills, and the smoke rising faintly. The towns appeared to be very small\n",
      "in comparison with the great purple mountains behind them.\n",
      "[4.71639134e-05 1.59969960e-12 3.57196090e-02 3.39541813e-12\n",
      " 2.99749181e-17 9.64233227e-01] Rachel followed her eyes and found that they rested for a second, on the\n",
      "robust figure of Richard Dalloway, who was engaged in striking a match\n",
      "on the sole of his boot; while Willoughby expounded something, which\n",
      "seemed to be of great interest to them both.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(predictions[i], paras[i+first_para])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paragraph with the index 100 was predicted as being \"Ulysses by James Joyce\". This paragraph contains the name \"Samuel Johnson\". \"Ulysses\" contains many occurences of \"Samuel\" and \"Johnson\", whereas \"Night and Day\" doesn't contain neither \"Samuel\" and \"Johnson\". So, this might be one of the reasons for the prediction. \n",
    "\n",
    "We had trained a Naive Bayes classifier by using ```MultinomialNB```. We want to train now a Neural Network. We will use ```MLPClassifier``` in the following. Be warned: It will take a long time, unless you have an extremely fast computer. On my computer it takes about five minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a classifier. This will take some time!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "print(\"Creating a classifier. This will take some time!\")\n",
    "classifier = MLPClassifier(random_state=1, max_iter=300).fit(vectors, train_targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9085465432770822\n",
      "F1-score:  0.9125873156984565\n"
     ]
    }
   ],
   "source": [
    "vectors_test = vectorizer.transform(test_data)\n",
    "\n",
    "predictions = classifier.predict(vectors_test)\n",
    "accuracy_score = metrics.accuracy_score(test_targets, \n",
    "                                        predictions)\n",
    "f1_score = metrics.f1_score(test_targets, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Prediction\n",
    "\n",
    "\n",
    "We will train now a classifier which will be capable of recognizing the language of a text for the languages:\n",
    "\n",
    "German, Danish, English, Spanish, French, Italian, Dutch and Swedish\n",
    "\n",
    "We will use two books of each language for training and testing purposes. \n",
    "The authors and book titles should be recognizable in the following file names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it_alessandro_manzoni_i_promessi_sposi.txt',\n",
       " 'es_antonio_de_alarcon_novelas_cortas.txt',\n",
       " 'de_nietzsche_also_sprach_zarathustra.txt',\n",
       " 'nl_lodewijk_van_deyssel.txt',\n",
       " 'de_goethe_leiden_des_jungen_werther2.txt',\n",
       " 'se_august_strindberg_röda_rummet.txt',\n",
       " 'license',\n",
       " 'it_amato_gennaro_una_sfida_al_polo.txt',\n",
       " 'nl_cornelis_johannes_kieviet_Dik_Trom_en_sijn_dorpgenooten.txt',\n",
       " 'fr_emile_zola_la_bete_humaine.txt',\n",
       " 'se_selma_lagerlöf_bannlyst.txt',\n",
       " 'de_goethe_leiden_des_jungen_werther1.txt',\n",
       " 'en_virginia_woolf_night_and_day.txt',\n",
       " 'original',\n",
       " 'es_mguel_de_cervantes_don_cuijote.txt',\n",
       " 'en_herman_melville_moby_dick.txt',\n",
       " 'dk_andreas_lauritz_clemmensen_beskrivelser_og_tegninger.txt',\n",
       " 'fr_emile_zola_germinal.txt']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"books/various_languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Virginia Woolf', 'Samuel Butler', 'Herman Melville', \n",
    "          'David Herbert Lawrence', 'Daniel Defoe', 'James Joyce']\n",
    "\n",
    "path = \"books/various_languages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'dk', 'en', 'es', 'fr', 'it', 'nl', 'se']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(\"books/various_languages\")\n",
    "labels = {fname[:2] for fname in files if fname.endswith(\".txt\")}\n",
    "labels = sorted(list(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it_alessandro_manzoni_i_promessi_sposi.txt', 'es_antonio_de_alarcon_novelas_cortas.txt', 'de_nietzsche_also_sprach_zarathustra.txt', 'nl_lodewijk_van_deyssel.txt', 'de_goethe_leiden_des_jungen_werther2.txt', 'se_august_strindberg_röda_rummet.txt', 'license', 'it_amato_gennaro_una_sfida_al_polo.txt', 'nl_cornelis_johannes_kieviet_Dik_Trom_en_sijn_dorpgenooten.txt', 'fr_emile_zola_la_bete_humaine.txt', 'se_selma_lagerlöf_bannlyst.txt', 'de_goethe_leiden_des_jungen_werther1.txt', 'en_virginia_woolf_night_and_day.txt', 'original', 'es_mguel_de_cervantes_don_cuijote.txt', 'en_herman_melville_moby_dick.txt', 'dk_andreas_lauritz_clemmensen_beskrivelser_og_tegninger.txt', 'fr_emile_zola_germinal.txt']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "targets = []\n",
    "\n",
    "for fname in files:\n",
    "    if fname.endswith(\".txt\"):\n",
    "        paras = text2paragraphs(path + fname, min_size=150)\n",
    "        data.extend(paras)\n",
    "        country = fname[:2]\n",
    "        index = labels.index(country)\n",
    "        targets += [index] * len(paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_targets = list(zip(data, targets))\n",
    "# create random permuation on list:\n",
    "data_targets = random.sample(data_targets, len(data_targets))\n",
    "\n",
    "data, targets = list(zip(*data_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "res = train_test_split(data, targets, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42)\n",
    "train_data, test_data, train_targets, test_targets = res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9946569178852643\n",
      "F1-score:  0.9966453736745848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "#vectorizer = CountVectorizer()\n",
    "\n",
    "vectors = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# creating a classifier\n",
    "classifier = MultinomialNB(alpha=.01)\n",
    "classifier.fit(vectors, train_targets)\n",
    "\n",
    "vectors_test = vectorizer.transform(test_data)\n",
    "\n",
    "predictions = classifier.predict(vectors_test)\n",
    "accuracy_score = metrics.accuracy_score(test_targets, \n",
    "                                        predictions)\n",
    "f1_score = metrics.f1_score(test_targets, \n",
    "                            predictions, \n",
    "                            average='macro')\n",
    "\n",
    "print(\"accuracy score: \", accuracy_score)\n",
    "print(\"F1-score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check this classifiert with some abitrary text in different languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_texts = [\"Es ist nicht von Bedeutung, wie langsam du gehst, solange du nicht stehenbleibst.\",\n",
    "              \"Man muss das Unmögliche versuchen, um das Mögliche zu erreichen.\",\n",
    "              \"It's so much darker when a light goes out than it would have been if it had never shone.\",\n",
    "              \"Rien n'est jamais fini, il suffit d'un peu de bonheur pour que tout recommence.\",\n",
    "              \"Girano le stelle nella notte ed io ti penso forte forte e forte ti vorrei\"]\n",
    "\n",
    "sources = [\"Konfuzius\", \"Hermann Hesse\", \"John Steinbeck\", \"Emile Zola\", \"Gianna Nannini\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 de\n",
      "0 de\n",
      "2 en\n",
      "4 fr\n",
      "5 it\n"
     ]
    }
   ],
   "source": [
    "vtest = vectorizer.transform(some_texts)\n",
    "predictions = classifier.predict(vtest)\n",
    "for label in predictions:\n",
    "    print(label, labels[label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
