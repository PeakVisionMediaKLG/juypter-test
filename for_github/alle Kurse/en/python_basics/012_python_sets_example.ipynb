{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Extensive Example for Sets\n",
    "\n",
    "\n",
    "### Python and the Best Novel\n",
    "\n",
    "<img width=250 height=250 class=\"imgright\" src=\"../images/joyce_and_others.webp\" srcset=\"../images/joyce_and_others_700w.webp 700w,../images/joyce_and_others_600w.webp 600w,../images/joyce_and_others_500w.webp 500w,../images/joyce_and_others_400w.webp 400w,../images/joyce_and_others_350w.webp 350w,../images/joyce_and_others_300w.webp 300w\" alt=\"Analysing James Joyce\" />\n",
    "\n",
    "This chapter deals with natural languages and literature. It will be also an extensive example and use case for Python sets. Novices in Python often think that sets are just a toy for mathematicians and that there is no real use case in programming. The contrary is true. There are multiple use cases for sets. They are used, for example, to get rid of doublets - multiple occurrences of elements - in a list, i.e. to make a list unique.\n",
    "\n",
    "In the following example we will use sets to determine the different words occurring in a novel. Our use case is build around a novel which has been praised by many, and regarded as the best novel in the English language and also as the hardest to read. We are talking about the novel \"Ulysses\" by James Joyce. We will not talk about or examine the beauty of the language or the language style. We will study the novel by having a close look at the words used in the novel. Our approach will be purely statitical. The claim is that James Joyce used in his novel more words than any other author. Actually his vocabulary is above and beyond all other authors, maybe even Shakespeare.\n",
    "\n",
    "Besides Ulysses we will use the novels \n",
    "[\"Sons and Lovers\"](books/sons_and_lovers_lawrence.txt) by D.H. Lawrence, [\"The Way of All Flesh\"](books/the_way_of_all_flash_butler.txt) by Samuel Butler, [\"Robinson Crusoe\"](books/robinson_crusoe_defoe.txt) by Daniel Defoe, [\"To the Lighthouse\"](books/to_the_lighthouse_woolf.txt) by Virginia Woolf, [\"Moby Dick\"](books/moby_dick_melville.txt) by Herman Melville and the Short Story [\"Metamorphosis\"](books/metamorphosis_kafka.txt) by Franz Kafka.\n",
    "\n",
    "Before you continue with this chapter of our tutorial it might be a good idea to read the chapter [Sets and Frozen Sets](python3_sets_frozensets.php) and the two chapter on [regular expressions](python3_re.php) and [advanced regular expressions](python3_re_advanced.php).\n",
    "\n",
    "### Different Words of a Text\n",
    "\n",
    "To cut out all the words of the novel \"Ulysses\" we can use the function findall from the module \"re\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 270653: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18646764c8d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# we don't care about case sensitivity and therefore use lower:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mulysses_txt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"books/james_joyce_ulysses.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\b[\\w-]+\\b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mulysses_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 270653: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# we don't care about case sensitivity and therefore use lower:\n",
    "ulysses_txt = open(\"books/james_joyce_ulysses.txt\").read().lower()\n",
    "\n",
    "words = re.findall(r\"\\b[\\w-]+\\b\", ulysses_txt)\n",
    "print(\"The novel ulysses contains \" + str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is the sum of all the words, together with the many words that occur multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'the' occurs 15112 times in the novel!\n",
      "The word 'while' occurs 123 times in the novel!\n",
      "The word 'good' occurs 321 times in the novel!\n",
      "The word 'bad' occurs 90 times in the novel!\n",
      "The word 'ireland' occurs 90 times in the novel!\n",
      "The word 'irish' occurs 117 times in the novel!\n"
     ]
    }
   ],
   "source": [
    "for word in [\"the\", \"while\", \"good\", \"bad\", \"ireland\", \"irish\"]:\n",
    "    print(\"The word '\" + word + \"' occurs \" + \\\n",
    "          str(words.count(word)) + \" times in the novel!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "272452 surely is a huge number of words for a novel, but on the other hand there are lots of novels with even more words. More interesting and saying more about the quality of a novel is the number of different words. This is the moment where we will finally need \"set\". We will turn the list of words \"words\" into a set. Applying \"len\" to this set will give us the number of different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ulysses' contains 29422 different words!\n"
     ]
    }
   ],
   "source": [
    "diff_words = set(words)\n",
    "print(\"'Ulysses' contains \" + str(len(diff_words)) + \" different words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed an impressive number. You can see this, if you look at the other novels below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sons_and_lovers_lawrence              : 10822\n",
      "metamorphosis_kafka                   :  3027\n",
      "the_way_of_all_flash_butler           : 11434\n",
      "robinson_crusoe_defoe                 :  6595\n",
      "to_the_lighthouse_woolf               : 11415\n",
      "james_joyce_ulysses                   : 29422\n",
      "moby_dick_melville                    : 18922\n"
     ]
    }
   ],
   "source": [
    "novels = ['sons_and_lovers_lawrence.txt', \n",
    "          'metamorphosis_kafka.txt', \n",
    "          'the_way_of_all_flash_butler.txt', \n",
    "          'robinson_crusoe_defoe.txt', \n",
    "          'to_the_lighthouse_woolf.txt', \n",
    "          'james_joyce_ulysses.txt', \n",
    "          'moby_dick_melville.txt']\n",
    "\n",
    "for novel in novels:\n",
    "    txt = open(\"books/\" + novel).read().lower()\n",
    "    words = re.findall(r\"\\b[\\w-]+\\b\", txt)\n",
    "    diff_words = set(words)\n",
    "    n = len(diff_words)\n",
    "    print(\"{name:38s}: {n:5d}\".format(name=novel[:-4], n=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Words in Ulysses\n",
    "\n",
    "We will subtract all the words occurring in the other novels from \"Ulysses\" in the following little Python program. It is amazing how many words are used by James Joyce and by none of the other authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15314\n"
     ]
    }
   ],
   "source": [
    "words_in_novel = {}\n",
    "for novel in novels:\n",
    "    txt = open(\"books/\" + novel).read().lower()\n",
    "    words = re.findall(r\"\\b[\\w-]+\\b\", txt)\n",
    "    words_in_novel[novel] = words\n",
    "    \n",
    "words_only_in_ulysses =  set(words_in_novel['james_joyce_ulysses.txt'])\n",
    "novels.remove('james_joyce_ulysses.txt')\n",
    "for novel in novels:\n",
    "    words_only_in_ulysses -= set(words_in_novel[novel])\n",
    "    \n",
    "with open(\"books/words_only_in_ulysses.txt\", \"w\") as fh:\n",
    "    txt = \" \".join(words_only_in_ulysses)\n",
    "    fh.write(txt)\n",
    "    \n",
    "print(len(words_only_in_ulysses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, Dr. Seuss wrote a book with only 50 different words: \"Green Eggs and Ham\"\n",
    "\n",
    "The [file with the words only occurring in Ulysses](books/words_only_in_ulysses.txt)  contains strange or seldom used words like:\n",
    "\n",
    "huntingcrop tramtrack pappin kithogue pennyweight undergarments scission nagyas√°gos wheedling begad dogwhip hawthornden turnbull calumet covey repudiated pendennis waistcoatpocket nostrum\n",
    "\n",
    "### Common Words\n",
    "\n",
    "It is also possible to find the words which occur in every book. To accomplish this, we need the set intersection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1745\n"
     ]
    }
   ],
   "source": [
    "# we start with the words in ulysses\n",
    "common_words = set(words_in_novel['james_joyce_ulysses.txt'])\n",
    "for novel in novels:\n",
    "    common_words &= set(words_in_novel[novel])\n",
    "    \n",
    "print(len(common_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing it Right\n",
    "\n",
    "We made a slight mistake in the previous calculations. If you look at the texts, you will notice that they have a header and footer part added by Project Gutenberg, which doesn't belong to the texts. The texts are positioned between the lines:\n",
    "\n",
    "***START OF THE PROJECT GUTENBERG EBOOK THE WAY OF ALL FLESH***\n",
    "\n",
    "and\n",
    "\n",
    "***END OF THE PROJECT GUTENBERG EBOOK THE WAY OF ALL FLESH***\n",
    "\n",
    "or\n",
    "\n",
    "*** START OF THIS PROJECT GUTENBERG EBOOK ULYSSES ***\n",
    "\n",
    "and\n",
    "\n",
    "*** END OF THIS PROJECT GUTENBERG EBOOK ULYSSES ***\n",
    "\n",
    "The function read_text takes care of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15341\n",
      "1279\n"
     ]
    }
   ],
   "source": [
    "def read_text(fname):\n",
    "    beg_e = re.compile(r\"\\*\\*\\* ?start of (this|the) project gutenberg ebook[^*]*\\*\\*\\*\")\n",
    "    end_e = re.compile(r\"\\*\\*\\* ?end of (this|the) project gutenberg ebook[^*]*\\*\\*\\*\")\n",
    "    txt = open(\"books/\" + fname).read().lower()\n",
    "    beg = beg_e.search(txt).end()\n",
    "    end = end_e.search(txt).start()\n",
    "    return txt[beg:end]\n",
    "\n",
    "words_in_novel = {}\n",
    "for novel in novels + ['james_joyce_ulysses.txt']:\n",
    "    txt = read_text(novel)\n",
    "    words = re.findall(r\"\\b[\\w-]+\\b\", txt)\n",
    "    words_in_novel[novel] = words\n",
    "\n",
    "words_in_ulysses =  set(words_in_novel['james_joyce_ulysses.txt'])\n",
    "for novel in novels:\n",
    "    words_in_ulysses -= set(words_in_novel[novel])\n",
    "    \n",
    "with open(\"books/words_in_ulysses.txt\", \"w\") as fh:\n",
    "    txt = \" \".join(words_in_ulysses)\n",
    "    fh.write(txt)\n",
    "    \n",
    "print(len(words_in_ulysses))\n",
    "\n",
    "\n",
    "# we start with the words in ulysses\n",
    "common_words = set(words_in_novel['james_joyce_ulysses.txt'])\n",
    "for novel in novels:\n",
    "    common_words &= set(words_in_novel[novel])\n",
    "    \n",
    "print(len(common_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words of the set \"common_words\" are words belong to the most frequently used words of the English language. Let's have a look at 30 arbitrary words of this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send, such, mentioned, writing, found, speak, fond, food, their, mother, household, through, prepared, flew, gently, work, station, naturally, near, empty, filled, move, unknown, left, alarm, listening, waited, showed, broke, laugh, "
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for word in common_words:\n",
    "    print(word, end=\", \")\n",
    "    counter += 1\n",
    "    if counter == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ancient, broke, breathing, laugh, divided, forced, wealth, ring, outside, throw, person, spend, better, errand, school, sought, knock, tell, inner, run, packed, another, since, touched, bearing, repeated, bitter, experienced, often, one, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
